<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>因果推断</title>
    <link href="/2023/09/10/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/"/>
    <url>/2023/09/10/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/</url>
    
    <content type="html"><![CDATA[<hr><span id="more"></span><h2 id="概述">概述</h2><p>机器学习到底在学什么？从数据中能够得到什么？如何保证得到的结论是正确的？要解决这些问题，就要跳出简单的相关性分析，而要深入到因果性分析。例如，我们可以收集到是否吸烟和是否得肺癌得数据，然后使用分类模型预测肺癌的患病率，或者计算处吸烟和肺癌具有相关性来说服公众。然而，我们为什么不根据肺癌预测吸烟率？显然这是一个无意义的研究目标。而我们为什么知道它无意义，以至于“显然”呢？得到这个结果的前提，是因为我们都确信吸烟有可能导致肺癌，而肺癌并非导致吸烟的原因。无论是获得在吸烟条件下得肺癌的概率，还是证明吸烟和肺癌具有相关性，我们得到的结论都是靠常识去推断的。小而言之，这样需要人参与推理的模型并非完美的算法，如同用算盘计算之于用计算器计算；大而言之，我们并非总有自信认为从相关性中就能得到一些正确的结论，它们并非像吸烟和肺癌的例子那样简单，比如下面的例子。</p><h3 id="simpson问题">Simpson问题</h3><p>某医院统计出某药物对患者的治疗效果，结果如下表所示。</p><p><img src="1.PNG"></p><p>从最终结果来看，不使用药物的治愈率比使用的高出5%，这是否说明不使用药物比较好？但是如果把性别分开，则发现，在男性中使用药物的治愈率比不使用的高6%，女性中高4%，那么这说明使用药物比较好？导致这一分歧的原因在于，女性整体治愈率明显低于男性，且女性药物使用率明显高于男性，最终在使用药物组中占主导作用的是女性中使用药物的群体，在不使用药物组占主导作用的是男性中不使用药物的群体，而男性中不使用药物的群体把不使用药物组最终的治愈率抬高了。用符号描述就是：<span class="math display">\[\frac{a_1}{b_1}&gt;\frac{a_2}{b_2},\frac{a_3}{b_3}&gt;\frac{a_4}{b_4}\not\equiv\frac{a_1+a_3}{b_1+b_3}&gt;\frac{a_2+a_4}{b_2+b_4}\]</span>试想，我们面对存在这种关系的数据集时，可以得到高准确率的预测是否治愈的模型，也可以从多个特征中选出包括使用药物和性别的特征子集，然而这些能够给出药物是否具有提高治愈率的结论吗？不能，这个问题已经不属于相关性问题了，而是深入到因果性。我们需要更深入的算法来解决这类问题。</p><h3 id="相关性与因果关系">相关性与因果关系</h3><p>相关性是反映因果关系的表面现象。因为存在变量间因果关系，所以它们具有相关性。反过来说，具有相关性的变量是否一定有因果关系？这个问题很有争议。在此需要给出一些定义，来说明为什么我认为具有相关性的变量必定具有因果关系。</p><p>稳定数据：如果收集到的数据长期符合某一确定分布，则称其为稳定数据。</p><p>相关性：对于稳定数据中的某些变量，如果它们不独立，则称它们具有相关性。</p><p>因果性：如果强制改变变量<span class="math inline">\(A\)</span>的值会伴随着变量<span class="math inline">\(B\)</span>的值的变化，而强制改变变量<span class="math inline">\(B\)</span>的值不会伴随变量<span class="math inline">\(A\)</span>的值的变化，则称<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>具有因果性，<span class="math inline">\(A\)</span>是因，<span class="math inline">\(B\)</span>是果。在有向无环图（Directed AcyclicGraph，DAG）中，用节点表示变量，有向边从因节点指向果节点。</p><p>因果关系：变量<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>具有因果关系的充分条件有以下情况：</p><ol type="1"><li><span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>具有因果性</li><li><span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>共因，即<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>是同一因节点的后代。</li></ol><p>基于上述定义，具有因果关系和具有相关性是充要的。例如，下图展示了阿美莉卡在科技领域投入和自杀人数的数据。</p><p><img src="2.PNG"></p><p>（更多的例子见<a href="https://www.tylervigen.com/spurious-correlations">SpuriousCorrelations (tylervigen.com)</a>）</p><p>上面的数据不符合稳定数据的定义，因为样本数量过少，但是，如果把每一天的数据都统计出来，连续10年，如果还有这样强的相关性，那么我们就不得不相信科技领域投入和自杀人数之间存在因果关系（比如开展了Frankenstein研究）。</p><p>另一种情况是，互因关系，我们不能用DAG来表示这些节点的关系。例如，在一个生态系统中，狼的数量增加-&gt;羊的数量减少-&gt;狼的数量减少-&gt;羊的数量增加-&gt;狼的数量增加。狼的数量和羊的数量不能用DAG表示。本文将不讨论这种情况，只讨论能够用DAG表示的关系。</p><h2 id="基础内容">基础内容</h2><p>用大写字母表示随机变量，小写字母代表随机变量的取值，公式中出现的<span class="math inline">\(P(X)\)</span>表示对<span class="math inline">\(X\)</span>的所有取值均成立，<span class="math inline">\(P(X=x)或\)</span><span class="math inline">\(P(x)\)</span>表示<span class="math inline">\(X=x\)</span>的概率。<span class="math inline">\(P(X=x|Y=y)=P(x|y)\)</span>表示在<span class="math inline">\(Y=y\)</span>时<span class="math inline">\(X=x\)</span>的概率。<span class="math inline">\(P(x_1,x_2,...x_n)\)</span>表示<span class="math inline">\(X_1=x_1\wedge X_2=x_2,...\wedgeX_n=x_n\)</span>的概率。</p><h3 id="条件独立">条件独立</h3><p>在随机变量<span class="math inline">\(C\)</span>的取值确定的情况下<span class="math inline">\((C=k)\)</span>，随机变量<span class="math inline">\(A\)</span>与<span class="math inline">\(B\)</span>独立，则<span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>关于<span class="math inline">\(C=k\)</span>条件独立，即：</p><p><span class="math inline">\(A\perp B|C=k\iffP(A|C=k)P(B|C=k)=P(AB|C=k)\)</span></p><p>若对任意<span class="math inline">\(C\)</span>的取值均有上述关系，则<span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>关于<span class="math inline">\(C\)</span>条件独立，记为<span class="math inline">\(A\perp B|C\)</span></p><p>上式等价于 <span class="math display">\[P(B|C=k)=P(B|C=k,A)\\P(A|C=k)=P(A|C=k,B)\]</span> <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>关于<span class="math inline">\(C\)</span>条件独立与<span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>独立既不充分也不必要，比如设<span class="math inline">\(A\)</span>：第一次扔硬币正面朝上，<span class="math inline">\(B\)</span>：第二次扔硬币正面朝上，<span class="math inline">\(C\)</span>：两次均正面朝上。此时<span class="math inline">\(A\)</span>与<span class="math inline">\(B\)</span>独立，但是<span class="math inline">\(P(A|C=1)=P(B|C=1)=P(AB|C=1)=1\)</span>，<span class="math inline">\(A\)</span>，<span class="math inline">\(B\)</span>关于<span class="math inline">\(C\)</span>不独立。另一方面，设<span class="math inline">\(A=B=C\)</span>，此时有<span class="math inline">\(P(A|C)P(B|C)=P(AB|C)=1\)</span>，但<span class="math inline">\(P(A)P(B)=P(A)^2\neq P(AB)=P(A)\)</span></p><h3 id="链式法则">链式法则</h3><p>对于随机变量<span class="math inline">\(X_1,X_2...X_n\)</span>，当它们的取值为<span class="math inline">\(X_i=x_i\)</span>时，有 <span class="math display">\[P(x_1,x_2,...x_n)=P(x_1)P(x_2|x_1)...P(x_n|x_1,x_2...x_{n-1})\]</span> 由条件概率定义即可证</p><h3 id="带函数的联合概率">带函数的联合概率</h3><p>设随机变量<span class="math inline">\(A\)</span>,<span class="math inline">\(B\)</span>,<span class="math inline">\(C\)</span>满足<span class="math inline">\(C=f(A,B)\)</span>, 则 <span class="math display">\[P(C=c,A=a)=P(B=u_b,A=a)\]</span> 其中<span class="math inline">\(u_b\)</span>是满足<span class="math inline">\(f(a,u_b)=c\)</span>的<span class="math inline">\(B\)</span>的取值集合</p><h2 id="概率模型伪因果模型">概率模型（伪因果模型）</h2><h3 id="朴素贝叶斯">朴素贝叶斯</h3><p>将分类任务看作是在给定特征的条件下目标取某个值的概率，最终要得到<span class="math inline">\(P(y|x_1,x_2,...x_n)\)</span></p><p>由条件概率关系（贝叶斯公式） <span class="math display">\[P(y|x_1,x_2,...x_n)=\frac{P(x_1,x_2,...x_n|y)P(y)}{P(x_1,x_2,...x_n)}\]</span> 其中，<span class="math inline">\(P(y)\)</span>可以由<span class="math inline">\(Y=y\)</span>的频率得到，在分类任务中，一般目标值的种类数远小于样本数，因此可以这么做。而<span class="math inline">\(P(x_1,x_2,...x_n|y)\)</span>和<span class="math inline">\(P(x_1,x_2,...x_n)\)</span>就不能这样了，这时特征的组合过多，设想每个特征都是二值的，那么就有<span class="math inline">\(2^n\)</span>种组合，未必找得到符合这种组合的样本。因此，需要做亿点简化，把特征均看作独立的，并且特征关于目标也都是独立的。于是把上式简化成<span class="math display">\[P(y|x_1,x_2,...x_n)=\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}\]</span> 这样<span class="math inline">\(P(x_k|y)\)</span>，<span class="math inline">\(P(x_k)\)</span>都可以用频率得到了。</p><p>毫无疑问，这种做法过于暴殄天物，数据集特征越多，特征冗余就越多，最后效果越不好。</p><h3 id="贝叶斯网络">贝叶斯网络</h3><p>朴素贝叶斯的做法把特征间的关系完全忽略了。贝叶斯网络则没有这么暴力，虽然还是做了一些简化。需要指出，贝叶斯网络本质上是描述随机变量相关性的关系图，贝叶斯网络中的箭头方向仅代表了特征的加入顺序，箭头连接的两个变量是相关的，但不一定具有因果性。</p><p>手动的贝叶斯网络的算法流程如下。（贝叶斯网络的构建属于NP问题，因此另一种方式是用群优化算法搜索）显然通过这种方式得到的是有向无环图（Directedacyclic graph, DAG）。</p><p>Input: Random variables <span class="math inline">\(V_1,V_2...V_n\)</span> Output: A directed acyclicgraph which describes the correlation among variables</p><ol type="1"><li>for <span class="math inline">\(i\)</span> in <span class="math inline">\(1,2...n\)</span> do</li><li>Add <span class="math inline">\(V_i\)</span> into the graph, if<span class="math inline">\(V_i\)</span> has correlation with any othernodes(which have a smaller order than <span class="math inline">\(V_i\)</span>) in the graph, add a directed edgebetween these two nodes with <span class="math inline">\(V_i\)</span> asthe end point. For example, if <span class="math inline">\(V_k(k&lt;i)\)</span> has a correlation with <span class="math inline">\(V_i\)</span>, then add a directed edge from <span class="math inline">\(V_k\)</span> to <span class="math inline">\(V_i\)</span></li><li>endfor</li><li>Compute the probability table for every point.</li></ol><h3 id="条件独立假设">条件独立假设</h3><p>可以看到，在贝叶斯网络中的相关性是手动添加的，手动的好处是让网络结构更简单。用群优化算法也可以得到一个贝叶斯网络，能这样做的原因是在贝叶斯网络中只考虑变量之间的相关性，而相关性可以直接由频率给出。例如下图：</p><p><img src="捕获.PNG" style="width:60.0%"></p><p>左右两图描述了同一件事，即最右边的概率表。最右边的概率表直接从数据集中得出。这两个结构孰是孰非，站在上帝视角来看是左边描述了正确的因果关系。然而，我们实际上从数据集中只得能得到概率表，也就只能判断出下雨和交通堵塞存在相关性。如何得到因果性会在下文讨论，在此先弄明白如何计算概率表。考虑一个<span class="math inline">\(n\)</span>个变量组成的贝叶斯网络，若要计算<span class="math inline">\(P(v_1,v_2,...v_n)\)</span>，则由链式法则计算：<span class="math display">\[P(v_n)=\prod _{i=1}^{n}P(v_i|v_1,...v_{i-1})\]</span></p><p>在贝叶斯网络中，若存在关系<span class="math inline">\(A\rightarrowB\rightarrow C\)</span>，则假设<span class="math inline">\(A\)</span>与<span class="math inline">\(C\)</span>关于<span class="math inline">\(B\)</span>条件独立，即假设： <span class="math display">\[v_i\perp \{v_1,...v_{i-1} \} \cap {\neg parent(v_i)}|parent(v_i)\]</span> 于是 <span class="math display">\[P(v_i|v_1,...v_{i-1})=P(v_i|parent(v_{i}))\\\P(v_n)=\prod _{i=1}^{n}P(v_i|parent(v_{i}))\tag{1}\]</span> 一般的贝叶斯网络节点的父节点数远小于总节点数，因此计算<span class="math inline">\(P(v_i|parent(v_{i}))\)</span>是可行的。条件独立假设仍然是一种简化，但比起朴素贝叶斯保留了更多的特征关系信息。</p><h2 id="因果模型">因果模型</h2><p>在贝叶斯网络之中，做了条件独立假设，这忽略了特征间的一些关系。关系<span class="math inline">\(A\rightarrow B\rightarrow C\)</span>中未必<span class="math inline">\(A\)</span>与<span class="math inline">\(C\)</span>关于<span class="math inline">\(B\)</span>独立。在贝叶斯网络的手动构建算法里，后加入的节点若与网络中的节点存在相关性，则连一条边，也就是在关系<span class="math inline">\(A\rightarrow B\rightarrow C\)</span>中，<span class="math inline">\(A\)</span>与<span class="math inline">\(C\)</span>独立，但未必关于<span class="math inline">\(B\)</span>条件独立。这在第一节已经讨论过了。然而，如果我们可以做到构建一个符合条件独立性的贝叶斯网络，那么这个网络就可以突破假设的局限性了，这是可以做到的，即通过因果性构建贝叶斯网络。因果性DAG中，因节点的取值会影响果节点的取值。（但不是必然影响，因为隐变量的存在）</p><h3 id="隐变量exogenous-variables">隐变量(exogenous variables)</h3><p>有一些变量对数据造成了影响，然而它们由于某些缘故并没有被记录为特征，这些变量即隐变量。因为隐变量的存在，我们无法确保DAG中的一个因的取值必定对应一个果的取值，而是因的取值和隐变量的取值共同决定了果的取值。假设隐变量与观察到的变量独立。</p><h3 id="定义">定义</h3><p>因果关系可以看作一个函数，因作为自变量，果作为因变量，即一个因或一些因的取值必定对应一个果的取值。基于这一定义，因果性DAG节点之间的关系有以下3种基本情况。（在因果性DAG中也可以运用以下关系，事实上在历史上下列关系首先出现在非因果性DAG中，只是在因果性DAG中可以根据定义严格证明以下内容，而在非因果性DAG中其仅作为简化计算方式而不具有严格数学意义）</p><ol type="1"><li>链式：<span class="math inline">\(A\rightarrow B\rightarrowC\)</span></li><li>共因：<span class="math inline">\(A\leftarrow B\rightarrowC\)</span></li><li>共果：<span class="math inline">\(A\rightarrow B\leftarrowC\)</span></li></ol><p>在链式关系中，根据因果性的定义，设<span class="math inline">\(A=f(U_A),B=g(A,U_B),C=h(B,U_C)\)</span>，其中<span class="math inline">\(U_A,U_B,U_C\)</span>为隐变量的集合。</p><p>1）若<span class="math inline">\(A=a\)</span>时，<span class="math inline">\(P(B=b)=0\)</span>，则<span class="math inline">\(A=a\)</span>与<span class="math inline">\(B=b\)</span>不独立。否则有 <span class="math display">\[\begin{aligned}P(B=b|A=a)&amp;=P(g(A,U_B)=b|A=a)=\frac{P(g(A,U_B)=b,A=a)}{P(A=a)}\\&amp;=\frac{P(A=a,U_B=u_b)}{P(A=a)}=P(U_B=u_b)\end{aligned}\]</span></p><p>其中<span class="math inline">\(g(a,u_b)=b\)</span>, <span class="math inline">\(u_b\)</span>是满足这个关系的所有<span class="math inline">\(U_B\)</span>的取值集合。</p><p>2）若<span class="math inline">\(P(B=b)=P(U_B=u_b)\)</span>,即仅当<span class="math inline">\(U_B=u_b\)</span>时<span class="math inline">\(B=b\)</span>成立，则<span class="math inline">\(A=a\)</span>与<span class="math inline">\(B=b\)</span>独立</p><p>3）否则<span class="math inline">\(P(B=b|A=a)\neq P(B=b)\)</span>.综合以上情况，仅当<span class="math inline">\(B\)</span>的值完全由<span class="math inline">\(U_B\)</span>决定时，才有<span class="math inline">\(A\)</span>与<span class="math inline">\(B\)</span>独立。类似地，<span class="math inline">\(B\)</span>与<span class="math inline">\(C\)</span>，<span class="math inline">\(A\)</span>与<span class="math inline">\(C\)</span>可能对于某些取值不独立，某些取值独立。而<span class="math inline">\(A\)</span>与<span class="math inline">\(C\)</span>关于<span class="math inline">\(B\)</span>必然条件独立，因为：</p><p>1） 若<span class="math inline">\(P(B=b|A=a)=0\)</span>, 则<span class="math inline">\(P(A=a,B=b)=0\)</span> , 则有<span class="math inline">\(P(A=a|B=b)P(C=c|B=b)=P(A=a,C=c|B=b)=0\)</span>,<span class="math inline">\(A\perp C|B\)</span>. 同理若<span class="math inline">\(P(C=c|B=b)=0\)</span>, 有<span class="math inline">\(A\perp C|B\)</span>.</p><p>2）否则，仿照上例有 <span class="math display">\[\begin{aligned}P(A=a,C=c|B=b)&amp;=\frac{P(A=a,B=b,C=c)}{P(B=b)}\\&amp;=\frac{P(A=a,B=b,U_C=u_c)}{P(B=b)}=\frac{P(A=a,U_B=u_b,U_C=u_c)}{P(B=b)}\\P(A=a|B=b)&amp;=\frac{P(A=a,U_B=u_b)}{P(B=b)}\\P(C=c|B=b)&amp;=P(U_C=u_c)\end{aligned}\]</span> 于是 <span class="math display">\[P(A=a,C=c|B=b)=P(A=a|B=b)P(C=c|B=b)\]</span> 综上，<span class="math inline">\(A\)</span>与<span class="math inline">\(C\)</span>关于<span class="math inline">\(B\)</span>必然条件独立。直观来看，当我们确定了<span class="math inline">\(B\)</span>的值时，<span class="math inline">\(C\)</span>的值大抵可以确定，但我们并不确定<span class="math inline">\(A\)</span>的哪一个取值导致了<span class="math inline">\(B\)</span>的这一取值。</p><p>可以证明，在共因关系中，<span class="math inline">\(A\)</span>与<span class="math inline">\(B\)</span>，<span class="math inline">\(C\)</span>与<span class="math inline">\(B\)</span>，<span class="math inline">\(A\)</span>与<span class="math inline">\(C\)</span>可能不独立，<span class="math inline">\(A\)</span>与<span class="math inline">\(C\)</span>必然关于<span class="math inline">\(B\)</span>条件独立。这一点也好理解，因为给定<span class="math inline">\(B\)</span>之后，<span class="math inline">\(A\)</span>和<span class="math inline">\(C\)</span>的值仅由隐变量决定。</p><p>在共果关系中，<span class="math inline">\(A\)</span>与<span class="math inline">\(B\)</span>，<span class="math inline">\(C\)</span>与<span class="math inline">\(B\)</span>可能不独立，<span class="math inline">\(A\)</span>与<span class="math inline">\(C\)</span>必然独立，<span class="math inline">\(A\)</span>与<span class="math inline">\(C\)</span>必然关于<span class="math inline">\(B\)</span>及其子节点条件不独立。可以考虑以下场景：</p><p>丢两次硬币，若至少有一次正面朝上，则铃响。设<span class="math inline">\(X\)</span>: 丢硬币第一次 <span class="math inline">\(Y\)</span>: 丢硬币第二次 <span class="math inline">\(Z\)</span>: 铃响，符合共果关系<span class="math inline">\(X\rightarrow Z\leftarrow Y\)</span> 则<span class="math inline">\(X,Y,Z\)</span>概率分布如下：</p><p><img src="3.PNG"></p><p>可以得到<span class="math inline">\(P(X=Head|Z=1)=\frac{2}{3},P(X=Head|Y=Head,Z=1)=\frac{1}{2}\)</span>.在得知了<span class="math inline">\(Y\)</span>正面向上后，我们认为<span class="math inline">\(X\)</span>正面向上的概率降低了。现在假设听到铃后，有一个不靠谱的掌铃人会告诉我们铃响，在铃响时100%报告，而铃不响时有50%的概率报告。设<span class="math inline">\(W\)</span>: 收到报告，这时的因果图为：</p><p><img src="4.PNG"></p><p>那么<span class="math inline">\(X,Y,W\)</span>概率分布如下：</p><p><img src="5.PNG"></p><p>可以得到<span class="math inline">\(P(X=Head|W=1)=\frac{0.5}{0.875},P(X=Head|Y=Head,W=1)=0.5\)</span>，可见<span class="math inline">\(X\)</span>与<span class="math inline">\(Y\)</span>关于<span class="math inline">\(W\)</span>条件不独立。</p><h3 id="有向分离directed-separation">有向分离(directed separation)</h3><p>有向分离用于判断两个变量在给定另一些变量的情况下是否相互独立。其总结了上面3种节点关系。</p><ul><li>如果两个变量<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>被变量集合<span class="math inline">\(Z\)</span>阻断，则<span class="math inline">\(X\perp Y|Z\)</span>.其中阻断包含以下两种情况：</li></ul><ol type="1"><li>从<span class="math inline">\(X\)</span>到<span class="math inline">\(Y\)</span>的路径<span class="math inline">\(p\)</span>包含链式关系<span class="math inline">\(A\rightarrow B\rightarrow C\)</span>或共因关系<span class="math inline">\(A\leftarrow B\rightarrowC\)</span>，且<span class="math inline">\(B\)</span>在<span class="math inline">\(Z\)</span>中；</li><li>从<span class="math inline">\(X\)</span>到<span class="math inline">\(Y\)</span>的路径<span class="math inline">\(p\)</span>包含共果关系<span class="math inline">\(A\rightarrow B\leftarrow C\)</span> ，且<span class="math inline">\(B\)</span>及其子节点不在<span class="math inline">\(Z\)</span>中。</li></ol><h3 id="干涉intervening">干涉(Intervening)</h3><p>“The difference between intervening on a variable and conditioning onthat variable should, hopefully, be obvious. When we intervene on avariable in a model, we fix its value. We change the system, and thevalues of other variables often change as a result. When we condition ona variable, we change nothing; we merely narrow our focus to the subsetof cases in which the variable takes the value we are interested in.What changes, then, is our perception about the world, not the worlditself.” ---Chapter 3.1 of <em>Casual Inference in Statistics</em></p><p>在吸烟和肺癌的例子中，我们不能通过相关性判断吸烟和肺癌的因果性。为了判断因果性，可以考虑以下场景：找一群原先不吸烟的人，强制他们吸烟，观测患肺癌的概率。再找一群没有肺癌的人，让他们患肺癌，观测吸烟的概率。设<span class="math inline">\(X\)</span>: 吸烟，<span class="math inline">\(Y\)</span>：患肺癌，前一种情况下，患肺癌的概率高于肺癌在人群中的概率，<span class="math inline">\(P(Y=1|do(X=1))\neq P(Y=1)\)</span>，而后一种情况，吸烟的概率等于人群中吸烟的概率，<span class="math inline">\(P(X=1|do(Y=1))=P(X=1)\)</span>.这样的结果说明，吸烟是导致肺癌的原因，而肺癌不是导致吸烟的原因。这里的<span class="math inline">\(do\)</span>算子的含义是：我们控制了变量<span class="math inline">\(X\)</span>的值，而不引起任何隐变量的值的改变，否则，在因果DAG中必须将发生改变的隐变量作为变量节点。（原文表述：Itis worth noting here that we are making a tacit assumption that theintervention has no “side effects,” that is, that assigning the value xfor the variable X for an individual does not alter subsequent variablesin a direct way. For example, being “assigned” a drug might have adifferent effect on recovery than being forced to take the drug againstone’s religious objections. When side effects are present, they need tobe specified explicitly in the model. 这里的"directway"说的比较模糊）需要注意，<span class="math inline">\(P(Y|do(X=1))\not\equiv P(Y|X=1)\)</span>.当我们强制令<span class="math inline">\(X=1\)</span>时，<span class="math inline">\(X\)</span>的父节点对它就没有影响了，在因果DAG中<span class="math inline">\(X\)</span>的入边全部消失；而<span class="math inline">\(X\)</span>的值没有强制改变时，<span class="math inline">\(X\)</span>的父节点可以对<span class="math inline">\(X\)</span>及<span class="math inline">\(Y\)</span>同时造成影响。例如，设<span class="math inline">\(X\)</span>: 冰糕店销量增加，<span class="math inline">\(Y\)</span>: 犯罪率升高，<span class="math inline">\(Z\)</span>: 天气变热，这时存在因果关系<span class="math inline">\(X\leftarrow Z \rightarrowY\)</span>，因果DAG如下图</p><p><img src="7.PNG"></p><p>如果我们采取措施让<span class="math inline">\(X=0\)</span>，例如关闭所有冰糕店，那么<span class="math inline">\(Z\)</span>就无法再影响到<span class="math inline">\(X\)</span>，因果DAG变为下图</p><p><img src="8.PNG"></p><p>总之，干预一个变量等价于在因果DAG中让它的入边全部消失，不再作为果。所以仅当<span class="math inline">\(X\)</span>是孤儿节点时，有<span class="math inline">\(P(Y|do(X=1))= P(Y|X=1)\)</span></p><h4 id="干预与观测">干预与观测</h4><p>只要能得到<span class="math inline">\(P(Y=y|do(X=x))\)</span>，我们就可以得到完美的因果DAG了。上面的例子给出了判断因果性的方法，美中不足的是，现实中我们并没有可能进行这种实验。为表区分，将直接控制变量的这一种方式称为干预(Intervention).干预可以直接得到<span class="math inline">\(P(Y=y|do(X=x))\)</span>.退而求其次，我们能不能尝试从相关性数据中找出因果关系？前面提到，对于稳定数据，相关性可以反映因果关系。如果获得了稳定数据，我们可以通过观测(Observation)来判断因果关系的强弱，来近似地计算<span class="math inline">\(P(Y=y|do(X=x))\)</span>.</p><h4 id="平均因果作用average-casual-effect-ace">平均因果作用(Averagecasual effect, ACE)</h4><p>还要说明一点，因为隐变量的存在，我们从观测的数据中无法得到决定性的因果关系，而只能计算变量之间的因果性强弱。一种直接的想法是，控制<span class="math inline">\(X\)</span>的取值，在不同<span class="math inline">\(X\)</span>取值下观测<span class="math inline">\(Y\)</span>的值的差异，例如下面的例子</p><p><img src="6.PNG"></p><p>为了衡量<span class="math inline">\(X\)</span>对<span class="math inline">\(Y\)</span>的因果性强弱，考虑计算使用药物时的治愈概率与不使用药物时的治愈概率之差：<span class="math display">\[ACE=P(Y=1|do(X=1))-P(Y=1|do(X=0))\]</span> 下面尝试计算<span class="math inline">\(P(Y=y|do(X=x))\)</span>.</p><h4 id="校正公式adjustment-formula">校正公式(Adjustment Formula)</h4><p>使用药物的例子中，当<span class="math inline">\(X\)</span>的取值被控制后，因果DAG变为下图</p><p><img src="9.PNG"></p><p>把这张图称为受控模型(manipulated model)，受控模型中出现的概率用<span class="math inline">\(P_m(*)\)</span>表示（原来的因果DAG中出现的概率用<span class="math inline">\(P(*)\)</span>表示）。考虑更一般的情况，如何计算<span class="math inline">\(P(Y=y|do(X=x))\)</span>. 由于<span class="math inline">\(X\)</span>受控后成为孤儿节点，则有<span class="math inline">\(P(Y=y|do(X=x))= P_m(Y=y|X=x)\)</span>，因为<span class="math inline">\(X\)</span>与<span class="math inline">\(Z\)</span>在受控模型中独立，则有 <span class="math display">\[\begin{aligned}P_m(Y=y|X=x)&amp;=\frac{P_m(Y=y,X=x)}{P_m(X=x)}=\sum_{z}\frac{P_m(Y=y,X=x,Z=z)}{P_m(X=x)}\\&amp;=\sum_{z}\frac{P_m(Y=y,X=x,Z=z)P_m(X=x,Z=z)}{P_m(X=x)P_m(X=x,Z=z)}\\&amp;=\sum_{z}P_m(Y=y|X=x,Z=z)P_m(Z=z|X=x)\\&amp;=\sum_{z}P_m(Y=y|X=x,Z=z)P_m(Z=z)\end{aligned}\]</span> 现在<span class="math inline">\(do\)</span>算子已经被消去，受控模型的使命到此为止。因为在原来的因果DAG与受控模型中，<span class="math inline">\(Y\)</span>与变量<span class="math inline">\(X,Z\)</span>的因果关系是相同的，即均有<span class="math inline">\(Y=f(X,Z,U_Y)\)</span>，因此<span class="math inline">\(P_m(Y=y|X=x,Z=z)=P(Y=y|X=x,Z=z)\)</span>, <span class="math inline">\(Z\)</span>在两图中均是因，即均有<span class="math inline">\(Z=g(U_Z)\)</span>, 因此<span class="math inline">\(P_m(Z=z)=P(Z=z)\)</span>. (在受控模型里只有<span class="math inline">\(X\)</span>的函数关系被改变，在原来的因果DAG中，<span class="math inline">\(X=h(Z,U_X)\)</span>，在受控模型中，<span class="math inline">\(X=h_m(U_X)\)</span> )。于是有 <span class="math display">\[P(Y=y|do(X=x))=\sum_{z}P(Y=y|X=x,Z=z)P(Z=z)\]</span> 上式即为在<span class="math inline">\(X\)</span>受控时关于<span class="math inline">\(Z\)</span>的校正公式。</p><p>现在考虑Simpson问题，设<span class="math inline">\(X\)</span>:使用药物，<span class="math inline">\(Y\)</span>: 治愈，<span class="math inline">\(Z\)</span>:性别，则因果DAG如平均因果作用一节所示。 <span class="math display">\[\begin{aligned}P(Y=1|do(X=1))&amp;=P(Y=1|X=1,Z=men)P(Z=men)+P(Y=1|X=1,Z=women)P(Z=women)\\&amp;=0.93\times \frac{87+270}{700}+0.73\times\frac{263+80}{700}=0.832\\P(Y=1|do(X=0))&amp;=P(Y=1|X=0,Z=men)P(Z=men)+P(Y=1|X=0,Z=women)P(Z=women)\\&amp;=0.87\times \frac{87+270}{700}+0.69\times \frac{263+80}{700}=0.7818\end{aligned}\]</span> 于是<span class="math inline">\(X\)</span>对<span class="math inline">\(Y=1\)</span>的平均因果作用为 <span class="math display">\[ACE=P(Y=1|do(X=1))-P(Y=0|do(X=1))=0.0502\]</span></p><h4 id="因果作用定理causal-effect-rule">因果作用定理(Causal EffectRule)</h4><p>校正公式给出了一种情况下的<span class="math inline">\(do\)</span>算子消去计算方式，而对于更一般的情况，同样借助受控模型，在受控模型中，有两种节点与<span class="math inline">\(X\)</span>独立，一种是原本就与<span class="math inline">\(X\)</span>独立的，另一种是<span class="math inline">\(X\)</span>的父节点<span class="math inline">\(PA(X)\)</span>. 对于前一种情况，如果<span class="math inline">\(Z\)</span>与<span class="math inline">\(X\)</span>独立，也就是因果关系为<span class="math inline">\(X\rightarrow Y \leftarrowZ\)</span>，那么原来的DAG和受控模型相同，有<span class="math inline">\(P(Y|do(X=x))=P_m(Y|X=x)=P(Y|X=x)=\sum_{z}P(Y|X=x,Z=z)P(Z=z)\)</span>.这说明了为什么下面的推导无需引入受控模型中所有与<span class="math inline">\(X\)</span>独立的变量。当只引入<span class="math inline">\(X\)</span>的父节点时，与贝叶斯网络的计算保持一致。<span class="math display">\[\begin{aligned}P(Y=y|do(X=x))&amp;=\frac{P_m(Y=y,X=x)}{P_m(X=x)}\\&amp;=\sum_{z}\frac{P_m(Y=y,X=x,PA(X)=z)}{P_m(X=x)}\\&amp;=\sum_{z}\frac{P_m(Y=y,X=x,PA(X)=z)P_m(X=x,PA(X)=z)}{P_m(X=x)P_m(X=x,PA(X)=z)}\\&amp;=\sum_{z}P_m(Y=y|X=x,PA(X)=z)P_m(PA(X)=z|X=x)\\&amp;=\sum_z P(Y=y|X=x,PA(X)=z)P(PA(X)=z)\\&amp;=\sum_z \frac{P(X=x,Y=y,PA(X)=z)}{P(X=x|PA(X)=z)}\end{aligned}\tag{2}\]</span></p><h4 id="后门准则backdoor-criterion">后门准则(Backdoor Criterion)</h4><p>观察上式的推导过程，核心点是：<span class="math inline">\(P_m(Y=y|X=x,PA(X)=z)=P(Y=y|X=x,PA(X)=z)\)</span>,<span class="math inline">\(P_m(PA(X)=z|X=x)=P_m(PA(X)=z)=P(PA(X)=z)\)</span>,</p><p>第一个等式成立的条件是在原来DAG和受控模型中，固定<span class="math inline">\(X,PA(X)\)</span>后<span class="math inline">\(Y\)</span>的概率分布相同，因为受控模型只消除了<span class="math inline">\(X\)</span>的入边，因此<span class="math inline">\(X,Y\)</span>之间存在的共因节点的值固定后，<span class="math inline">\(Y\)</span>的取值将只受<span class="math inline">\(X\)</span>和与<span class="math inline">\(X\)</span>独立的节点影响。例如，假设存在如下因果关系：</p><p><img src="10.PNG"></p><p>有 <span class="math display">\[\begin{aligned}P(Y=y|X=x,Z=z)&amp;=\frac{P(Y=y,X=x,Z=z)}{P(X=x,Z=z)}\\&amp;=\frac{P(f(X,Z,W,U_Y)=y,X=x,Z=z)}{P(X=x,Z=z)}\\&amp;=\frac{P(W=u_w,U_Y=u_y,X=x,Z=z)}{P(X=x,Z=z)}\end{aligned}\tag{3}\]</span> 其中<span class="math inline">\(u_w,u_y\)</span>是所有满足<span class="math inline">\(f(x,z,W,U_Y)=y\)</span>的<span class="math inline">\(W,U_Y\)</span>取值的集合，因为<span class="math inline">\(W\)</span>与<span class="math inline">\(X\)</span>不独立，所以不能得到<span class="math inline">\(P(W=u_w,U_Y=u_y,X=x,Z=z)=P(W=u_w)P(U_Y=u_y)P(X=x,Z=z)\)</span>，而<span class="math display">\[\begin{aligned}P_m(Y=y|X=x,Z=z)&amp;=\frac{P_m(X=x,Z=z,W=u_w,U_Y=u_y)}{P_m(X=x,Z=z)}\\&amp;=\frac{P_m(X=x)P_m(Z=z)P_m(W=u_w)P_m(U_Y=u_y)}{P_m(X=x)P_m(Z=z)}\\&amp;=P_m(W=u_w)P_m(U_Y=u_y)\end{aligned}\tag{4}\]</span> 观察(3)(4)的区别，可见如果(3)式想要消去分母，就要让<span class="math inline">\(W,X,Z\)</span>同时被消去，即可以得到 <span class="math display">\[\begin{aligned}P(Y=y|X=x,Z=z,W=w)&amp;=\frac{P(W=w,U_Y=u_y,X=x,Z=z)}{P(X=x,Z=z,W=w)}\\&amp;=P(U_Y=u_y)\end{aligned}\]</span> 另一方面 <span class="math display">\[\begin{aligned}P_m(Y=y|X=x,Z=z,W=w)&amp;=\frac{P_m(X=x,Z=z,W=w,U_Y=u_y)}{P_m(X=x,Z=z,W=w)}\\&amp;=P_m(U_Y=u_y)\\&amp;=P(Y=y|X=x,Z=z,W=w)\end{aligned}\]</span> 因此，只有把所有的共因节点的值全部固定，才能得到<span class="math inline">\(P_m(Y=y|X=x,Z=z,W=w)=P(Y=y|X=x,Z=z,W=w)\)</span>.</p><p>第二个等式成立的条件是受控模型中，<span class="math inline">\(PA(X)\)</span>与<span class="math inline">\(X\)</span>独立。那么，对于受控模型中任何与<span class="math inline">\(X\)</span>独立的节点的集合都可以替换上面推导的<span class="math inline">\(PA(X)\)</span>. 因此，当<span class="math inline">\(X\)</span>的父节点难以找出时，根据上面的两个条件，就可以找到替换父节点作为计算的节点，并且得到相同的结果。这称为后门准则，如(5)式。<span class="math display">\[P(Y=y|do(X=x))=\sum_zP(Y=y|X=x,W=w)P(W=w)\tag{5}\]</span> 这样的节点集合<span class="math inline">\(W\)</span>满足以下两个条件：</p><ol type="1"><li><span class="math inline">\(W\)</span>不在<span class="math inline">\(X\)</span>的后代节点中；</li><li><span class="math inline">\(W\)</span>阻断了<span class="math inline">\(X\)</span>与<span class="math inline">\(Y\)</span>之间指向<span class="math inline">\(X\)</span>的路径</li></ol><p>这两个条件与上面的推导得到的结论是等价的，即：1.<span class="math inline">\(W\)</span>包含了所有共因节点； 2.受控模型中<span class="math inline">\(X\)</span>与<span class="math inline">\(W\)</span>中的节点独立。</p>]]></content>
    
    
    <categories>
      
      <category>ML</category>
      
      <category>Casual Inference</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Machine Learning</tag>
      
      <tag>Casual Inference</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Book of Why (Extract)</title>
    <link href="/2023/09/10/TBOW/"/>
    <url>/2023/09/10/TBOW/</url>
    
    <content type="html"><![CDATA[<p>​ I was probably six or seven years old when I first read the story ofAdam and Eve in the Garden of Eden. My classmates and I were not at allsurprised by God’s capricious demands, forbidding them to eat from theTree of Knowledge. Deities have their reasons, we thought. What we weremore intrigued by was the idea that as soon as they ate from the Tree ofKnowledge, Adam and Eve became conscious, like us, of their nakedness.As teenagers, our interest shifted slowly to the more philosophicalaspects of the story. (Israeli students read Genesis several times ayear.) Of primary concern to us was the notion that the emergence ofhuman knowledge was not a joyful process but a painful one, accompaniedby disobedience, guilt, and punishment. Was it worth giving up thecarefree life of Eden? some asked. Were the agricultural and scientificrevolutions that followed worth the economic hardships, wars, and socialinjustices that modern life entails?</p><p>​ Don’t get me wrong: we were no creationists; even our teachers wereDarwinists at heart. We knew, however, that the author who choreographedthe story of Genesis struggled to answer the most pressing philosophicalquestions of his time. We likewise suspected that this story bore thecultural footprints of the actual process by which Homo sapiens gaineddominion over our planet. What, then, was the sequence of steps in thisspeedy, super-evolutionary process?</p><p>​ My interest in these questions waned in my early career as aprofessor of engineering but was reignited suddenly in the 1990s, when,while writing my book Causality, I confronted the Ladder of Causation.As I reread Genesis for the hundredth time, I noticed a nuance that hadsomehow eluded my attention for all those years. When God finds Adamhiding in the garden, he asks, “Have you eaten from the tree which Iforbade you?” And Adam answers, “The woman you gave me for a companion,she gave me fruit from the tree and I ate.” “What is this you havedone?” God asks Eve. She replies, “The serpent deceived me, and Iate.”</p><p>​ As we know, this blame game did not work very well on the Almighty,who banished both of them from the garden. But here is the point I hadmissed before: God asked “what,” and they answered “why.” God asked forthe facts, and they replied with explanations. Moreover, both werethoroughly convinced that naming causes would somehow paint theiractions in a different light. Where did they get this idea?</p><p>​ For me, these nuances carried three profound implications. First,very early in our evolution, we humans realized that the world is notmade up only of dry facts (what we might call data today); rather, thesefacts are glued together by an intricate web of cause-effectrelationships. Second, causal explanations, not dry facts, make up thebulk of our knowledge, and should be the cornerstone of machineintelligence. Finally, our transition from processors of data to makersof explanations was not gradual; it was a leap that required an externalpush from an uncommon fruit. This matched perfectly with what I hadobserved theoretically in the Ladder of Causation: No machine can deriveexplanations from raw data. It needs a push.</p><p>​ If we seek confirmation of these messages from evolutionary science,we won’t find the Tree of Knowledge, of course, but we still see a majorunexplained transition. We understand now that humans evolved fromapelike ancestors over a period of 5 million to 6 million years and thatsuch gradual evolutionary processes are not uncommon to life on earth.But in roughly the last 50,000 years, something unique happened, whichsome call the Cognitive Revolution and others (with a touch of irony)call the Great Leap Forward. Humans acquired the ability to modify theirenvironment and their own abilities at a dramatically faster rate. Forexample, over millions of years, eagles and owls have evolved trulyamazing eyesight—yet they’ve never devised eyeglasses, microscopes,telescopes, or night-vision goggles. Humans have produced these miraclesin a matter of centuries. I call this phenomenon the “super-evolutionaryspeedup.” Some readers might object to my comparing apples and oranges,evolution to engineering, but that is exactly my point.</p><p>​ Evolution has endowed us with the ability to engineer our lives, agift she has not bestowed on eagles and owls, and the question, again,is “Why?” What computational facility did humans suddenly acquire thateagles did not? Many theories have been proposed, but one is especiallypertinent to the idea of causation. In his book Sapiens, historian YuvalHarari posits that our ancestors’ capacity to imagine nonexistent thingswas the key to everything, for it allowed them to communicate better.Before this change, they could only trust people from their immediatefamily or tribe. Afterward their trust extended to larger communities,bound by common fantasies (for example, belief in invisible yetimaginable deities, in the afterlife, and in the divinity of the leader)and expectations. Whether or not you agree with Harari’s theory, theconnection between imagining and causal relations is almostself-evident. It is useless to ask for the causes of things unless youcan imagine their consequences. Conversely, you cannot claim that Evecaused you to eat from the tree unless you can imagine a world in which,counter to facts, she did not hand you the apple.</p><p>​ Back to our Homo sapiens ancestors: their newly acquired causalimagination enabled them to do many things more efficiently through atricky process we call “planning.” Imagine a tribe preparing for amammoth hunt. What would it take for them to succeed? My mammoth-huntingskills are rusty, I must admit, but as a student of thinking machines, Ihave learned one thing: a thinking entity (computer, caveman, orprofessor) can only accomplish a task of such magnitude by planningthings in advance—by deciding how many hunters to recruit; by gauging,given wind conditions, the direction from which to approach the mammoth;in short, by imagining and comparing the consequences of several huntingstrategies. To do this, the thinking entity must possess, consult, andmanipulate a mental model of its reality. Note that there are multiplecauses for the chances of success and that none of them aredeterministic. That is, we cannot be sure that having more hunters willenable success or that rain will prevent it, but these factors do changethe probability of success.</p><p>​ The mental model is the arena where imagination takes place. Itenables us to experiment with different scenarios by making localalterations to the model. Somewhere in our hunters’ mental model was asubroutine that evaluated the effect of the number of hunters. When theyconsidered adding more, they didn’t have to evaluate every other factorfrom scratch. They could make a local change to the model, replacing“Hunters = 8” with “Hunters = 9,” and reevaluate the probability ofsuccess. This modularity is a key feature of causal models. I don’t meanto imply, of course, that early humans actually drew a pictorial modellike this one. But when we seek to emulate human thought on a computer,or indeed when we try to solve unfamiliar scientific problems, drawingan explicit dots-and-arrows picture is extremely useful. These causaldiagrams are the computational core of the “causal inference engine”.</p>]]></content>
    
    
    <categories>
      
      <category>Literature</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Judea Pearl</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>傅里叶变换</title>
    <link href="/2023/07/08/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/"/>
    <url>/2023/07/08/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/</url>
    
    <content type="html"><![CDATA[<hr><h1 id="傅里叶变换">傅里叶变换</h1><p>​任意函数可以表示为一系列三角函数的和。通过傅里叶变换找出这些三角函数。实际应用上，即使不知道函数的表达式，也可以通过傅里叶变换将其转换为频率函数，以获取原函数的性质。</p><span id="more"></span><h2 id="正交函数系">正交函数系</h2><p><span class="math display">\[\forall i,j\in N,s.t.\int_{a}^{b} f_i(x)f_j(x)dx=\left\{\begin{array}{l}0,\quad i\neq j \\1,\quad i=j \\\end{array}\right.\]</span></p><p>则所有<span class="math inline">\(f_i(x)\)</span>构成正交函数系。</p><p>结论：在一定区间<span class="math inline">\((a,b)\)</span>上，正交函数系可以表出任意函数。根据这个结论，可以将任意函数转换为许多带系数正交函数的和，即：<span class="math display">\[\forall f(x),\exists{a_k},f_{m_k}(x),s.t.f(x)=\sum_{k=1}^{n}a_kf_{m_k}(x)\]</span> 那么，对于具体的<span class="math inline">\(f(x)\)</span>，是否可以找出这些<span class="math inline">\(a_k\)</span>和<span class="math inline">\(f_{m_k}(x)\)</span>?这就是傅里叶变换要做的事。</p><p>上述结论证明：</p><p>希望找到一组<span class="math inline">\(a_k,f_{m_k}(x)\)</span>，使得<span class="math inline">\(f(x)=\sum_{k=1}^{n}a_kf_{m_k}(x)\)</span>，因此，考虑其中一个正交函数<span class="math inline">\(f_{m_n}(x)\)</span>，令<span class="math inline">\(f^{(1)}(x)=f(x)-a_nf_{m_n}(x)\)</span>，如果能证明<span class="math display">\[\int_{a}^{b} [f^{(1)}(x)]^2dx&lt;\int_{a}^{b} [f(x)]^2dx\tag{1}\]</span> 则说明<span class="math inline">\(f^{(1)}(x)\)</span>比<span class="math inline">\(f(x)\)</span>更贴近x轴。同样的，可以证明 <span class="math display">\[0=\int_{a}^{b} [f^{(n)}(x)]^2dx&lt;...&lt;\int_{a}^{b}[f^{(1)}(x)]^2dx&lt;\int_{a}^{b} [f(x)]^2dx\tag{2}\]</span>即原函数每次减去某个正交函数后，都会得到一个更贴近x轴的函数，直到缩减到0.而<span class="math display">\[\begin{aligned}&amp;\int_{a}^{b} [f^{(1)}(x)]^2dx&lt;\int_{a}^{b} [f(x)]^2dx\\&amp;\Leftarrow \int_{a}^{b} [f(x)]^2dx-\int_{a}^{b}2a_nf(x)f_{m_n}(x)dx+\int_{a}^{b} [a_nf_{m_n}(x)]^2dx&lt;\int_{a}^{b}[f(x)]^2dx\\&amp;\Leftarrow a_n&lt;\int_{a}^{b} 2f(x)f_{m_n}(x)dx\end{aligned}\]</span> 由于<span class="math inline">\(a_n\)</span>为常数，于是必定存在满足条件的<span class="math inline">\(a_n\)</span>，使得式(1)能成立，同理可证明式(2).</p><p>在傅里叶变换中用三角函数系 <span class="math display">\[\{ sinmx,cosmx|m\in N\}\]</span> 容易证明三角函数系满足正交函数系的条件.</p><h2 id="傅里叶级数">傅里叶级数</h2><p>​由上一节的结论，可知在一定区间上，正交函数系可以表出任意函数。选用三角函数系的原因在于便于找出这些正交函数的系数。换句话说，正交函数系是无穷多的，我们只用便于计算的。傅里叶级数就是把任意函数表示成确定系数的三角函数的和。</p><p>任意函数<span class="math inline">\(f(x)\)</span>用三角函数系表示为：<span class="math display">\[f(x)=\sum_{k=0}^{\infty}(a_ksinkx+b_kcoskx)\tag{3}\]</span> 要计算出<span class="math inline">\(a_k,b_k\)</span>，用正交函数的积分性质： <span class="math display">\[\int_{-\pi}^{\pi} f(x)sinkxdx=\int_{-\pi}^{\pi}a_ksin^2kxdx=\pia_k\tag{4}\\\int_{-\pi}^{\pi} f(x)coskxdx=\int_{-\pi}^{\pi}a_kcos^2kxdx=\pi b_k\]</span> 注意在三角函数系中只有在周期<span class="math inline">\(2\pi\)</span>上才是正交函数系。为此，希望将周期扩展到任意周期<span class="math inline">\(2L\)</span>，这样，当<span class="math inline">\(L\rightarrow\infty\)</span>，则可将任意函数用三角函数系表出。令 <span class="math display">\[t=\frac{Lx}{\pi}\]</span> 于是式(3)转化为： <span class="math display">\[g(t)=f(x)=f(\frac{\pi t}{L})=\sum_{k=0}^{\infty}(a_ksin\frac{k\pit}{L}+b_kcos\frac{k\pi t}{L})\tag{5}\]</span> 将式(4)中所有<span class="math inline">\(f(*)\)</span>换成<span class="math inline">\(g(*)\)</span> <span class="math display">\[a_k=\frac{1}{L}\int_{-L}^{L} g(t)sin\frac{k\pi t}{L}dt\\b_k=\frac{1}{L}\int_{-L}^{L} g(t)cos\frac{k\pi t}{L}dt\tag{6}\]</span></p><h2 id="频率函数">频率函数</h2><p>​上一节的结论说明任意一个函数可以表示成一系列三角函数之和，为方便表示，令<span class="math inline">\(w_0=\frac{\pi}{L}\)</span>，当<span class="math inline">\(L\rightarrow \infty\)</span>，<span class="math inline">\(w_0\rightarrow 0\)</span>，则<span class="math inline">\(kw_0,(k\in N)\)</span>连续。则 <span class="math display">\[\begin{aligned}g(t)&amp;=\sum_{k=0}^{\infty}(a_ksin\frac{k\pi t}{L}+b_kcos\frac{k\pit}{L})\\&amp;=\sum_{k=0}^{\infty}[(sin\frac{k\pi t}{L})\frac{1}{L}\int_{-L}^{L}g(t)sin\frac{k\pi t}{L}dt+(cos\frac{k\pi t}{L})\frac{1}{L}\int_{-L}^{L}g(t)cos\frac{k\pi t}{L}dt]\\&amp;=\lim_{w_0 \to0}\sum_{k=0}^{\infty}\frac{w_0}{\pi}[sinkw_0t\int_{-\frac{\pi}{w_0}}^{\frac{\pi}{w_0}}g(t)sin(kw_0t)dt+coskw_0t\int_{-\frac{\pi}{w_0}}^{\frac{\pi}{w_0}}g(t)cos(kw_0t)dt]\end{aligned}\tag{7}\]</span>根据积分定义，大区间分成的所有同阶无穷小区间上的任意一点（以下为取小区间左点）函数值的累和为该大区间上的积分：<span class="math display">\[\lim_{\Delta x\to 0,n\to \infty}\sum_{i=0}^{n-1}f(\Delta xi)\Deltax=\int_{0}^{\infty}f(x)dx\tag{8}\]</span> 在式(7)中，注意到<span class="math inline">\(w_0\)</span>为无穷小量，可以将其化为对变量<span class="math inline">\(w\)</span>的积分，因此，(7)(8)有以下对应关系：<span class="math display">\[w_0 \iff \Delta x,f(\Delta xi)\iff\frac{1}{\pi}[sinkw_0t\int_{-\frac{\pi}{w_0}}^{\frac{\pi}{w_0}}g(t)sin(kw_0t)dt+coskw_0t\int_{-\frac{\pi}{w_0}}^{\frac{\pi}{w_0}}g(t)cos(kw_0t)dt]\]</span> 进一步可以得出<span class="math inline">\(f(\Deltaxi)\)</span>的一般形式： <span class="math display">\[f(w)=\frac{1}{\pi}[sinwt\int_{-\infty}^{\infty}g(t)sin(wt)dt+coswt\int_{-\infty}^{\infty} g(t)cos(wt)dt]\]</span> 注意到式(7)中的积分 <span class="math display">\[\int_{-\frac{\pi}{w_0}}^{\frac{\pi}{w_0}} g(t)sin(kw_0t)dt\]</span> 在给定<span class="math inline">\(kw_0\)</span>情况下是一个数值，即在实数范围<span class="math inline">\(R\)</span>上对<span class="math inline">\(t\)</span>的积分，积分上下限即为<span class="math inline">\((-\infty,\infty)\)</span></p><p>于是得到傅里叶变换的一种形式 <span class="math display">\[\begin{aligned}g(t)&amp;=\int_{0}^{\infty} f(w)dw\\f(w)&amp;=\frac{1}{\pi}[sinwt\int_{-\infty}^{\infty}g(t)sin(wt)dt+coswt\int_{-\infty}^{\infty} g(t)cos(wt)dt]\end{aligned}\tag{9}\]</span> 上面的<span class="math inline">\(w\)</span>是三角函数的频率，这组公式揭示了：任意一个函数<span class="math inline">\(g(t)\)</span>等价于无穷多组三角函数（无初相）的和，这些三角函数的频率由第二个公式确定，得到一个频率函数<span class="math inline">\(f(w)\)</span>。根据这些三角函数的频率函数<span class="math inline">\(f(w)\)</span>同样能得到原函数<span class="math inline">\(g(t)\)</span></p><h2 id="一般形式">一般形式</h2><p>在常见的傅里叶变换公式中，一般用如下形式： <span class="math display">\[F(w)=\int_{-\infty}^{\infty} f(t)e^{-iwt}dt\\f(t)=\frac{1}{2\pi}\int_{-\infty}^{\infty}F(w)e^{iwt}dw\tag{10}\]</span>式(10)实际上与式(9)等价。式(10)更简洁，将三角函数用欧拉公式转换成指数形式，且更易计算。下面从(9)推到(10):</p><p>用欧拉公式 <span class="math display">\[e^{i\theta}=cos\theta+isin\theta\]</span> 于是 <span class="math display">\[cos\theta=\frac{e^{i\theta}+e^{-i\theta}}{2}\\sin\theta=-i\frac{e^{i\theta}-e^{-i\theta}}{2}\]</span> 带入(9)中<span class="math inline">\(f(w)\)</span> <span class="math display">\[\begin{aligned}f(w)&amp;=\frac{1}{\pi}[-i\frac{e^{iwt}-e^{-iwt} }{2}\int_{-\infty}^{\infty} g(t)(-i\frac{e^{iwt}-e^{-iwt}}{2})dt+\frac{e^{iwt}+e^{-iwt}}{2}\int_{-\infty}^{\infty}g(t)\frac{e^{iwt}+e^{-iwt} }{2}dt]\\&amp;=\frac{1}{\pi}[-(e^{iwt}-e^{-iwt} ) \int_{-\infty}^{\infty}g(t)(\frac{e^{iwt}-e^{-iwt}}{2})dt+(e^{iwt}+e^{-iwt})\int_{-\infty}^{\infty}g(t)\frac{e^{iwt}+e^{-iwt} }{2}dt]\\&amp;=\frac{1}{\pi}[e^{iwt} \int_{-\infty}^{\infty} g(t)\frac{e^{-iwt}}{2}dt+e^{-iwt} \int_{-\infty}^{\infty} g(t)\frac{e^{iwt} }{2}dt]\\&amp;=\frac{e^{iwt}+e^{-iwt} }{2\pi}\int_{-\infty}^{\infty}g(t)e^{-iwt}dt\end{aligned}\]</span> 令 <span class="math display">\[F(w)=\int_{-\infty}^{\infty} g(t)e^{-iwt}dt\]</span> 注意<span class="math inline">\(F(w)=F(-w)\)</span>，于是<span class="math display">\[\begin{aligned}g(t)&amp;=\int_{0}^{\infty} f(w)dw\\&amp;= \int_{0}^{\infty}\frac{e^{iwt}+e^{-iwt} }{2\pi} F(w)dw\\&amp;=\frac{1}{2\pi}[\int_{0}^{\infty}  F(w)e^{iwt}dw+\int_{0}^{\infty}F(w)e^{-iwt}dw]\\&amp;=\frac{1}{2\pi}[\int_{0}^{\infty}  F(w)e^{iwt}dw+\int_{-\infty}^{0}F(w)e^{iwt}dw]\\&amp;=\frac{1 }{2\pi}\int_{-\infty}^{\infty} F(w)e^{iwt}dw\end{aligned}\]</span> 即得到一般形式傅里叶变换</p>]]></content>
    
    
    <categories>
      
      <category>Math</category>
      
      <category>Calculus</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>混乱的机器学习</title>
    <link href="/2023/07/08/%E6%B7%B7%E4%B9%B1%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <url>/2023/07/08/%E6%B7%B7%E4%B9%B1%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<hr><span id="more"></span><h1 id="概述">概述</h1><h2 id="no-free-lunch">No Free Lunch</h2><p>​NFL告诉我们，一个算法不可能在所有数据上完胜另一个算法。从现在的评估手段来看，这个结论显而易见的，假设算法A在数据集D=（特征列X，类别列Y）上的准确率超过了算法B，那么我们只要在D的类别列Y上动手脚，把原来D的类别以某种顺序重新排列得到新的数据集D‘，最终能得到B的准确率超过A（A更倾向于把符合某些特征的样本分类成正确的，而我们把这部分A的优势样本的类故意改成错的）。实验角度来看，在UCIHeart数据集上，多少层深度网络都不是经典决策树的对手。在Six-HumpCamel-BackFunction上，一众新型动物园算法输给了经典遗传算法。（这两个问题都会在后面详细阐述原因）然而，正因NFL的存在，贡献了ML领域大部分论文😓</p><h2 id="方向">方向？</h2><p>​既然NFL存在，那么我们就不能奢求一个更好的算法了吗？其实，NFL的前提是依据一些评估指标去评估分类效果，在这个前提下，不存在最优算法。NFL的证明中假设目标函数是服从均匀分布的随机变量。然而，是否因为存在这样的数据集，就无法比较算法的优劣？机器学习尝试从数据中找规律，数据特征列与目标列并不是独立的，这样的数据才是有分析意义的。在这个角度考虑，算法就存在优劣。例如，实验经验告诉我们，对于时序相关数据，循环网络通常优于全连接网络。但是，关于算法优劣比较目前的定义仍旧十分模糊。下面讨论各个主流方向存在的问题，并给出我的一些偏见。</p><h1 id="内分析">内分析</h1><p>​一些算法试图分析数据的一些特点，另一些则首先建立起一个模型，通过一些手段让这个模型尽可能贴合数据。这节与下一节就讨论这两种不同出发点的算法。这节讨论前一种。</p><h2 id="信息与噪音">信息与噪音</h2><p>​现在公认的公理：数据=信息+噪音。有效的数据就是信息，无效的数据就是噪音。然而，信息和噪音的界限在哪里？如何剔除数据中的噪音，然后只对信息进行？这个问题是至关重要的，可以说，解决了这个问题，所有问题就可以解决了，我们可以让算法达到完美的分类回归效果。遗憾的是，这个问题至今没有一种完美的解决办法。仅有一些降低噪音的方法（后面会提到，这些方法是否真的有效）</p><h2 id="信息熵">信息熵</h2><p>​ 有一些算法希望解决这个问题，例如信息熵 <span class="math display">\[H(X)=-\sum_{i=1}^{n}p_ilogp_i,\quad \sum_{i=1}^{n}p_i=1\]</span> ​这个定义有个好处，把信息和概率联系起来了。把每一种情况的信息量的数学期望作为信息熵来考虑，符合信息的可加性公理，也就是<span class="math display">\[I(A)+I(B)=-logp_A+(-logp_B)=-logp_Ap_B\]</span> ​ 其中<span class="math inline">\(A,B\)</span>是两个独立随机事件，<span class="math inline">\(I(A),I(B)\)</span>是<span class="math inline">\(A,B\)</span>的信息量。同样地可以计算在某个条件下所有随机事件的信息熵的数学期望，简称条件熵。用原始信息熵-条件熵，也就得到了这个特征带来的信息增益：<span class="math display">\[特征C带来的信息增益=H(原始数据集的目标Y)-\sum_{特征C的每种情况c_i}H(满足C=c_i的子数据集的目标Y_i)\timesc_i出现的概率p_i\]</span> ​在ID3算法中采用了信息增益来衡量特征的重要性，这是一个很好的想法，显然这符合另一条公理：独立事件带来的信息量是可加的。然而传统信息熵存在两个严重缺陷：</p><ol type="1"><li><p>它是离散的；</p></li><li><p>信息熵函数本身随n增大趋于0，这样类别数不同的信息熵的量纲也不同，取值越多的特征信息增益相对偏大。</p><p>为此，ID3的升级版C4.5算法则采用了信息增益比，用某特征的信息增益÷该特征的信息熵=该特征的信息增益比。例如，设<span class="math inline">\(C\)</span>有<span class="math inline">\(k\)</span>个取值，<span class="math inline">\(Y\)</span>有<span class="math inline">\(n\)</span>个取值，那么<span class="math inline">\(C\)</span>的信息增益比的期望为：</p></li></ol><p><span class="math display">\[E(C的信息增益比)=E(\frac{H(Y)-\sum_{i=1}^{k}H(Y_i|C=c_i)P(C=c_i)}{H(C)})\\=E(\frac{-\sum_{j=1}^nP(Y=y_j)logP(Y=y_j)+\sum_{i=1}^k\sum_{j=1}^nP(C=c_i)P(Y=y_j,C=c_i)logP(Y=y_j,C=c_i)}{-\sum_{i=1}^kP(C=c_i)logP(C=c_i)})\\=\]</span></p><h2 id="相对熵">相对熵</h2><h1 id="外分析">外分析</h1><h2 id="按图索骥">按图索骥</h2><p>​神经网络通过最小化损失函数来使预先建立的模型拟合数据，但是，每个结构不同的神经网络都拟合出了一个与众不同的函数，而符合数据集的实际上只有一个。哪怕损失可以降到0，最终仍然难以得到数据集中蕴含的函数关系。道理很简单，有限的点可以符合无穷多个分布，损失小的神经网络拟合出的函数也只是无穷多个分布里的沧海一粟，于是这时奥卡姆剃刀就派上用场了。然而损失小的神经网络里面难道最简单的那个就最符合数据？显然不是，只是方便计算。另外，一般的数据都不会有过于复杂的关系，用梯度下降容易拟合多项式级别的关系，这也够应付大多数数据了。但对于有多个局部极值点的函数，用梯度下降就无法达到很好的拟合效果，比如像Generalized_rastrigin这种函数，示例代码如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generalized_rastrigin</span>(<span class="hljs-params">x</span>):<br>    A = <span class="hljs-number">10</span><br>    n = <span class="hljs-built_in">len</span>(x)<br>    <span class="hljs-keyword">return</span> A * n + np.<span class="hljs-built_in">sum</span>(x**<span class="hljs-number">2</span> - A * np.cos(<span class="hljs-number">2</span> * np.pi * x))<br><br>np.random.seed(<span class="hljs-number">0</span>)<br>num_samples = <span class="hljs-number">2000</span><br>input_dim = <span class="hljs-number">30</span>  <br>X_train = np.random.rand(num_samples, input_dim)<br>Y_train = np.array([generalized_rastrigin(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X_train])<br><br>model = tf.keras.Sequential([<br>    tf.keras.layers.Dense(<span class="hljs-number">640</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>, input_dim=input_dim),<br>    tf.keras.layers.Dense(<span class="hljs-number">640</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    tf.keras.layers.Dense(<span class="hljs-number">1</span>) <br>])<br>model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, loss=<span class="hljs-string">&#x27;mean_squared_error&#x27;</span>)<br>history = model.fit(X_train, Y_train, epochs=<span class="hljs-number">500</span>, batch_size=<span class="hljs-number">32</span>, verbose=<span class="hljs-number">1</span>)<br>X_test = np.random.rand(<span class="hljs-number">100</span>, input_dim)<br>Y_test = np.array([generalized_rastrigin(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X_test])<br>Y_pred = model.predict(X_test)<br><br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>))<br>plt.scatter(Y_test, Y_pred)<br>plt.xlabel(<span class="hljs-string">&#x27;Y_test&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Y_pred&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Y_test vs. Y_pred&#x27;</span>)<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>结果如下图所示。</p><p><img src="1.png"></p><h2 id="补救">补救</h2><h3 id="如何初始化">如何初始化？</h3><h4 id="maml">MAML</h4><p>​借用集成法，多创建几个神经网络，把它们训练后的参数汇总起来，再作为另一个网络的初始参数，能从一定程度上避免这个问题。</p><h4 id="群优化初始参数">群优化初始参数</h4><p>​既然神经网络容易陷入局部最优，初始参数又影响神经网络效果，那用能跳出局部最优的启发式算法搜索一组初始化参数，再用梯度下降。</p><h3 id="可解释性">可解释性？</h3>]]></content>
    
    
    <categories>
      
      <category>ML</category>
      
      <category>Summary</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>矩阵论</title>
    <link href="/2023/04/27/%E7%9F%A9%E9%98%B5%E8%AE%BA/"/>
    <url>/2023/04/27/%E7%9F%A9%E9%98%B5%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<span id="more"></span><h1 id="矩阵导数"><a href="#矩阵导数" class="headerlink" title="矩阵导数"></a>矩阵导数</h1><p>​矩阵导数即对矩阵的多个元素求导，作为一种简便的计算方式。在下面的计算过程中，标量，向量均作为特殊的矩阵，如无指出是标量或向量，矩阵的含义均包括了是标量，向量，矩阵的情况。在矩阵导数中，因变量矩阵的每个元素与自变量矩阵中的每个元素均存在映射。</p><p>​在可能混淆的地方，小写字母为标量，加粗或希腊字母为向量，大写字母为矩阵。在函数中用$y,Y$做自变量，$f$表示函数关系，$dy,dY$表示全微分，$\frac{\partial f}{\partial x_i}$表示$y$对$x_i$的偏导数。将矩阵每个元素视为自变量，要求因变量（标量或矩阵）对该矩阵每个元素的导数构成的矩阵。在不引起混淆的情况下，使用$f’(X)$作为$\frac{\partial f(X)}{\partial X}$的简写形式。</p><p>​在形如$Y&#x3D;f(X_{m\times n})$的矩阵运算中，自变量矩阵$X$每个元素与因变量矩阵$Y$存在映射，因此$Y$为1×1矩阵(或标量)时，$\frac{\partial f}{\partial X}$应有$m\times n$个元素，若$Y$为$p\times q$矩阵时，$\frac{\partial f}{\partial X}$应有$p\times q\times m\times n$个元素。然而，这可能有多种表示方式。本文采用向量化方法，详见下。</p><h2 id="计算技巧"><a href="#计算技巧" class="headerlink" title="计算技巧"></a>计算技巧</h2><h3 id="Hadamard积"><a href="#Hadamard积" class="headerlink" title="Hadamard积"></a>Hadamard积</h3><p>对同形矩阵$A,B\in R^{m\times n}$，表现为按元素乘:<br>$$<br>[A\odot B]<em>{ij}&#x3D;a</em>{ij}b_{ij}<br>$$<br>在numpy中可以直接将两个同形矩阵相乘：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>A = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br>B = np.array([[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]])<br>Hadamard_A_B = A*B<br></code></pre></td></tr></table></figure><h3 id="Kronecker积"><a href="#Kronecker积" class="headerlink" title="Kronecker积"></a>Kronecker积</h3><p>对于$A\in R^{m\times n},B\in R^{p\times q}$，B对A中每个元素$a_{ij}$进行数乘并将结果取代$a_{ij}$位置:<br>$$<br>[A\otimes B]<em>{ij}&#x3D;a</em>{ij}B<br>$$<br>numpy实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>A = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br>B = np.array([[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]])<br>Kronecker_A_B = np.kron(A, B)<br></code></pre></td></tr></table></figure><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>$vec(X_{m\times n})$表示将$X$逐列转换为列向量：<br>$$<br>[vec(X)]<em>{i}&#x3D;X</em>{[\frac{i}{m}],i-m[\frac{i}{m}]+1}<br>$$<br>其中$[a]$表示对有理数$a$向下取整。在numpy中使用reshape()即可。</p><h3 id="变量矩阵等价的唯一性"><a href="#变量矩阵等价的唯一性" class="headerlink" title="变量矩阵等价的唯一性"></a>变量矩阵等价的唯一性</h3><p>​对于变量矩阵$X_{mn},Y_{nq},Z_{nq}$，若存在$X&#x3D;X’$使得$X$列满秩，且有$XY&#x3D;XZ$，则$Y&#x3D;Z$。关于这个结论，暂未查找到资料。然而这是十分重要的。所以，在此给出一个证明。</p><p>​For all column vectors $X_{.1},X_{.2},…X_{.n}$ in $X$：<br>$$<br>k_1X_{.1}+k_2X_{.2}+…+k_nX_{.n}&#x3D;0 \quad \Rightarrow k_1,k_2,…k_n&#x3D;0\tag{1}<br>$$<br>​Suppose $Y\neq Z,XY&#x3D;XZ$, then<br>$$<br>X(Y-Z)&#x3D;0<br>$$<br>​Let $W&#x3D;Y-Z$, then $W\neq 0$, for all column vectors $W_{.1},W_{.2}…W_{.q}$ in $W$:<br>$$<br>\exists {i} \quad s.t.W_{.i}\neq0<br>$$<br>Consider the multiplication of $X$ and $W_{.i}$:<br>$$<br>XW_{.i}&#x3D;(X_{.1},X_{.2},…X_{.n})W_{.i}&#x3D;X_{.1}W_{1i}+X_{.2}W_{2i}+…X_{.n}W_{ni}&#x3D;0<br>$$<br>​这与(1)矛盾。对于可行满秩的变量矩阵，同理，改为左乘即可。在下面计算矩阵导数时将用到这个结论。</p><h2 id="多元函数导数"><a href="#多元函数导数" class="headerlink" title="多元函数导数"></a>多元函数导数</h2><p>​对于多元函数$$y&#x3D;f(x_1,x_2,…,x_n)$$，其微分$dy$为：<br>$$<br>dy&#x3D;\sum_{i&#x3D;1}^{n}\frac{\partial f}{\partial x_i}dx_i<br>$$<br>以向量形式表示，令<br>$$<br>d\mathbf{x}&#x3D;(dx_1,dx_2,…,dx_n)\<br>\frac{\partial f}{\partial \mathbf{x}}&#x3D;(\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},…,\frac{\partial f}{\partial x_n})<br>$$<br>$\frac{\partial f}{\partial \mathbf{x} }$即在点$(x_1,x_2,…,x_n)$的梯度。则将$dy$表示为<br>$$<br>dy&#x3D;\frac{\partial f}{\partial \mathbf{x}}^Td\mathbf{x}<br>$$</p><h2 id="标量对矩阵导数"><a href="#标量对矩阵导数" class="headerlink" title="标量对矩阵导数"></a>标量对矩阵导数</h2><p>​对于$y&#x3D;f(X_{m\times n})$，$f$将矩阵$X$映射为标量，$y$与$X$的全体元素存在映射关系。仿照多元函数，微分$dy$为：<br>$$<br>dy&#x3D;\sum_{i&#x3D;1}^{m}{\sum_{j&#x3D;1}^{n}{\frac{\partial f}{\partial X_{ij} } } }dX_{ij}<br>$$<br>其中<br>$$<br>dX&#x3D;\left[<br>\begin{matrix}<br>dX_{11} &amp; \cdots &amp; dX_{1n}\<br>\vdots &amp; \ddots &amp;  \<br>dX_{m1} &amp;  &amp; dX_{mn}<br>\end{matrix}<br>\right]<br>$$<br>$dX$是与$X$同形的矩阵，$dX_{ij}$表示$X_{ij}$的微分。在$X$中，每个元素都是一个自变量。令<br>$$<br>\frac{\partial f}{\partial X}&#x3D;\left[<br>\begin{matrix}<br>\frac{\partial{f} }{dX_{11} } &amp; \cdots &amp; \frac{\partial{f} }{dX_{1n} }\<br>\vdots &amp; \ddots &amp;  \<br>\frac{\partial{f} }{dX_{m1} } &amp;  &amp; \frac{\partial{f} }{dX_{mn} }<br>\end{matrix}<br>\right]<br>$$<br>于是<br>$$<br>dy&#x3D;tr(\frac{\partial f}{\partial X}^TdX)\tag{*}<br>$$<br>$tr(X)$ is the trace of $X$，which stands for the summary of the diagonal elements of $X$. Notice that the $X$ here is a square.</p><h3 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h3><ol><li>若$y&#x3D;f(Y),Y&#x3D;AXB+C$，则根据(*)式：</li></ol><p>$$<br>dy&#x3D;tr(\frac{\partial f}{\partial Y}^TdY)&#x3D;tr(\frac{\partial f}{\partial X}^TdX)<br>$$</p><p>因为<br>$$<br>dY&#x3D;AdXB<br>$$<br>则根据性质10：<br>$$<br>tr(\frac{\partial f}{\partial Y}^TdY)&#x3D;tr(\frac{\partial f}{\partial Y}^TAdXB)&#x3D;tr(B\frac{\partial f}{\partial Y}^TAdX)&#x3D;tr(\frac{\partial f}{\partial X}^TdX)<br>$$<br>于是<br>$$<br>\frac{\partial f}{\partial X}^T&#x3D;B\frac{\partial f}{\partial Y}^TA\<br>\frac{\partial f}{\partial X}&#x3D;A^T\frac{\partial f}{\partial Y}B^T<br>$$</p><ol start="2"><li>若$y&#x3D;f(Y),Y&#x3D;\sigma(X)$，其中$\sigma(X)$为逐元素的函数，因为</li></ol><p>$$<br>dY&#x3D;\frac{\partial \sigma}{\partial X_{} }\odot dX<br>$$</p><p>则根据性质11：<br>$$<br>tr(\frac{\partial f}{\partial Y}^TdY)&#x3D;tr(\frac{\partial f}{\partial Y}^T(\frac{\partial \sigma}{\partial X_{} }\odot dX))&#x3D;tr((\frac{\partial f}{\partial Y}^T\odot \frac{\partial \sigma}{\partial X }^T)dX))&#x3D;tr(\frac{\partial f}{\partial X}^TdX)<br>$$<br>于是<br>$$<br>\frac{\partial f}{\partial X}&#x3D;\frac{\partial f}{\partial Y}\odot \frac{\partial \sigma}{\partial X }<br>$$</p><h2 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h2><ol><li>$d(X \pm Y)&#x3D;d(X)\pm d(Y)$</li><li>$d(X_{m \times p}Y_{p\times n})&#x3D;(dX)Y+X(dY)$</li></ol><p>​证明：<br>$$<br>d(XY)<em>{ij}&#x3D;d(\sum</em>{i&#x3D;1}^p{X_{ip}Y_{pj} })&#x3D;\sum_{i&#x3D;1}^p(dX_{ip}Y_{pj})+\sum_{i&#x3D;1}^p(X_{ip}dY_{pj})&#x3D;(dX)<em>{i.}Y</em>{.j}+(dX)<em>{.i}Y</em>{j.}<br>$$</p><ol start="3"><li><p>$d(X^T)&#x3D;(dX)^T$</p></li><li><p>$(dX)^{-1}&#x3D;-X^{-1}dXX^{-1}$</p></li></ol><p>​证明：<br>$$<br>XX^{-1}&#x3D;I\<br>Xd(X^{-1})+(dX)X^{-1}&#x3D;d(XX^{-1})&#x3D;d(I)&#x3D;0\<br>(dX)^{-1}&#x3D;-X^{-1}dXX^{-1}<br>$$</p><ol start="5"><li>$d(\sigma(X))&#x3D;\frac{\partial \sigma}{\partial X}\odot dX$</li></ol><p>证明：<br>$$<br>\begin{aligned}<br>d(\sigma(X))<br>&#x3D;\left[<br>\begin{matrix}<br>\frac{\partial \sigma}{\partial X_{11}}dX_{11} &amp; \cdots &amp; \frac{\partial \sigma}{\partial X_{1n}}dX_{1n}\<br>\vdots &amp; \ddots &amp;  \<br>\frac{\partial \sigma}{\partial X_{m1}}dX_{m1} &amp;  &amp; \frac{\partial \sigma}{\partial X_{mn} }dX_{mn}<br>\end{matrix}<br>\right]&#x3D; \left[<br>\begin{matrix}<br>\frac{\partial \sigma}{\partial X_{11} } &amp; \cdots &amp; \frac{\partial \sigma}{\partial X_{1n} }\<br>\vdots &amp; \ddots &amp;  \<br>\frac{\partial \sigma}{\partial X_{m1} } &amp;  &amp; \frac{\partial \sigma}{\partial X_{mn} }<br>\end{matrix}<br>\right]\odot\left[<br>\begin{matrix}<br>dX_{11} &amp; \cdots &amp;dX_{1n}\<br>\vdots &amp; \ddots &amp;  \<br>dX_{m1} &amp;  &amp; dX_{mn}<br>\end{matrix}<br>\right]<br>&#x3D;\frac{\partial \sigma}{\partial X_{} }\odot dX<br>\end{aligned}<br>$$</p><ol start="6"><li><p>$d(X\odot Y)&#x3D;dX\odot Y+X\odot dY$</p></li><li><p>$x$为标量，或1×1矩阵，$x&#x3D;tr(x)$</p></li><li><p>$tr(X^T)&#x3D;tr(X)$</p></li><li><p>$dtr(X)&#x3D;tr(dX)$</p></li><li><p>$tr(XY)&#x3D;tr(YX)&#x3D;tr(X^TY^T)$</p></li><li><p>$tr(A^T(B\odot C))&#x3D;tr((A\odot B)^TC)&#x3D;tr((A^T\odot B^T)C)$     可以看到，只有$B$变成了$B^T$</p></li><li><p>$u,v$是维数相同的向量，$\mathbf 1^T(u\odot v)&#x3D;u^Tv$</p></li><li><p>$u,v$是维数相同的向量，$u^Tv&#x3D;v^Tu$</p></li><li><p>$vec(A+B)&#x3D;vec(A)+vec(B)$</p></li><li><p>$vec(AXB)&#x3D;(B^T\otimes A)vec(X)$</p></li></ol><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>简便起见，将$\frac{\partial f(a)}{\partial a}$记为$f’(a)$</p><h3 id="简单单层网络"><a href="#简单单层网络" class="headerlink" title="简单单层网络"></a>简单单层网络</h3><p>已知<br>$$<br>\hat{y}&#x3D;XW,\quad X\in R^{1\times m},W\in R^{m\times n}<br>$$<br>损失函数<br>$$<br>L&#x3D;\frac{1}{2}(\hat{y}-y)({\hat{y}-y)^T }<br>$$<br>$W$更新公式<br>$$<br>W-&#x3D;lr<em>\frac{\partial L}{\partial W}<br>$$<br>为了求$\frac{\partial L}{\partial W}$，根据(</em>)式先求$dL$<br>$$<br>\begin{aligned}<br>dL&amp;&#x3D;\frac{1}{2}d((\hat{y}-y))({\hat{y}-y)^T }+\frac{1}{2}(\hat{y}-y)d(({\hat{y}-y)^T) }\<br>&amp;&#x3D;\frac{1}{2}(XdW)({\hat{y}-y)^T }+\frac{1}{2}(\hat{y}-y)(XdW)^T \<br>\end{aligned}<br>$$<br>注意到$(\hat{y}-y)^T$与$dWX$均为向量，则根据性质13：<br>$$<br>dL&#x3D;(XdW)(\hat{y}-y)^T\tag{1}<br>$$<br>由于<br>$$<br>dL&#x3D;tr(\frac{\partial f}{\partial W}^TdW)<br>$$<br>则要将(1)式化为$tr(M^TdW)$形式，以得出$\frac{\partial L}{\partial W}$。注意到$dL$为$1\times1$方阵，于是<br>$$<br>dL&#x3D;tr(dL)&#x3D;tr((XdW)(\hat{y}-y)^T)&#x3D;tr((\hat{y}-y)^TXdW)&#x3D;tr((X^T(\hat{y}-y))^TdW)<br>$$<br>因此有<br>$$<br>\frac{\partial L}{\partial W}&#x3D;X^T(\hat{y}-y)<br>$$</p><h3 id="单隐藏层网络"><a href="#单隐藏层网络" class="headerlink" title="单隐藏层网络"></a>单隐藏层网络</h3><p>已知<br>$$<br>\hat{y}&#x3D;g(hW_2+b_2),h&#x3D;g(XW_1+b_1)\<br>\quad X\in R^{1\times m},W_1\in R^{m\times n},b_1\in R^{1\times n},W_2\in R^{n\times q},b_2\in R^{1\times q}<br>$$<br>损失函数<br>$$<br>L&#x3D;\frac{1}{2}(\hat{y}-y)({\hat{y}-y)^T }<br>$$<br>参数更新公式<br>$$<br>W_1&#x3D;lr<em>\frac{\partial L}{\partial W_1},W_2-&#x3D;lr</em>\frac{\partial L}{\partial W_2},b_1-&#x3D;lr<em>\frac{\partial L}{\partial b_1},b_2-&#x3D;lr</em>\frac{\partial L}{\partial b_2}<br>$$<br>在求某个变量的梯度时，暂且将其他变量视为常量。先求$\frac{\partial L}{\partial b_2}$，由性质11<br>$$<br>\begin{aligned}<br>dL&amp;&#x3D;tr(dL)&#x3D;tr((g’(hW_2+b_2)\odot db_2)(\hat{y}-y)^T)\<br>&amp;&#x3D;tr((\hat{y}-y)(g’(hW_2+b_2)^T\odot db_2^T))\<br>&amp;&#x3D;tr(((\hat{y}-y)\odot g’(hW_2+b_2))db_2^T)\<br>&amp;&#x3D;tr(((\hat{y}-y)\odot g’(hW_2+b_2))^Tdb_2)<br>\end{aligned}<br>$$<br>于是<br>$$<br>\frac{\partial L}{\partial b_2}&#x3D;(\hat{y}-y)\odot g’(hW_2+b_2)<br>$$<br>对于$\frac{\partial L}{\partial W_2}$：<br>$$<br>\begin{aligned}<br>dL&amp;&#x3D;tr(dL)&#x3D;tr((g’(hW_2+b_2)\odot (hdW_2))(\hat{y}-y)^T)\<br>&amp;&#x3D;tr((\hat{y}-y)(g’(hW_2+b_2)^T\odot (hdW_2)^T))\<br>&amp;&#x3D;tr(((\hat{y}-y)\odot g’(hW_2+b_2))(hdW_2)^T)\<br>&amp;&#x3D;tr(((\hat{y}-y)\odot g’(hW_2+b_2))^ThdW_2)\<br>&amp;&#x3D;tr(((h^T((\hat{y}-y)\odot g’(hW_2+b_2)))^TdW_2)<br>\end{aligned}<br>$$<br>于是<br>$$<br>\frac{\partial L}{\partial W_2}&#x3D;h^T((\hat{y}-y)\odot g’(hW_2+b_2))<br>$$<br>为求$\frac{\partial L}{\partial b_1}$，$\frac{\partial L}{\partial W_1}$，根据链式法则，先求出$\frac{\partial L}{\partial h}$<br>$$<br>\begin{aligned}<br>dL&amp;&#x3D;tr(dL)&#x3D;tr((g’(hW_2+b_2)\odot (dhW_2))(\hat{y}-y)^T)\<br>&amp;&#x3D;tr((\hat{y}-y)(g’(hW_2+b_2)^T\odot (dhW_2)^T))\<br>&amp;&#x3D;tr(((\hat{y}-y)\odot g’(hW_2+b_2))(dhW_2)^T)\<br>&amp;&#x3D;tr(((\hat{y}-y)\odot g’(hW_2+b_2))^TdhW_2)\<br>&amp;&#x3D;tr((((\hat{y}-y)\odot g’(hW_2+b_2))W_2^T)^Tdh)<br>\end{aligned}<br>$$<br>于是<br>$$<br>\frac{\partial L}{\partial h}&#x3D;((\hat{y}-y)\odot g’(hW_2+b_2))W_2^T<br>$$<br>设$a&#x3D;XW_1+b_1$，则$h&#x3D;g(a)$，根据链式法则<br>$$<br>\frac{\partial L}{\partial a}&#x3D;\frac{\partial L}{\partial h}\odot \frac{\partial g}{\partial a}\<br>\frac{\partial L}{\partial b_1}&#x3D;\frac{\partial L}{\partial a}\<br>\frac{\partial L}{\partial W_1}&#x3D;X^T\frac{\partial L}{\partial a}<br>$$<br>于是得到<br>$$<br>\frac{\partial L}{\partial b_1}&#x3D;(((\hat{y}-y)\odot g’(hW_2+b_2))W_2^T)\odot \frac{\partial g}{\partial a}\<br>\frac{\partial L}{\partial W_1}&#x3D;X^T((((\hat{y}-y)\odot g’(hW_2+b_2))W_2^T)\odot \frac{\partial g}{\partial a})<br>$$</p><h1 id="特征值与特征向量"><a href="#特征值与特征向量" class="headerlink" title="特征值与特征向量"></a>特征值与特征向量</h1><p>“There are some matrices for which there are not a full set of eigenvectors. that’s really the main sort of annoying point in the whole subject of linear algebra is some matrices don’t have enough eigenvectors. “</p><p>对于任何一个n阶方阵$A_{n\times n}$，它的特征值一定有n个（算上虚特征值和重复特征值），换句话说这等价于实数特征值不超过n个，因为求特征值用$|A-\lambda I|&#x3D;0$,最终必定可以化为$\lambda$的n阶方程，实数解的个数不超过n（代数基本定理）。</p><ul><li>定理1：不同特征值对应的特征向量线性无关。证明：</li></ul><p>设$A$有两个特征值$\lambda_{1},\lambda_{2}$ 和两个对应的特征向量$x_1,x_2$，且$\lambda_{1}\neq\lambda_{2}$.假设$x_1,x_2$线性相关，则$x_1&#x3D;kx_2$成立，则$Ax_2&#x3D;\lambda_2 x_2&#x3D;A kx_1&#x3D;kAx_1&#x3D;k\lambda_1x_1$,所以(1)当$\lambda_1\ne0$，有$x_2&#x3D;\frac{\lambda_1}{\lambda_2}kx_1&#x3D;kx_1$,$\lambda_1&#x3D;\lambda_2$，矛盾。(2)当$\lambda_1&#x3D;0$,因为特征向量非零，所以$\lambda_2&#x3D;0&#x3D;\lambda_1$，矛盾。于是得证</p><h2 id="实数特征值与虚数特征值"><a href="#实数特征值与虚数特征值" class="headerlink" title="实数特征值与虚数特征值"></a>实数特征值与虚数特征值</h2><p>从矩阵$A$的特征值$\lambda$与特征向量$x$的计算来看，即$Ax&#x3D;\lambda x$，在$x$方向上，矩阵$A$只起到了拉伸的作用。若特征值为0，则$A$将某些向量映射到0空间，这说明$A$不满秩。若特征值为实数，说明对某些方向上的向量，$A$仅起到了拉伸的效果，下面将这种矩阵称作拉伸矩阵。若特征值是虚数，说明$A$对所有方向的向量都起到了旋转的效果，下面将这种矩阵称为旋转矩阵。</p><h3 id="拉伸矩阵"><a href="#拉伸矩阵" class="headerlink" title="拉伸矩阵"></a>拉伸矩阵</h3><p>对于一个特征向量空间内的向量，拉伸矩阵让一个向量更靠近它最大的特征值对应的特征向量方向。也就是说，在拉伸矩阵的特征向量空间中的向量，一直让它左乘这个拉伸矩阵，那么它最终会逼近最大特征值对应的特征向量方向。为什么要强调是特征向量空间内？因为前面的定理1说明，r个不同特征值可以形成r维特征向量空间，而相同的k重特征值却不一定能形成k维特征向量空间（因为几何重数不大于代数重数，Jordan标准型给出证明）,因此，特征向量空间的维度不一定等于原矩阵的维度（即使原矩阵是满秩的）。因此要强调是特征向量空间内的向量，因为它可以由特征向量表出。</p><p>证明：设矩阵$A_{n\times n}$有$r$个不同的特征向量$\lambda_1,…\lambda_r$，有一个特征向量空间内的向量$x$,特征向量的基底是$e_1,e_2,…e_r$,于是<br>$$<br>\exists k_1,k_2,…k_n,s.t.\quad x&#x3D;\sum_{i&#x3D;1}^rk_ie_i<br>$$<br>则<br>$$<br>\lim_{y\to \infty}A^yx&#x3D;A^y\sum_{i&#x3D;1}^rk_ie_i&#x3D;\sum_{i&#x3D;1}^rk_iA^ye_i&#x3D;\sum_{i&#x3D;1}^rk_i\lambda_i^ye_i<br>$$<br>容易得知最大的特征值的特征向量方向对结果方向影响最大</p><h3 id="旋转矩阵的分解"><a href="#旋转矩阵的分解" class="headerlink" title="旋转矩阵的分解"></a>旋转矩阵的分解</h3><p>旋转矩阵对所有实数向量都起到了旋转的效果，但也可能起到拉伸效果。事实上，旋转矩阵可以分解成一个纯旋转矩阵和一个拉伸矩阵的矩阵乘积。（<strong>该结论暂未证明</strong>）纯旋转矩阵指的是，左乘纯旋转矩阵只改变向量的方向，而不改变向量的模长。纯旋转矩阵一定是正交矩阵。满足下列关系的矩阵为纯旋转矩阵：<br>$$<br>\forall \textbf x,k\in[-1,1],cos&lt;\textbf x,A\textbf x&gt;&#x3D;\frac{\sum_{i&#x3D;1}^{n}\sum_{j&#x3D;1}^{n}A_{ij}x_ix_j}{\sum_{i&#x3D;1}^{n}a_i^2}\equiv k<br>$$<br>即旋转矩阵对任意向量的旋转角均相等。这个结论直接计算化简即得。                                                                                                                                                         </p><h2 id="相似对角化"><a href="#相似对角化" class="headerlink" title="相似对角化"></a>相似对角化</h2><p>矩阵$A$可以相似于对角矩阵$\Lambda$，当且仅当矩阵$A_{n\times n}$有n个线性无关的特征向量。即：<br>$$<br>\begin{aligned}<br>\Lambda&amp;&#x3D;P^{-1}AP\<br>P&amp;&#x3D;(p_1,p_2,..p_n)\<br>\Lambda&amp;&#x3D;\left[<br>\begin{matrix}<br>\lambda_1 &amp;  &amp; \<br> &amp; \ddots &amp;  \<br>&amp;  &amp; \lambda_{n}<br>\end{matrix}<br>\right]<br>\end{aligned}<br>$$<br>其中，$\lambda_i$对应的特征向量为$p_i$. 直接计算$AP&#x3D;P\Lambda$即可证明。若$A$相似于对角矩阵，则很容易算$A$的次方：<br>$$<br>A^k&#x3D;P\Lambda^kP^{-1}<br>$$</p><h1 id="Jordan标准型"><a href="#Jordan标准型" class="headerlink" title="Jordan标准型"></a>Jordan标准型</h1><p>任何n阶方阵可以相似于具有以下形式的矩阵：<br>$$<br>J_{n\times n}&#x3D;\left[\begin{matrix}<br>J_1 &amp;  &amp; &amp;\<br> &amp; J_2&amp; \<br>  &amp;  &amp; \ddots&amp; \<br>&amp;  &amp;&amp; J_{k}<br>\end{matrix}<br>\right],J_i&#x3D;\left[\begin{matrix}<br>\lambda_i &amp;1 &amp; &amp; \<br> &amp; \lambda_i &amp;1 &amp;  \<br>  &amp;  &amp;\ddots &amp; \ddots \<br>&amp; &amp; &amp; \lambda_{i}<br>\end{matrix}<br>\right]<br>$$<br>其中$\sum_{i&#x3D;1}^nR(J_i)&#x3D;n$. 在Gilbert Strang 的 <em>Linear Algebra and Its Applications</em>, Appendix B 一章通过构造$P$证明了$\forall A\in R^{n\times n},\exists P,  s.t.A&#x3D;P^{-1}JP$.</p><p>下面分析Jordan型的意义。</p><p>注意小块最后一行，这一行只有一列非零，因此，$|J_i-\lambda_i I|&#x3D;0$，$\lambda_i$是$J$的特征值。</p><p>接下来考虑$J$的特征向量。对于小块$J_i$，$(J_i-\lambda_iI)x&#x3D;0$的解空间维数为1，因此小块$J_i$对应一个1维特征向量空间。于是容易理解，代数重数大于等于几何重数的原因。每个小块至少是1维，当小块为1维时，这个小块的特征值对应的特征向量空间是1维，此时代数重数等于几何重数。如果小块大于1维，代数重数大于几何重数，这个小块的代数重数等于小块的维数，特征向量空间是1维。相同的特征值可能对应不同的特征向量，此时不同小块中存在相同的特征值，而这些不同的小块对应不同的特征向量。</p><h1 id="变换"><a href="#变换" class="headerlink" title="变换"></a>变换</h1><p>矩阵乘向量在几何意义上就是向量的线性变换，满秩矩阵把这个向量映射到同维空间的另一个向量，不满秩矩阵把它映射到低维空间的另一个向量。矩阵乘矩阵在几何意义上是基底变换，一个矩阵的每一列视为一个基底向量，基底变换就是把一组基底映射到另一组基底，满秩矩阵把基底映射到同维空间的另一组基底，不满秩矩阵把基底映射到低维空间的另一组基底。</p><h2 id="基底变换"><a href="#基底变换" class="headerlink" title="基底变换"></a>基底变换</h2><p>一个可逆矩阵可以将一组基底变换到在同维空间中的任意一组基底。这其实就是矩阵乘的意义。设有一组基底$(e_1,e_2,…e_n)$，他们构成矩阵$M$，其中$e_i$是$n\times 1$列向量，对于n维空间中任意一组基底构成的矩阵$N$，设经过左乘矩阵$P$从$M$变换到$N$，则<br>$$<br>\begin{aligned}<br>N&amp;&#x3D;MP\<br>P&amp;&#x3D;M^{-1}N<br>\end{aligned}<br>$$<br>于是可知存在唯一的矩阵$P$使得基底$M$变换到基底$N$</p><p>基底变换定理：基底变换不改变向量本身，通过基底变换变的是向量在这组基底下的各个分量的取值。也就是说，在标准正交基下（或者任何一组作为参考的基底）的向量是不变的。基底$M$经过基底变换得到基底$N$，即有$N&#x3D;MP$，对同一个标准正交基下的向量$\textbf x$，其在$M,N$下的分量为$\alpha,\alpha’$，即有$M\alpha&#x3D;N\alpha’&#x3D;\textbf x,\alpha’&#x3D;P^{-1}\alpha$</p><p>从这个结论来看，向量的线性变换的意义可以归结为以下两个等价的观点：</p><p>1）在基底不变的情况下，$\textbf x$变换到同基底下的$P\textbf x$</p><p>2）在向量不变的情况下，基底$M$变换到$PM$，$\textbf x$在新基底$PM$下的分量为$P^{-1}\textbf x$</p><p>这两种观点是等价的，区别只在于选择向量还是基底作为参考，而用两种观点来描述的他们之间的相对关系是一样的。基底不变，向量变等价于向量不变，基底变。</p><h2 id="相似变换"><a href="#相似变换" class="headerlink" title="相似变换"></a>相似变换</h2><p>基底$M$经过基底变换得到基底$N$，即有$N&#x3D;MP$，对同一个标准正交基下的向量$\textbf x$，其在$M,N$下为$\alpha,\alpha’$，即有$M\alpha&#x3D;N\alpha’&#x3D;\textbf x,\alpha’&#x3D;P^{-1}\alpha$，$\alpha$经过线性变换得到向量$\beta$，即$\beta&#x3D;A\alpha$，设$\beta$在$N$下为$\beta’$，于是有$\beta’&#x3D;P^{-1}\beta$，设矩阵$A’$使得$\beta’&#x3D;A’\alpha’$，于是<br>$$<br>\beta’&#x3D;A’\alpha’&#x3D;P^{-1}\beta&#x3D;P^{-1}A\alpha&#x3D;P^{-1}AP\alpha’<br>$$<br>则<br>$$<br>A’&#x3D;P^{-1}AP\tag{2}<br>$$<br>上面的结论说明，一个向量被映射到另一个向量，在两个不同的基底下的两个变换关系之间具有(2)式描述的关系。因此，相似变换是同一个线性变换在不同基底下的不同表现形式。</p><h3 id="相似对角化-1"><a href="#相似对角化-1" class="headerlink" title="相似对角化"></a>相似对角化</h3><p>若方阵经相似变换能化为对角矩阵，则这个方阵可相似对角化。</p><p>$A_{n\times n}$可相似对角化$\iff$$A_{n\times n}$有n个线性无关的特征向量</p><p>证明：充分性：若$A_{n\times n}$可相似对角化，设其相似于对角矩阵$B&#x3D;diag(b_1,b_2,…b_n)$，且$B&#x3D;P^{-1}AP$，则$PB&#x3D;AP$. 将$P$以列向量形式表示，$P&#x3D;{p_{.1},p_{.2},…p_{.n})$ . 因为<br>$$<br>PB&#x3D;(p_{.1},p_{.2},…p_{.n})<br>$$<br>从几何角度考虑，相似对角化实质上是把原矩阵$A$对应的基底变换到特征向量方向上，这时新的基底两两正交。</p><h2 id="合同变换"><a href="#合同变换" class="headerlink" title="合同变换"></a>合同变换</h2><p>任意一个二次型<br>$$<br>f(\textbf x)&#x3D;\sum_{i&#x3D;1}^n\sum_{j&#x3D;1}^{n} k_{ij}x_ix_j,\textbf x&#x3D;(x_1,x_2,…x_n)^T<br>$$<br>均可以化为矩阵乘形式$\textbf x^TA\textbf x$，可以证明这样的$A$有无数个。当$A$为对称矩阵，则$A$是唯一的，此时有<br>$$<br>a_{ij}&#x3D;\left{<br>\begin{array}{l}<br> \frac{k_{ij}+k_{ji} }{2},\quad i\neq j \<br>k_{ij},\quad i&#x3D;j \<br>\end{array}<br>\right.<br>$$<br>当对基底$M$下的向量$\textbf x$进行线性变换$C^{-1}$(为了让结论形式看起来简洁，写成C的逆矩阵。其实只要这里是个可逆矩阵就行)，得到另一个基底$N$下的向量$\textbf y$，即$\textbf x&#x3D;C\textbf y$，这时$f(\textbf x)&#x3D;g(\textbf y)&#x3D;\textbf x^TA\textbf x&#x3D;\textbf y^TC^TAC\textbf y&#x3D;\textbf y^TA’\textbf y$，于是<br>$$<br>A’&#x3D;C^TAC<br>$$<br>上面的结论说明，同一个二次型在两个不同的基底下，两个变换关系之间具有(2)式描述的关系。</p><h3 id="惯性定理"><a href="#惯性定理" class="headerlink" title="惯性定理"></a>惯性定理</h3><p>上面说到，合同变换可以视为同一个二次型在两个不同的基底下的不同的描述形式，因此，我们可以通过变换基底，让二次型只含有平方项，不含有$x_1x_2$这种混合乘积，并且平方项系数只能为0，1，-1，也就是化成标准型。可以想象，一定存在某一个位置可以达到这种效果，例如一个椭圆，两个基底向量分别它的长轴和短轴重合时，就只含有平方项，将长轴一半和短轴一半长度作为两个基底向量的单位长度，就得到标准型，变成了一个单位圆。事实上，将任意二次型化成标准型存在且只存在一种合同变换。下面证明这个结论。</p><p>证明：设有二次型$f(\textbf x)&#x3D;g(\textbf y)&#x3D;\textbf x^TA\textbf x&#x3D;\textbf y^T B\textbf y$，其中$B$为对角矩阵。若证明了$B$是对角矩阵，则可将$B$化为只含0，1，-1的对角矩阵。于是原命题等价于证存在$C$满足$B&#x3D;C^TAC$.</p><p>因为$A$为对称矩阵，根据对称矩阵的结论，存在正交矩阵$P$使得$A&#x3D;P^{-1}B P$，根据正交矩阵的结论，$P^{-1}&#x3D;P^T$，于是$B&#x3D;PAP^T$，即证</p><p>标准型中正、负号的个数称为正、负惯性系数。因为合同变换是同一个二次型在不同基底下的形式，而将任意二次型化成标准型存在且只存在一种合同变换，因此易知合同变换不改变正负惯性系数。</p><h2 id="正交化"><a href="#正交化" class="headerlink" title="正交化"></a>正交化</h2><h3 id="向量投影"><a href="#向量投影" class="headerlink" title="向量投影"></a>向量投影</h3><p>向量$\alpha$在一组标准正交基$V&#x3D;(\beta_1,\beta_2,…\beta_m)$下的投影为<br>$$<br>proj_V\alpha&#x3D;\sum_{i&#x3D;1}^m(\alpha\cdot\beta_i)\beta_i<br>$$<br>其中，$\alpha,\beta_i$均为n维向量（$n\ge m$），$\alpha\cdot\beta_i$是$\alpha$在$\beta_i$方向上投影的向量长度。当$n&#x3D;m$时，$\alpha$可由$V$线性表出，此时$\alpha&#x3D;proj_V\alpha$，当$n&gt; m$，$\alpha_p&#x3D;\alpha-proj_V\alpha$与空间$V$垂直。这里与空间垂直的意义是，$\alpha_p$与每个基向量垂直。向量与线和平面垂直的情况容易想象。从想象中可以得到以下定理（<strong>该结论暂未证明</strong>）：所有$\alpha$对应的$\alpha_p$形成的向量空间的维度&#x3D;m-n.</p><h3 id="正交化-1"><a href="#正交化-1" class="headerlink" title="正交化"></a>正交化</h3><p>任意线性无关的一组向量均可以化为该向量空间下的一组标准正交基。</p><p>根据投影定义可以知道，任何一个与向量空间$V$中所有基向量线性无关的向量$\alpha$均能表示成这个空间内的投影$proj_V\alpha$+与这个空间垂直的向量$\alpha_p$.因此，可以采用如下方法得到一组基$(\beta_1,\beta_2,…\beta_m)$的标准正交基：</p><p>1）取第一个向量$\beta_1$，将其归一化，加入到子空间$W$中</p><p>2）取第k(k&gt;1)个向量$\beta_k$，计算其与子空间$W$垂直的向量$\beta_{kp}&#x3D;\beta-proj_V\beta_{k}$，并将$\beta_{kp}$归一化，加入到$W$中</p><p>3）重复2），直到k&#x3D;n</p><h3 id="正交向量空间"><a href="#正交向量空间" class="headerlink" title="正交向量空间"></a>正交向量空间</h3><p>若k维空间$V$中的一组基$(\alpha_1,…\alpha_k)$与另一r维空间$W$中的一组基$(\beta_1,…\beta_r)$正交($\alpha_i,\beta_j$为同维向量)，则空间$V$与空间$W$中所有向量均正交。</p><p>证明：设$V$中一向量$\alpha&#x3D;\sum_{i&#x3D;1}^{k}m_i\alpha_i$,$W$中一向量$\beta&#x3D;\sum_{i&#x3D;1}^{r}n_i\beta_i$, 则<br>$$<br>\alpha \cdot \beta&#x3D;(\sum_{i&#x3D;1}^{k}m_i\alpha_i)(\sum_{i&#x3D;1}^{r}n_i\beta_i)&#x3D;\sum_{i&#x3D;1}^{k}\sum_{j&#x3D;1}^r(m_in_j\alpha_i\cdot\beta_j)&#x3D;0<br>$$<br>于是两空间中任意向量均正交。</p><h1 id="特殊矩阵"><a href="#特殊矩阵" class="headerlink" title="特殊矩阵"></a>特殊矩阵</h1><h2 id="正交矩阵"><a href="#正交矩阵" class="headerlink" title="正交矩阵"></a>正交矩阵</h2><ul><li>正交矩阵满足$A^{-1}&#x3D;A^T$</li></ul><p>将矩阵写成列向量形式$A&#x3D;(a_1,a_2,…a_n)$，若<br>$$<br>a_i\cdot a_{j}&#x3D;\left{<br>\begin{array}{l}<br> 0,\quad i\neq j \<br>1,\quad i&#x3D;j \<br>\end{array}<br>\right.<br>$$<br>则$A$为正交矩阵。</p><p>若$A$为正交矩阵，则$A^TA&#x3D;I$，因为<br>$$<br>A^TA&#x3D;\left(<br>\begin{matrix}<br>a_1^T \<br>a_2^T    \<br>…\<br> a_n^T<br>\end{matrix}<br>\right)(a_1,a_2,…a_n)&#x3D;\left(<br>\begin{matrix}<br>a_1^Ta_1 &amp;a_1^Ta_2 &amp;…  \<br>a_2^Ta_1 &amp; \ddots &amp;  \<br>…&amp;  &amp; a_n^Ta_n<br>\end{matrix}<br>\right)&#x3D;I<br>$$<br>于是可得$A^{-1}&#x3D;A^T$</p><h2 id="对称矩阵"><a href="#对称矩阵" class="headerlink" title="对称矩阵"></a>对称矩阵</h2><ul><li>任何一个矩阵 A可以表示为一个反对称矩阵M + 一个对称矩阵S</li></ul><p>证明：如果原命题成立，只要证明存在这样的$M$和$N$，因为<br>$$<br>\begin{aligned}<br>A_{ij}&amp;&#x3D;M_{ij}+S_{ij}\<br>A_{ji}&amp;&#x3D;M_{ji}+S_{ji}&#x3D;-M_{ji}+S_{ij}<br>\end{aligned}<br>$$<br>两个未知数$A_{ij},A_{ji}$两个等式可以解出两个未知数$M_{ij},S_{ij}$，即证</p><ul><li>矩阵$A$是实对称矩阵$\iff$$A&#x3D;P^{-1}\Lambda P$，$\Lambda$是对角矩阵，$P$是正交矩阵</li></ul><p>证明：1）充分性：若$A$为实对称矩阵，设$J&#x3D;P^{-1}AP$,  $J$为Jordan矩阵。假设$A$不可对角化，则$J$存在维数大于1的小块。不妨设$J_1$维数大于1，即<br>$$<br>J_1&#x3D;\left[\begin{matrix}<br>\lambda_1 &amp;1 &amp; &amp; \<br> &amp; \lambda_1 &amp;\ddots &amp;  \<br>  &amp;  &amp;\ddots &amp;  \<br>\end{matrix}<br>\right]<br>$$<br>令$B&#x3D;P^TP$，则$B$为对称矩阵，$b_{12}&#x3D;b_{21}$，且$b_{11}&#x3D;\sum_{i&#x3D;1}^{n}p_{i1}^2$，因为$P$可逆，所以$p_{i1}$不全为0，$b_{11}&gt;0$. 因为$P^TAP&#x3D;P^TPJ&#x3D;BJ$, $PAP^T$为对称矩阵，则$BJ$为对称矩阵。因为$BJ$第1行第2列元素为$b_{11}+\lambda_1b_{12}$，第2行第1列元素为$\lambda_1b_{21}$，则$BJ$为非对称矩阵，矛盾。因此$A$可对角化。</p><p>下面证明实对称矩阵$A$不同特征值对应的特征向量之间相互正交。</p><p>设$\alpha_1,\alpha_2$是$A$的两个不同的特征向量，$A\alpha_1&#x3D;\lambda_1\alpha_1,A\alpha_2&#x3D;\lambda_2\alpha_2$, 且$\lambda_2\ne\lambda_1$, 分别左乘$\alpha_2^T,\alpha_1^T$, 则$\alpha_2^TA\alpha_1&#x3D;\lambda_1\alpha_2^T\alpha_1,\alpha_1^TA\alpha_2&#x3D;\lambda_2\alpha_1^T\alpha_2$, 因为向量数乘可交换，所以$\alpha_2^TA\alpha_1&#x3D;(A\alpha_2)^T\alpha_1&#x3D;\alpha_1^T(A\alpha_2)&#x3D;\alpha_1^TA\alpha_2$, 于是$\lambda_2\alpha_1^T\alpha_2&#x3D;\lambda_1\alpha_2^T\alpha_1$, $(\lambda_2-\lambda_1)\alpha_1^T\alpha_2&#x3D;0$, 则$\alpha_1^T\alpha_2&#x3D;0$.</p><p>因此$A$的不同特征值对应的特征向量构成正交向量空间，在每个r维特征向量空间中选取r个线性无关的模长为1的向量，按列构成矩阵$P$即可。</p><p>2）必要性：若$A&#x3D;P^{-1}\Lambda P$，$\Lambda$是对角矩阵，$P$是正交矩阵，则$P^{-1}&#x3D;P^T,A&#x3D;P^T\Lambda P,A^T&#x3D;P^T\Lambda^TP&#x3D;P^T\Lambda P&#x3D;A$, 则$A$为对称矩阵。下面证明$A$的特征值均为实数。</p><p>设$Ax&#x3D;\lambda x$, $\lambda&#x3D;a+bi$, $\bar{x}^TAx&#x3D;\lambda \bar{x}^Tx$, 直接计算可证$\bar{AB}&#x3D;\bar{A}\bar{B}$, 因此$\bar{x}^TAx&#x3D;\bar{x}^T\bar{A}^Tx&#x3D;(\bar{Ax})^Tx&#x3D;\lambda \bar{x}^Tx$, 于是$\bar{Ax}&#x3D;\lambda \bar{x}$, 则$Ax&#x3D;\bar{\lambda} x$, 则$b&#x3D;0$. 因此$A$所有特征值为实数。</p><h2 id="正定矩阵"><a href="#正定矩阵" class="headerlink" title="正定矩阵"></a>正定矩阵</h2><p>若实对称矩阵所有特征值均&gt;0,则为正定，若均$\ge0$, 则为半正定，均&lt;0为负定矩阵。</p><p>从几何意义考虑，一个正定矩阵$A_{n\times n}$满足$\forall x\in R^{n\times1}, x^TAx&gt;0$，这表明向量$x$与$Ax$的内积为正，如对于二，三维向量就是夹角小于$\frac{\pi}{2}$. </p>]]></content>
    
    
    <categories>
      
      <category>Math</category>
      
      <category>Matrix</category>
      
    </categories>
    
    
    <tags>
      
      <tag>matrix</tag>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Euler&#39;s Formula</title>
    <link href="/2023/04/06/Euler-s-Formula/"/>
    <url>/2023/04/06/Euler-s-Formula/</url>
    
    <content type="html"><![CDATA[<hr><span id="more"></span><h1 id="eulers-formula">Euler's Formula</h1><h2 id="transition">Transition</h2><p>​ It is known the natural logarithm can be defined as: <span class="math display">\[e=\displaystyle\lim_{ x \to \infty}(1+\frac{ 1}{ x})^x\]</span> ​ If we want to popularize it to imaginary, one form ismeaningful: <span class="math display">\[f(e)=\displaystyle\lim_{x \to \infty}(1+\frac{i}{x})^x\]</span> ​ I use <span class="math inline">\(\mit f\)</span><span class="math inline">\((e)\)</span> to stand the result we get. It is animaginary number, in other words it can be expressed as: <span class="math display">\[f(e)=a+bi\]</span> ​ It is worth noting that <span class="math inline">\(f\)</span><span class="math inline">\((e)\)</span>has another form, actually every power of e has this property. <span class="math display">\[\displaystyle\lim_{x \to \infty}(1+\frac{i}{x})^x=\displaystyle\lim_{x\to \infty}((1+\frac{i}{x})^{\frac{x}{i} })^i=e^i=a+bi\]</span> ​ It seems just a trick of mathematic manipulation. However, itbring us a lot convenience in the future steps. We will see its powersoon.</p><p>​ The next goal is to find the <span class="math inline">\(\mit{a}\)</span> and <span class="math inline">\(\mit{b}\)</span>. To achieve this, we should knowsome interesting property and mathematic meaning of the imaginarynumber.</p><p>##Imaginary number and rotary</p><p>​ Frankly speaking I had been confused by imaginary number for a longtime, until I got it can solve rotary problems one day. Let's see anexample, suppose we have two imaginary number <span class="math inline">\({\mit{a} }=r_1(cos\alpha_{1}+{\mit{i}}sin\beta_{1})\)</span> and <span class="math inline">\({\mit{b}}=r_2(cos\alpha_{2}+{\mit{i}}sin\beta_{2})\)</span> , then consider the multiplication of <span class="math inline">\(\mit{a}\)</span> and <span class="math inline">\(b\)</span>: <span class="math display">\[\begin{aligned}a\timesb&amp;=r_1r_2(cos\alpha_{1}cos\alpha_{2}-sin\alpha_{1}sin\alpha_{2}+{\mit{i}}   (cos\alpha_1sin\alpha_2+cos\alpha_2sin\alpha_1))\\&amp;= r_1r_2(cos(\alpha_1+\alpha_2)+isin(\alpha_1+\alpha_2))\end{aligned}\]</span> ​ Look, the multiplication of <span class="math inline">\(\mit{a}\)</span> and <span class="math inline">\(\mit{b}\)</span> results to a new imaginarynumber, whose argument is the sum of that of <span class="math inline">\(\mit{a}\)</span> and <span class="math inline">\(\mit{b}\)</span> and the length is themultiplication of that of <span class="math inline">\(\mit{a}\)</span>and <span class="math inline">\(\mit{b}\)</span>. In fact, this propertycan be popularized to the multiplication of <span class="math inline">\(\mit{n}\)</span> imaginary numbers or an imaginaryto the power of <span class="math inline">\(n\)</span>, as following:<span class="math display">\[\begin{aligned}a^n&amp;=(r(cos\alpha+isin\alpha))^n=r^n(cos(n\alpha)+isin(n\alpha))\quad(n\inR)\end{aligned}\]</span></p><p>​ It will help us in the next step.</p><p>##When comes to infinite</p><p>​ It is said above <span class="math inline">\(e^i=\displaystyle\lim_{x \to\infty}(1+\frac{i}{x})^x\)</span> is an imaginary number, so we need toconsider two aspects of it: argument and length. It is themultiplication of many <span class="math inline">\((1+\frac{i}{x})\)</span>s. Let me give it a name,called <span class="math inline">\({\mit{I}}\)</span> (Just think it isan individual of the big population). Then the argument of <span class="math inline">\(e^i\)</span> will be: <span class="math display">\[arg(e^i)=x\times arg(I)\quad(x \to \infty)\]</span> And the tangent of the argument of <span class="math inline">\(\mit{i}\)</span> is: <span class="math display">\[tan(arg(I))=\frac{1}{x}\quad(x \to \infty)\]</span> So we can get the argument of <span class="math inline">\(e^i\)</span> : <span class="math display">\[\displaystyle\lim_{x \to \infty}arg(e^i)=\displaystyle\lim_{x \to\infty}x\times arctan\frac{1}{x}=\displaystyle\lim_{t \to0}\frac{arctan\,t}{t}=1\]</span> This transformation can be derived by L'Hôpital's rule. Thenconsider the length of <span class="math inline">\(e^i\)</span>: <span class="math display">\[L(e^i)=L(I)^x\quad(x\to \infty)\]</span> And the length of <span class="math inline">\(I\)</span> is:<span class="math display">\[L(I)=\sqrt{1+\frac{1}{x^2} }\quad(x\to\infty)\]</span> So we can get the length of <span class="math inline">\(e^i\)</span>: <span class="math display">\[L(e^i)=\displaystyle\lim_{x \to \infty}({\sqrt{1+\frac{1}{x^2} }})^x=\displaystyle\lim_{x \to \infty}({ {1+\frac{1}{x^2} }})^{\frac{x}{2} }=\displaystyle\lim_{x \to \infty}(({ {1+\frac{1}{x^2} }})^{ {x^2} })^\frac{1}{2x}=\displaystyle\lim_{x \to\infty}e^{\frac{1}{2x} }=1\]</span> The argument and the length of <span class="math inline">\(e^i\)</span> are both 1, so <span class="math inline">\(e^i=cos1+isin1\)</span>. Then we can deriveEuler's formula: <span class="math display">\[e^{i\theta}=(e^i)^\theta=(cos1+isin1)^\theta=cos\theta+isin\theta\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Math</category>
      
      <category>Calculus</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>爬和撞</title>
    <link href="/2023/04/01/%E7%88%AC%E5%92%8C%E6%92%9E/"/>
    <url>/2023/04/01/%E7%88%AC%E5%92%8C%E6%92%9E/</url>
    
    <content type="html"><![CDATA[<p>​从前梁实秋教授曾经说过：穷人总是要爬，往上爬，爬到富翁的地位。不但穷人，奴隶也是要爬的，有了爬得上的机会，连奴隶也会觉得自己是神仙，天下自然太平了。</p><p>​虽然爬得上的很少，然而个个以为这正是他自己。这样自然都安分的去耕田，种地，拣大粪或是坐冷板凳，克勤克俭，背着苦恼的命运，和自然奋斗着，拚命的爬，爬，爬。可是爬的人那么多，而路只有一条，十分拥挤。老实的照着章程规规矩矩的爬，大都是爬不上去的。聪明人就会推，把别人推开，推倒，踏在脚底下，踹着他们的肩膀和头顶，爬上去了。大多数人却还只是爬，认定自己的冤家并不在上面，而只在旁边——是那些一同在爬的人。他们大都忍耐着一切，两脚两手都着地，一步步的挨上去又挤下来，挤下来又挨上去，没有休止的。</p><span id="more"></span><p>​然而爬的人太多，爬得上的太少，失望也会渐渐的侵蚀善良的人心，至少，也会发生跪着的革命。于是爬之外，又发明了撞。</p><p>​这是明知道你太辛苦了，想从地上站起来，所以在你的背后猛然的叫一声：撞罢。一个个发麻的腿还在抖着，就撞过去。这比爬要轻松得多，手也不必用力，膝盖也不必移动，只要横着身子，晃一晃，就撞过去。撞得好就是五十万元大洋，妻，财，子，禄都有了。撞不好，至多不过跌一交，倒在地下。那又算得什么呢，——他原本是伏在地上的，他仍旧可以爬。何况有些人不过撞着玩罢了，根本就不怕跌交的。</p><p>​爬是自古有之。例如从童生到状元，从小瘪三到康白度。撞却似乎是近代的发明。要考据起来，恐怕只有古时候“小姐抛彩球”有点像给人撞的办法。小姐的彩球将要抛下来的时候，——一个个想吃天鹅肉的男子汉仰着头，张着嘴，馋涎拖得几尺长……可惜，古人究竟呆笨，没有要这些男子汉拿出几个本钱来，否则，也一定可以收着几万万的。</p><p>​爬得上的机会越少，愿意撞的人就越多，那些早已爬在上面的人们，就天天替你们制造撞的机会，叫你们化些小本钱，而豫约着你们名利双收的神仙生活。所以撞得好的机会，虽然比爬得上的还要少得多，而大家都愿意来试试的。这样，爬了来撞，撞不着再爬……鞠躬尽瘁，死而后已。</p>]]></content>
    
    
    <categories>
      
      <category>文学</category>
      
      <category>杂文</category>
      
    </categories>
    
    
    <tags>
      
      <tag>鲁迅</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
