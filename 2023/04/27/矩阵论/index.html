

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/cube32.png">
  <link rel="icon" href="/img/cube32.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="M.Q.">
  <meta name="keywords" content="">
  
    <meta name="description" content="矩阵导数​	矩阵导数即对矩阵的多个元素求导，作为一种简便的计算方式。在下面的计算过程中，标量，向量均作为特殊的矩阵，如无指出是标量或向量，矩阵的含义均包括了是标量，向量，矩阵的情况。在矩阵导数中，因变量矩阵的每个元素与自变量矩阵中的每个元素均存在映射。 ​	在可能混淆的地方，小写字母为标量，加粗或希腊字母为向量，大写字母为矩阵。在函数中用$y,Y$做自变量，$f$表示函数关系，$dy,dY$表">
<meta property="og:type" content="article">
<meta property="og:title" content="矩阵论">
<meta property="og:url" content="https://qdhdusdc.github.io/2023/04/27/%E7%9F%A9%E9%98%B5%E8%AE%BA/index.html">
<meta property="og:site_name" content="UTOPIA">
<meta property="og:description" content="矩阵导数​	矩阵导数即对矩阵的多个元素求导，作为一种简便的计算方式。在下面的计算过程中，标量，向量均作为特殊的矩阵，如无指出是标量或向量，矩阵的含义均包括了是标量，向量，矩阵的情况。在矩阵导数中，因变量矩阵的每个元素与自变量矩阵中的每个元素均存在映射。 ​	在可能混淆的地方，小写字母为标量，加粗或希腊字母为向量，大写字母为矩阵。在函数中用$y,Y$做自变量，$f$表示函数关系，$dy,dY$表">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-04-27T07:30:10.000Z">
<meta property="article:modified_time" content="2023-09-19T08:24:20.453Z">
<meta property="article:author" content="M.Q.">
<meta property="article:tag" content="matrix">
<meta property="article:tag" content="math">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>矩阵论 - UTOPIA</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"qdhdusdc.github.io","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/0.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"AgcF1mQgwJbHELHjHp7JRJcB-gzGzoHsz","app_key":"0fENfeqhc4WLt6AcCXBkTH91","server_url":"https://agcf1mqg.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":true}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Home</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="矩阵论"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-04-27 15:30" pubdate>
          April 27, 2023 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          18k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          150 mins
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          Read <span id="leancloud-page-views"></span> times
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">矩阵论</h1>
            
            
              <div class="markdown-body">
                
                <span id="more"></span>

<h1 id="矩阵导数"><a href="#矩阵导数" class="headerlink" title="矩阵导数"></a>矩阵导数</h1><p>​	矩阵导数即对矩阵的多个元素求导，作为一种简便的计算方式。在下面的计算过程中，标量，向量均作为特殊的矩阵，如无指出是标量或向量，矩阵的含义均包括了是标量，向量，矩阵的情况。在矩阵导数中，因变量矩阵的每个元素与自变量矩阵中的每个元素均存在映射。</p>
<p>​	在可能混淆的地方，小写字母为标量，加粗或希腊字母为向量，大写字母为矩阵。在函数中用$y,Y$做自变量，$f$表示函数关系，$dy,dY$表示全微分，$\frac{\partial f}{\partial x_i}$表示$y$对$x_i$的偏导数。将矩阵每个元素视为自变量，要求因变量（标量或矩阵）对该矩阵每个元素的导数构成的矩阵。在不引起混淆的情况下，使用$f’(X)$作为$\frac{\partial f(X)}{\partial X}$的简写形式。</p>
<p>​	在形如$Y&#x3D;f(X_{m\times n})$的矩阵运算中，自变量矩阵$X$每个元素与因变量矩阵$Y$存在映射，因此$Y$为1×1矩阵(或标量)时，$\frac{\partial f}{\partial X}$应有$m\times n$个元素，若$Y$为$p\times q$矩阵时，$\frac{\partial f}{\partial X}$应有$p\times q\times m\times n$个元素。然而，这可能有多种表示方式。本文采用向量化方法，详见下。</p>
<h2 id="计算技巧"><a href="#计算技巧" class="headerlink" title="计算技巧"></a>计算技巧</h2><h3 id="Hadamard积"><a href="#Hadamard积" class="headerlink" title="Hadamard积"></a>Hadamard积</h3><p>对同形矩阵$A,B\in R^{m\times n}$，表现为按元素乘:<br>$$<br>[A\odot B]<em>{ij}&#x3D;a</em>{ij}b_{ij}<br>$$<br>在numpy中可以直接将两个同形矩阵相乘：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>A = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br>B = np.array([[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]])<br>Hadamard_A_B = A*B<br></code></pre></td></tr></table></figure>



<h3 id="Kronecker积"><a href="#Kronecker积" class="headerlink" title="Kronecker积"></a>Kronecker积</h3><p>对于$A\in R^{m\times n},B\in R^{p\times q}$，B对A中每个元素$a_{ij}$进行数乘并将结果取代$a_{ij}$位置:<br>$$<br>[A\otimes B]<em>{ij}&#x3D;a</em>{ij}B<br>$$<br>numpy实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>A = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br>B = np.array([[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]])<br>Kronecker_A_B = np.kron(A, B)<br></code></pre></td></tr></table></figure>

<h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>$vec(X_{m\times n})$表示将$X$逐列转换为列向量：<br>$$<br>[vec(X)]<em>{i}&#x3D;X</em>{[\frac{i}{m}],i-m[\frac{i}{m}]+1}<br>$$<br>其中$[a]$表示对有理数$a$向下取整。在numpy中使用reshape()即可。</p>
<h3 id="变量矩阵等价的唯一性"><a href="#变量矩阵等价的唯一性" class="headerlink" title="变量矩阵等价的唯一性"></a>变量矩阵等价的唯一性</h3><p>​	对于变量矩阵$X_{mn},Y_{nq},Z_{nq}$，若存在$X&#x3D;X’$使得$X$列满秩，且有$XY&#x3D;XZ$，则$Y&#x3D;Z$。关于这个结论，暂未查找到资料。然而这是十分重要的。所以，在此给出一个证明。</p>
<p>​	For all column vectors $X_{.1},X_{.2},…X_{.n}$ in $X$：<br>$$<br>k_1X_{.1}+k_2X_{.2}+…+k_nX_{.n}&#x3D;0 \quad \Rightarrow k_1,k_2,…k_n&#x3D;0\tag{1}<br>$$<br>​	Suppose $Y\neq Z,XY&#x3D;XZ$, then<br>$$<br>X(Y-Z)&#x3D;0<br>$$<br>​	Let $W&#x3D;Y-Z$, then $W\neq 0$, for all column vectors $W_{.1},W_{.2}…W_{.q}$ in $W$:<br>$$<br>\exists {i} \quad s.t.W_{.i}\neq0<br>$$<br>Consider the multiplication of $X$ and $W_{.i}$:<br>$$<br>XW_{.i}&#x3D;(X_{.1},X_{.2},…X_{.n})W_{.i}&#x3D;X_{.1}W_{1i}+X_{.2}W_{2i}+…X_{.n}W_{ni}&#x3D;0<br>$$<br>​	这与(1)矛盾。对于可行满秩的变量矩阵，同理，改为左乘即可。在下面计算矩阵导数时将用到这个结论。</p>
<h2 id="多元函数导数"><a href="#多元函数导数" class="headerlink" title="多元函数导数"></a>多元函数导数</h2><p>​	对于多元函数$$y&#x3D;f(x_1,x_2,…,x_n)$$，其微分$dy$为：<br>$$<br>dy&#x3D;\sum_{i&#x3D;1}^{n}\frac{\partial f}{\partial x_i}dx_i<br>$$<br>以向量形式表示，令<br>$$<br>d\mathbf{x}&#x3D;(dx_1,dx_2,…,dx_n)\<br>\frac{\partial f}{\partial \mathbf{x}}&#x3D;(\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},…,\frac{\partial f}{\partial x_n})<br>$$<br>$\frac{\partial f}{\partial \mathbf{x} }$即在点$(x_1,x_2,…,x_n)$的梯度。则将$dy$表示为<br>$$<br>dy&#x3D;\frac{\partial f}{\partial \mathbf{x}}^Td\mathbf{x}<br>$$</p>
<h2 id="标量对矩阵导数"><a href="#标量对矩阵导数" class="headerlink" title="标量对矩阵导数"></a>标量对矩阵导数</h2><p>​	对于$y&#x3D;f(X_{m\times n})$，$f$将矩阵$X$映射为标量，$y$与$X$的全体元素存在映射关系。仿照多元函数，微分$dy$为：<br>$$<br>dy&#x3D;\sum_{i&#x3D;1}^{m}{\sum_{j&#x3D;1}^{n}{\frac{\partial f}{\partial X_{ij} } } }dX_{ij}<br>$$<br>其中<br>$$<br>dX&#x3D;\left[<br>\begin{matrix}<br>dX_{11} &amp; \cdots &amp; dX_{1n}\<br>\vdots &amp; \ddots &amp;  \<br>dX_{m1} &amp;  &amp; dX_{mn}<br>\end{matrix}<br>\right]<br>$$<br>$dX$是与$X$同形的矩阵，$dX_{ij}$表示$X_{ij}$的微分。在$X$中，每个元素都是一个自变量。令<br>$$<br>\frac{\partial f}{\partial X}&#x3D;\left[<br>\begin{matrix}<br>\frac{\partial{f} }{dX_{11} } &amp; \cdots &amp; \frac{\partial{f} }{dX_{1n} }\<br>\vdots &amp; \ddots &amp;  \<br>\frac{\partial{f} }{dX_{m1} } &amp;  &amp; \frac{\partial{f} }{dX_{mn} }<br>\end{matrix}<br>\right]<br>$$<br>于是<br>$$<br>dy&#x3D;tr(\frac{\partial f}{\partial X}^TdX)\tag{*}<br>$$<br>$tr(X)$ is the trace of $X$，which stands for the summary of the diagonal elements of $X$. Notice that the $X$ here is a square.</p>
<h3 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h3><ol>
<li>若$y&#x3D;f(Y),Y&#x3D;AXB+C$，则根据(*)式：</li>
</ol>
<p>$$<br>dy&#x3D;tr(\frac{\partial f}{\partial Y}^TdY)&#x3D;tr(\frac{\partial f}{\partial X}^TdX)<br>$$</p>
<p>因为<br>$$<br>dY&#x3D;AdXB<br>$$<br>则根据性质10：<br>$$<br>tr(\frac{\partial f}{\partial Y}^TdY)&#x3D;tr(\frac{\partial f}{\partial Y}^TAdXB)&#x3D;tr(B\frac{\partial f}{\partial Y}^TAdX)&#x3D;tr(\frac{\partial f}{\partial X}^TdX)<br>$$<br>于是<br>$$<br>\frac{\partial f}{\partial X}^T&#x3D;B\frac{\partial f}{\partial Y}^TA\<br>\frac{\partial f}{\partial X}&#x3D;A^T\frac{\partial f}{\partial Y}B^T<br>$$</p>
<ol start="2">
<li>若$y&#x3D;f(Y),Y&#x3D;\sigma(X)$，其中$\sigma(X)$为逐元素的函数，因为</li>
</ol>
<p>$$<br>dY&#x3D;\frac{\partial \sigma}{\partial X_{} }\odot dX<br>$$</p>
<p>则根据性质11：<br>$$<br>tr(\frac{\partial f}{\partial Y}^TdY)&#x3D;tr(\frac{\partial f}{\partial Y}^T(\frac{\partial \sigma}{\partial X_{} }\odot dX))&#x3D;tr((\frac{\partial f}{\partial Y}^T\odot \frac{\partial \sigma}{\partial X }^T)dX))&#x3D;tr(\frac{\partial f}{\partial X}^TdX)<br>$$<br>于是<br>$$<br>\frac{\partial f}{\partial X}&#x3D;\frac{\partial f}{\partial Y}\odot \frac{\partial \sigma}{\partial X }<br>$$</p>
<h2 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h2><ol>
<li>$d(X \pm Y)&#x3D;d(X)\pm d(Y)$</li>
<li>$d(X_{m \times p}Y_{p\times n})&#x3D;(dX)Y+X(dY)$</li>
</ol>
<p>​	证明：<br>$$<br>d(XY)<em>{ij}&#x3D;d(\sum</em>{i&#x3D;1}^p{X_{ip}Y_{pj} })&#x3D;\sum_{i&#x3D;1}^p(dX_{ip}Y_{pj})+\sum_{i&#x3D;1}^p(X_{ip}dY_{pj})&#x3D;(dX)<em>{i.}Y</em>{.j}+(dX)<em>{.i}Y</em>{j.}<br>$$</p>
<ol start="3">
<li><p>$d(X^T)&#x3D;(dX)^T$</p>
</li>
<li><p>$(dX)^{-1}&#x3D;-X^{-1}dXX^{-1}$</p>
</li>
</ol>
<p>​	证明：<br>$$<br>XX^{-1}&#x3D;I\<br>Xd(X^{-1})+(dX)X^{-1}&#x3D;d(XX^{-1})&#x3D;d(I)&#x3D;0\<br>(dX)^{-1}&#x3D;-X^{-1}dXX^{-1}<br>$$</p>
<ol start="5">
<li>$d(\sigma(X))&#x3D;\frac{\partial \sigma}{\partial X}\odot dX$</li>
</ol>
<p>证明：<br>$$<br>\begin{aligned}<br>d(\sigma(X))<br>&#x3D;\left[<br>\begin{matrix}<br>\frac{\partial \sigma}{\partial X_{11}}dX_{11} &amp; \cdots &amp; \frac{\partial \sigma}{\partial X_{1n}}dX_{1n}\<br>\vdots &amp; \ddots &amp;  \<br>\frac{\partial \sigma}{\partial X_{m1}}dX_{m1} &amp;  &amp; \frac{\partial \sigma}{\partial X_{mn} }dX_{mn}<br>\end{matrix}<br>\right]&#x3D; \left[<br>\begin{matrix}<br>\frac{\partial \sigma}{\partial X_{11} } &amp; \cdots &amp; \frac{\partial \sigma}{\partial X_{1n} }\<br>\vdots &amp; \ddots &amp;  \<br>\frac{\partial \sigma}{\partial X_{m1} } &amp;  &amp; \frac{\partial \sigma}{\partial X_{mn} }<br>\end{matrix}<br>\right]\odot\left[<br>\begin{matrix}<br>dX_{11} &amp; \cdots &amp;dX_{1n}\<br>\vdots &amp; \ddots &amp;  \<br>dX_{m1} &amp;  &amp; dX_{mn}<br>\end{matrix}<br>\right]<br>&#x3D;\frac{\partial \sigma}{\partial X_{} }\odot dX<br>\end{aligned}<br>$$</p>
<ol start="6">
<li><p>$d(X\odot Y)&#x3D;dX\odot Y+X\odot dY$</p>
</li>
<li><p>$x$为标量，或1×1矩阵，$x&#x3D;tr(x)$</p>
</li>
<li><p>$tr(X^T)&#x3D;tr(X)$</p>
</li>
<li><p>$dtr(X)&#x3D;tr(dX)$</p>
</li>
<li><p>$tr(XY)&#x3D;tr(YX)&#x3D;tr(X^TY^T)$</p>
</li>
<li><p>$tr(A^T(B\odot C))&#x3D;tr((A\odot B)^TC)&#x3D;tr((A^T\odot B^T)C)$     可以看到，只有$B$变成了$B^T$</p>
</li>
<li><p>$u,v$是维数相同的向量，$\mathbf 1^T(u\odot v)&#x3D;u^Tv$</p>
</li>
<li><p>$u,v$是维数相同的向量，$u^Tv&#x3D;v^Tu$</p>
</li>
<li><p>$vec(A+B)&#x3D;vec(A)+vec(B)$</p>
</li>
<li><p>$vec(AXB)&#x3D;(B^T\otimes A)vec(X)$</p>
</li>
</ol>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>简便起见，将$\frac{\partial f(a)}{\partial a}$记为$f’(a)$</p>
<h3 id="简单单层网络"><a href="#简单单层网络" class="headerlink" title="简单单层网络"></a>简单单层网络</h3><p>已知<br>$$<br>\hat{y}&#x3D;XW,\quad X\in R^{1\times m},W\in R^{m\times n}<br>$$<br>损失函数<br>$$<br>L&#x3D;\frac{1}{2}(\hat{y}-y)({\hat{y}-y)^T }<br>$$<br>$W$更新公式<br>$$<br>W-&#x3D;lr<em>\frac{\partial L}{\partial W}<br>$$<br>为了求$\frac{\partial L}{\partial W}$，根据(</em>)式先求$dL$<br>$$<br>\begin{aligned}<br>dL&amp;&#x3D;\frac{1}{2}d((\hat{y}-y))({\hat{y}-y)^T }+\frac{1}{2}(\hat{y}-y)d(({\hat{y}-y)^T) }\<br>&amp;&#x3D;\frac{1}{2}(XdW)({\hat{y}-y)^T }+\frac{1}{2}(\hat{y}-y)(XdW)^T \<br>\end{aligned}<br>$$<br>注意到$(\hat{y}-y)^T$与$dWX$均为向量，则根据性质13：<br>$$<br>dL&#x3D;(XdW)(\hat{y}-y)^T\tag{1}<br>$$<br>由于<br>$$<br>dL&#x3D;tr(\frac{\partial f}{\partial W}^TdW)<br>$$<br>则要将(1)式化为$tr(M^TdW)$形式，以得出$\frac{\partial L}{\partial W}$。注意到$dL$为$1\times1$方阵，于是<br>$$<br>dL&#x3D;tr(dL)&#x3D;tr((XdW)(\hat{y}-y)^T)&#x3D;tr((\hat{y}-y)^TXdW)&#x3D;tr((X^T(\hat{y}-y))^TdW)<br>$$<br>因此有<br>$$<br>\frac{\partial L}{\partial W}&#x3D;X^T(\hat{y}-y)<br>$$</p>
<h3 id="单隐藏层网络"><a href="#单隐藏层网络" class="headerlink" title="单隐藏层网络"></a>单隐藏层网络</h3><p>已知<br>$$<br>\hat{y}&#x3D;g(hW_2+b_2),h&#x3D;g(XW_1+b_1)\<br>\quad X\in R^{1\times m},W_1\in R^{m\times n},b_1\in R^{1\times n},W_2\in R^{n\times q},b_2\in R^{1\times q}<br>$$<br>损失函数<br>$$<br>L&#x3D;\frac{1}{2}(\hat{y}-y)({\hat{y}-y)^T }<br>$$<br>参数更新公式<br>$$<br>W_1&#x3D;lr<em>\frac{\partial L}{\partial W_1},W_2-&#x3D;lr</em>\frac{\partial L}{\partial W_2},b_1-&#x3D;lr<em>\frac{\partial L}{\partial b_1},b_2-&#x3D;lr</em>\frac{\partial L}{\partial b_2}<br>$$<br>在求某个变量的梯度时，暂且将其他变量视为常量。先求$\frac{\partial L}{\partial b_2}$，由性质11<br>$$<br>\begin{aligned}<br>dL&amp;&#x3D;tr(dL)&#x3D;tr((g’(hW_2+b_2)\odot db_2)(\hat{y}-y)^T)\<br>&amp;&#x3D;tr((\hat{y}-y)(g’(hW_2+b_2)^T\odot db_2^T))\<br>&amp;&#x3D;tr(((\hat{y}-y)\odot g’(hW_2+b_2))db_2^T)\<br>&amp;&#x3D;tr(((\hat{y}-y)\odot g’(hW_2+b_2))^Tdb_2)<br>\end{aligned}<br>$$<br>于是<br>$$<br>\frac{\partial L}{\partial b_2}&#x3D;(\hat{y}-y)\odot g’(hW_2+b_2)<br>$$<br>对于$\frac{\partial L}{\partial W_2}$：<br>$$<br>\begin{aligned}<br>dL&amp;&#x3D;tr(dL)&#x3D;tr((g’(hW_2+b_2)\odot (hdW_2))(\hat{y}-y)^T)\<br>&amp;&#x3D;tr((\hat{y}-y)(g’(hW_2+b_2)^T\odot (hdW_2)^T))\<br>&amp;&#x3D;tr(((\hat{y}-y)\odot g’(hW_2+b_2))(hdW_2)^T)\<br>&amp;&#x3D;tr(((\hat{y}-y)\odot g’(hW_2+b_2))^ThdW_2)\<br>&amp;&#x3D;tr(((h^T((\hat{y}-y)\odot g’(hW_2+b_2)))^TdW_2)<br>\end{aligned}<br>$$<br>于是<br>$$<br>\frac{\partial L}{\partial W_2}&#x3D;h^T((\hat{y}-y)\odot g’(hW_2+b_2))<br>$$<br>为求$\frac{\partial L}{\partial b_1}$，$\frac{\partial L}{\partial W_1}$，根据链式法则，先求出$\frac{\partial L}{\partial h}$<br>$$<br>\begin{aligned}<br>dL&amp;&#x3D;tr(dL)&#x3D;tr((g’(hW_2+b_2)\odot (dhW_2))(\hat{y}-y)^T)\<br>&amp;&#x3D;tr((\hat{y}-y)(g’(hW_2+b_2)^T\odot (dhW_2)^T))\<br>&amp;&#x3D;tr(((\hat{y}-y)\odot g’(hW_2+b_2))(dhW_2)^T)\<br>&amp;&#x3D;tr(((\hat{y}-y)\odot g’(hW_2+b_2))^TdhW_2)\<br>&amp;&#x3D;tr((((\hat{y}-y)\odot g’(hW_2+b_2))W_2^T)^Tdh)<br>\end{aligned}<br>$$<br>于是<br>$$<br>\frac{\partial L}{\partial h}&#x3D;((\hat{y}-y)\odot g’(hW_2+b_2))W_2^T<br>$$<br>设$a&#x3D;XW_1+b_1$，则$h&#x3D;g(a)$，根据链式法则<br>$$<br>\frac{\partial L}{\partial a}&#x3D;\frac{\partial L}{\partial h}\odot \frac{\partial g}{\partial a}\<br>\frac{\partial L}{\partial b_1}&#x3D;\frac{\partial L}{\partial a}\<br>\frac{\partial L}{\partial W_1}&#x3D;X^T\frac{\partial L}{\partial a}<br>$$<br>于是得到<br>$$<br>\frac{\partial L}{\partial b_1}&#x3D;(((\hat{y}-y)\odot g’(hW_2+b_2))W_2^T)\odot \frac{\partial g}{\partial a}\<br>\frac{\partial L}{\partial W_1}&#x3D;X^T((((\hat{y}-y)\odot g’(hW_2+b_2))W_2^T)\odot \frac{\partial g}{\partial a})<br>$$</p>
<h1 id="特征值与特征向量"><a href="#特征值与特征向量" class="headerlink" title="特征值与特征向量"></a>特征值与特征向量</h1><p>“There are some matrices for which there are not a full set of eigenvectors. that’s really the main sort of annoying point in the whole subject of linear algebra is some matrices don’t have enough eigenvectors. “</p>
<p>对于任何一个n阶方阵$A_{n\times n}$，它的特征值一定有n个（算上虚特征值和重复特征值），换句话说这等价于实数特征值不超过n个，因为求特征值用$|A-\lambda I|&#x3D;0$,最终必定可以化为$\lambda$的n阶方程，实数解的个数不超过n（代数基本定理）。</p>
<ul>
<li>定理1：不同特征值对应的特征向量线性无关。证明：</li>
</ul>
<p>设$A$有两个特征值$\lambda_{1},\lambda_{2}$ 和两个对应的特征向量$x_1,x_2$，且$\lambda_{1}\neq\lambda_{2}$.假设$x_1,x_2$线性相关，则$x_1&#x3D;kx_2$成立，则$Ax_2&#x3D;\lambda_2 x_2&#x3D;A kx_1&#x3D;kAx_1&#x3D;k\lambda_1x_1$,所以(1)当$\lambda_1\ne0$，有$x_2&#x3D;\frac{\lambda_1}{\lambda_2}kx_1&#x3D;kx_1$,$\lambda_1&#x3D;\lambda_2$，矛盾。(2)当$\lambda_1&#x3D;0$,因为特征向量非零，所以$\lambda_2&#x3D;0&#x3D;\lambda_1$，矛盾。于是得证</p>
<h2 id="实数特征值与虚数特征值"><a href="#实数特征值与虚数特征值" class="headerlink" title="实数特征值与虚数特征值"></a>实数特征值与虚数特征值</h2><p>从矩阵$A$的特征值$\lambda$与特征向量$x$的计算来看，即$Ax&#x3D;\lambda x$，在$x$方向上，矩阵$A$只起到了拉伸的作用。若特征值为0，则$A$将某些向量映射到0空间，这说明$A$不满秩。若特征值为实数，说明对某些方向上的向量，$A$仅起到了拉伸的效果，下面将这种矩阵称作拉伸矩阵。若特征值是虚数，说明$A$对所有方向的向量都起到了旋转的效果，下面将这种矩阵称为旋转矩阵。</p>
<h3 id="拉伸矩阵"><a href="#拉伸矩阵" class="headerlink" title="拉伸矩阵"></a>拉伸矩阵</h3><p>对于一个特征向量空间内的向量，拉伸矩阵让一个向量更靠近它最大的特征值对应的特征向量方向。也就是说，在拉伸矩阵的特征向量空间中的向量，一直让它左乘这个拉伸矩阵，那么它最终会逼近最大特征值对应的特征向量方向。为什么要强调是特征向量空间内？因为前面的定理1说明，r个不同特征值可以形成r维特征向量空间，而相同的k重特征值却不一定能形成k维特征向量空间（因为几何重数不大于代数重数，Jordan标准型给出证明）,因此，特征向量空间的维度不一定等于原矩阵的维度（即使原矩阵是满秩的）。因此要强调是特征向量空间内的向量，因为它可以由特征向量表出。</p>
<p>证明：设矩阵$A_{n\times n}$有$r$个不同的特征向量$\lambda_1,…\lambda_r$，有一个特征向量空间内的向量$x$,特征向量的基底是$e_1,e_2,…e_r$,于是<br>$$<br>\exists k_1,k_2,…k_n,s.t.\quad x&#x3D;\sum_{i&#x3D;1}^rk_ie_i<br>$$<br>则<br>$$<br>\lim_{y\to \infty}A^yx&#x3D;A^y\sum_{i&#x3D;1}^rk_ie_i&#x3D;\sum_{i&#x3D;1}^rk_iA^ye_i&#x3D;\sum_{i&#x3D;1}^rk_i\lambda_i^ye_i<br>$$<br>容易得知最大的特征值的特征向量方向对结果方向影响最大</p>
<h3 id="旋转矩阵的分解"><a href="#旋转矩阵的分解" class="headerlink" title="旋转矩阵的分解"></a>旋转矩阵的分解</h3><p>旋转矩阵对所有实数向量都起到了旋转的效果，但也可能起到拉伸效果。事实上，旋转矩阵可以分解成一个纯旋转矩阵和一个拉伸矩阵的矩阵乘积。（<strong>该结论暂未证明</strong>）纯旋转矩阵指的是，左乘纯旋转矩阵只改变向量的方向，而不改变向量的模长。纯旋转矩阵一定是正交矩阵。满足下列关系的矩阵为纯旋转矩阵：<br>$$<br>\forall \textbf x,k\in[-1,1],cos&lt;\textbf x,A\textbf x&gt;&#x3D;\frac{\sum_{i&#x3D;1}^{n}\sum_{j&#x3D;1}^{n}A_{ij}x_ix_j}{\sum_{i&#x3D;1}^{n}a_i^2}\equiv k<br>$$<br>即旋转矩阵对任意向量的旋转角均相等。这个结论直接计算化简即得。                                                                                                                                                         </p>
<h2 id="相似对角化"><a href="#相似对角化" class="headerlink" title="相似对角化"></a>相似对角化</h2><p>矩阵$A$可以相似于对角矩阵$\Lambda$，当且仅当矩阵$A_{n\times n}$有n个线性无关的特征向量。即：<br>$$<br>\begin{aligned}<br>\Lambda&amp;&#x3D;P^{-1}AP\<br>P&amp;&#x3D;(p_1,p_2,..p_n)\<br>\Lambda&amp;&#x3D;\left[<br>\begin{matrix}<br>\lambda_1 &amp;  &amp; \<br> &amp; \ddots &amp;  \<br>&amp;  &amp; \lambda_{n}<br>\end{matrix}<br>\right]<br>\end{aligned}<br>$$<br>其中，$\lambda_i$对应的特征向量为$p_i$. 直接计算$AP&#x3D;P\Lambda$即可证明。若$A$相似于对角矩阵，则很容易算$A$的次方：<br>$$<br>A^k&#x3D;P\Lambda^kP^{-1}<br>$$</p>
<h1 id="Jordan标准型"><a href="#Jordan标准型" class="headerlink" title="Jordan标准型"></a>Jordan标准型</h1><p>任何n阶方阵可以相似于具有以下形式的矩阵：<br>$$<br>J_{n\times n}&#x3D;\left[\begin{matrix}<br>J_1 &amp;  &amp; &amp;\<br> &amp; J_2&amp; \<br>  &amp;  &amp; \ddots&amp; \<br>&amp;  &amp;&amp; J_{k}<br>\end{matrix}<br>\right],J_i&#x3D;\left[\begin{matrix}<br>\lambda_i &amp;1 &amp; &amp; \<br> &amp; \lambda_i &amp;1 &amp;  \<br>  &amp;  &amp;\ddots &amp; \ddots \<br>&amp; &amp; &amp; \lambda_{i}<br>\end{matrix}<br>\right]<br>$$<br>其中$\sum_{i&#x3D;1}^nR(J_i)&#x3D;n$. 在Gilbert Strang 的 <em>Linear Algebra and Its Applications</em>, Appendix B 一章通过构造$P$证明了$\forall A\in R^{n\times n},\exists P,  s.t.A&#x3D;P^{-1}JP$.</p>
<p>下面分析Jordan型的意义。</p>
<p>注意小块最后一行，这一行只有一列非零，因此，$|J_i-\lambda_i I|&#x3D;0$，$\lambda_i$是$J$的特征值。</p>
<p>接下来考虑$J$的特征向量。对于小块$J_i$，$(J_i-\lambda_iI)x&#x3D;0$的解空间维数为1，因此小块$J_i$对应一个1维特征向量空间。于是容易理解，代数重数大于等于几何重数的原因。每个小块至少是1维，当小块为1维时，这个小块的特征值对应的特征向量空间是1维，此时代数重数等于几何重数。如果小块大于1维，代数重数大于几何重数，这个小块的代数重数等于小块的维数，特征向量空间是1维。相同的特征值可能对应不同的特征向量，此时不同小块中存在相同的特征值，而这些不同的小块对应不同的特征向量。</p>
<h1 id="变换"><a href="#变换" class="headerlink" title="变换"></a>变换</h1><p>矩阵乘向量在几何意义上就是向量的线性变换，满秩矩阵把这个向量映射到同维空间的另一个向量，不满秩矩阵把它映射到低维空间的另一个向量。矩阵乘矩阵在几何意义上是基底变换，一个矩阵的每一列视为一个基底向量，基底变换就是把一组基底映射到另一组基底，满秩矩阵把基底映射到同维空间的另一组基底，不满秩矩阵把基底映射到低维空间的另一组基底。</p>
<h2 id="基底变换"><a href="#基底变换" class="headerlink" title="基底变换"></a>基底变换</h2><p>一个可逆矩阵可以将一组基底变换到在同维空间中的任意一组基底。这其实就是矩阵乘的意义。设有一组基底$(e_1,e_2,…e_n)$，他们构成矩阵$M$，其中$e_i$是$n\times 1$列向量，对于n维空间中任意一组基底构成的矩阵$N$，设经过左乘矩阵$P$从$M$变换到$N$，则<br>$$<br>\begin{aligned}<br>N&amp;&#x3D;MP\<br>P&amp;&#x3D;M^{-1}N<br>\end{aligned}<br>$$<br>于是可知存在唯一的矩阵$P$使得基底$M$变换到基底$N$</p>
<p>基底变换定理：基底变换不改变向量本身，通过基底变换变的是向量在这组基底下的各个分量的取值。也就是说，在标准正交基下（或者任何一组作为参考的基底）的向量是不变的。基底$M$经过基底变换得到基底$N$，即有$N&#x3D;MP$，对同一个标准正交基下的向量$\textbf x$，其在$M,N$下的分量为$\alpha,\alpha’$，即有$M\alpha&#x3D;N\alpha’&#x3D;\textbf x,\alpha’&#x3D;P^{-1}\alpha$</p>
<p>从这个结论来看，向量的线性变换的意义可以归结为以下两个等价的观点：</p>
<p>1）在基底不变的情况下，$\textbf x$变换到同基底下的$P\textbf x$</p>
<p>2）在向量不变的情况下，基底$M$变换到$PM$，$\textbf x$在新基底$PM$下的分量为$P^{-1}\textbf x$</p>
<p>这两种观点是等价的，区别只在于选择向量还是基底作为参考，而用两种观点来描述的他们之间的相对关系是一样的。基底不变，向量变等价于向量不变，基底变。</p>
<h2 id="相似变换"><a href="#相似变换" class="headerlink" title="相似变换"></a>相似变换</h2><p>基底$M$经过基底变换得到基底$N$，即有$N&#x3D;MP$，对同一个标准正交基下的向量$\textbf x$，其在$M,N$下为$\alpha,\alpha’$，即有$M\alpha&#x3D;N\alpha’&#x3D;\textbf x,\alpha’&#x3D;P^{-1}\alpha$，$\alpha$经过线性变换得到向量$\beta$，即$\beta&#x3D;A\alpha$，设$\beta$在$N$下为$\beta’$，于是有$\beta’&#x3D;P^{-1}\beta$，设矩阵$A’$使得$\beta’&#x3D;A’\alpha’$，于是<br>$$<br>\beta’&#x3D;A’\alpha’&#x3D;P^{-1}\beta&#x3D;P^{-1}A\alpha&#x3D;P^{-1}AP\alpha’<br>$$<br>则<br>$$<br>A’&#x3D;P^{-1}AP\tag{2}<br>$$<br>上面的结论说明，一个向量被映射到另一个向量，在两个不同的基底下的两个变换关系之间具有(2)式描述的关系。因此，相似变换是同一个线性变换在不同基底下的不同表现形式。</p>
<h3 id="相似对角化-1"><a href="#相似对角化-1" class="headerlink" title="相似对角化"></a>相似对角化</h3><p>若方阵经相似变换能化为对角矩阵，则这个方阵可相似对角化。</p>
<p>$A_{n\times n}$可相似对角化$\iff$$A_{n\times n}$有n个线性无关的特征向量</p>
<p>证明：充分性：若$A_{n\times n}$可相似对角化，设其相似于对角矩阵$B&#x3D;diag(b_1,b_2,…b_n)$，且$B&#x3D;P^{-1}AP$，则$PB&#x3D;AP$. 将$P$以列向量形式表示，$P&#x3D;{p_{.1},p_{.2},…p_{.n})$ . 因为<br>$$<br>PB&#x3D;(p_{.1},p_{.2},…p_{.n})<br>$$<br>从几何角度考虑，相似对角化实质上是把原矩阵$A$对应的基底变换到特征向量方向上，这时新的基底两两正交。</p>
<h2 id="合同变换"><a href="#合同变换" class="headerlink" title="合同变换"></a>合同变换</h2><p>任意一个二次型<br>$$<br>f(\textbf x)&#x3D;\sum_{i&#x3D;1}^n\sum_{j&#x3D;1}^{n} k_{ij}x_ix_j,\textbf x&#x3D;(x_1,x_2,…x_n)^T<br>$$<br>均可以化为矩阵乘形式$\textbf x^TA\textbf x$，可以证明这样的$A$有无数个。当$A$为对称矩阵，则$A$是唯一的，此时有<br>$$<br>a_{ij}&#x3D;\left{<br>\begin{array}{l}<br> \frac{k_{ij}+k_{ji} }{2},\quad i\neq j \<br>k_{ij},\quad i&#x3D;j \<br>\end{array}<br>\right.<br>$$<br>当对基底$M$下的向量$\textbf x$进行线性变换$C^{-1}$(为了让结论形式看起来简洁，写成C的逆矩阵。其实只要这里是个可逆矩阵就行)，得到另一个基底$N$下的向量$\textbf y$，即$\textbf x&#x3D;C\textbf y$，这时$f(\textbf x)&#x3D;g(\textbf y)&#x3D;\textbf x^TA\textbf x&#x3D;\textbf y^TC^TAC\textbf y&#x3D;\textbf y^TA’\textbf y$，于是<br>$$<br>A’&#x3D;C^TAC<br>$$<br>上面的结论说明，同一个二次型在两个不同的基底下，两个变换关系之间具有(2)式描述的关系。</p>
<h3 id="惯性定理"><a href="#惯性定理" class="headerlink" title="惯性定理"></a>惯性定理</h3><p>上面说到，合同变换可以视为同一个二次型在两个不同的基底下的不同的描述形式，因此，我们可以通过变换基底，让二次型只含有平方项，不含有$x_1x_2$这种混合乘积，并且平方项系数只能为0，1，-1，也就是化成标准型。可以想象，一定存在某一个位置可以达到这种效果，例如一个椭圆，两个基底向量分别它的长轴和短轴重合时，就只含有平方项，将长轴一半和短轴一半长度作为两个基底向量的单位长度，就得到标准型，变成了一个单位圆。事实上，将任意二次型化成标准型存在且只存在一种合同变换。下面证明这个结论。</p>
<p>证明：设有二次型$f(\textbf x)&#x3D;g(\textbf y)&#x3D;\textbf x^TA\textbf x&#x3D;\textbf y^T B\textbf y$，其中$B$为对角矩阵。若证明了$B$是对角矩阵，则可将$B$化为只含0，1，-1的对角矩阵。于是原命题等价于证存在$C$满足$B&#x3D;C^TAC$.</p>
<p>因为$A$为对称矩阵，根据对称矩阵的结论，存在正交矩阵$P$使得$A&#x3D;P^{-1}B P$，根据正交矩阵的结论，$P^{-1}&#x3D;P^T$，于是$B&#x3D;PAP^T$，即证</p>
<p>标准型中正、负号的个数称为正、负惯性系数。因为合同变换是同一个二次型在不同基底下的形式，而将任意二次型化成标准型存在且只存在一种合同变换，因此易知合同变换不改变正负惯性系数。</p>
<h2 id="正交化"><a href="#正交化" class="headerlink" title="正交化"></a>正交化</h2><h3 id="向量投影"><a href="#向量投影" class="headerlink" title="向量投影"></a>向量投影</h3><p>向量$\alpha$在一组标准正交基$V&#x3D;(\beta_1,\beta_2,…\beta_m)$下的投影为<br>$$<br>proj_V\alpha&#x3D;\sum_{i&#x3D;1}^m(\alpha\cdot\beta_i)\beta_i<br>$$<br>其中，$\alpha,\beta_i$均为n维向量（$n\ge m$），$\alpha\cdot\beta_i$是$\alpha$在$\beta_i$方向上投影的向量长度。当$n&#x3D;m$时，$\alpha$可由$V$线性表出，此时$\alpha&#x3D;proj_V\alpha$，当$n&gt; m$，$\alpha_p&#x3D;\alpha-proj_V\alpha$与空间$V$垂直。这里与空间垂直的意义是，$\alpha_p$与每个基向量垂直。向量与线和平面垂直的情况容易想象。从想象中可以得到以下定理（<strong>该结论暂未证明</strong>）：所有$\alpha$对应的$\alpha_p$形成的向量空间的维度&#x3D;m-n.</p>
<h3 id="正交化-1"><a href="#正交化-1" class="headerlink" title="正交化"></a>正交化</h3><p>任意线性无关的一组向量均可以化为该向量空间下的一组标准正交基。</p>
<p>根据投影定义可以知道，任何一个与向量空间$V$中所有基向量线性无关的向量$\alpha$均能表示成这个空间内的投影$proj_V\alpha$+与这个空间垂直的向量$\alpha_p$.因此，可以采用如下方法得到一组基$(\beta_1,\beta_2,…\beta_m)$的标准正交基：</p>
<p>1）取第一个向量$\beta_1$，将其归一化，加入到子空间$W$中</p>
<p>2）取第k(k&gt;1)个向量$\beta_k$，计算其与子空间$W$垂直的向量$\beta_{kp}&#x3D;\beta-proj_V\beta_{k}$，并将$\beta_{kp}$归一化，加入到$W$中</p>
<p>3）重复2），直到k&#x3D;n</p>
<h3 id="正交向量空间"><a href="#正交向量空间" class="headerlink" title="正交向量空间"></a>正交向量空间</h3><p>若k维空间$V$中的一组基$(\alpha_1,…\alpha_k)$与另一r维空间$W$中的一组基$(\beta_1,…\beta_r)$正交($\alpha_i,\beta_j$为同维向量)，则空间$V$与空间$W$中所有向量均正交。</p>
<p>证明：设$V$中一向量$\alpha&#x3D;\sum_{i&#x3D;1}^{k}m_i\alpha_i$,$W$中一向量$\beta&#x3D;\sum_{i&#x3D;1}^{r}n_i\beta_i$, 则<br>$$<br>\alpha \cdot \beta&#x3D;(\sum_{i&#x3D;1}^{k}m_i\alpha_i)(\sum_{i&#x3D;1}^{r}n_i\beta_i)&#x3D;\sum_{i&#x3D;1}^{k}\sum_{j&#x3D;1}^r(m_in_j\alpha_i\cdot\beta_j)&#x3D;0<br>$$<br>于是两空间中任意向量均正交。</p>
<h1 id="特殊矩阵"><a href="#特殊矩阵" class="headerlink" title="特殊矩阵"></a>特殊矩阵</h1><h2 id="正交矩阵"><a href="#正交矩阵" class="headerlink" title="正交矩阵"></a>正交矩阵</h2><ul>
<li>正交矩阵满足$A^{-1}&#x3D;A^T$</li>
</ul>
<p>将矩阵写成列向量形式$A&#x3D;(a_1,a_2,…a_n)$，若<br>$$<br>a_i\cdot a_{j}&#x3D;\left{<br>\begin{array}{l}<br> 0,\quad i\neq j \<br>1,\quad i&#x3D;j \<br>\end{array}<br>\right.<br>$$<br>则$A$为正交矩阵。</p>
<p>若$A$为正交矩阵，则$A^TA&#x3D;I$，因为<br>$$<br>A^TA&#x3D;\left(<br>\begin{matrix}<br>a_1^T \<br>a_2^T    \<br>…\<br> a_n^T<br>\end{matrix}<br>\right)(a_1,a_2,…a_n)&#x3D;\left(<br>\begin{matrix}<br>a_1^Ta_1 &amp;a_1^Ta_2 &amp;…  \<br>a_2^Ta_1 &amp; \ddots &amp;  \<br>…&amp;  &amp; a_n^Ta_n<br>\end{matrix}<br>\right)&#x3D;I<br>$$<br>于是可得$A^{-1}&#x3D;A^T$</p>
<h2 id="对称矩阵"><a href="#对称矩阵" class="headerlink" title="对称矩阵"></a>对称矩阵</h2><ul>
<li>任何一个矩阵 A可以表示为一个反对称矩阵M + 一个对称矩阵S</li>
</ul>
<p>证明：如果原命题成立，只要证明存在这样的$M$和$N$，因为<br>$$<br>\begin{aligned}<br>A_{ij}&amp;&#x3D;M_{ij}+S_{ij}\<br>A_{ji}&amp;&#x3D;M_{ji}+S_{ji}&#x3D;-M_{ji}+S_{ij}<br>\end{aligned}<br>$$<br>两个未知数$A_{ij},A_{ji}$两个等式可以解出两个未知数$M_{ij},S_{ij}$，即证</p>
<ul>
<li>矩阵$A$是实对称矩阵$\iff$$A&#x3D;P^{-1}\Lambda P$，$\Lambda$是对角矩阵，$P$是正交矩阵</li>
</ul>
<p>证明：1）充分性：若$A$为实对称矩阵，设$J&#x3D;P^{-1}AP$,  $J$为Jordan矩阵。假设$A$不可对角化，则$J$存在维数大于1的小块。不妨设$J_1$维数大于1，即<br>$$<br>J_1&#x3D;\left[\begin{matrix}<br>\lambda_1 &amp;1 &amp; &amp; \<br> &amp; \lambda_1 &amp;\ddots &amp;  \<br>  &amp;  &amp;\ddots &amp;  \<br>\end{matrix}<br>\right]<br>$$<br>令$B&#x3D;P^TP$，则$B$为对称矩阵，$b_{12}&#x3D;b_{21}$，且$b_{11}&#x3D;\sum_{i&#x3D;1}^{n}p_{i1}^2$，因为$P$可逆，所以$p_{i1}$不全为0，$b_{11}&gt;0$. 因为$P^TAP&#x3D;P^TPJ&#x3D;BJ$, $PAP^T$为对称矩阵，则$BJ$为对称矩阵。因为$BJ$第1行第2列元素为$b_{11}+\lambda_1b_{12}$，第2行第1列元素为$\lambda_1b_{21}$，则$BJ$为非对称矩阵，矛盾。因此$A$可对角化。</p>
<p>下面证明实对称矩阵$A$不同特征值对应的特征向量之间相互正交。</p>
<p>设$\alpha_1,\alpha_2$是$A$的两个不同的特征向量，$A\alpha_1&#x3D;\lambda_1\alpha_1,A\alpha_2&#x3D;\lambda_2\alpha_2$, 且$\lambda_2\ne\lambda_1$, 分别左乘$\alpha_2^T,\alpha_1^T$, 则$\alpha_2^TA\alpha_1&#x3D;\lambda_1\alpha_2^T\alpha_1,\alpha_1^TA\alpha_2&#x3D;\lambda_2\alpha_1^T\alpha_2$, 因为向量数乘可交换，所以$\alpha_2^TA\alpha_1&#x3D;(A\alpha_2)^T\alpha_1&#x3D;\alpha_1^T(A\alpha_2)&#x3D;\alpha_1^TA\alpha_2$, 于是$\lambda_2\alpha_1^T\alpha_2&#x3D;\lambda_1\alpha_2^T\alpha_1$, $(\lambda_2-\lambda_1)\alpha_1^T\alpha_2&#x3D;0$, 则$\alpha_1^T\alpha_2&#x3D;0$.</p>
<p>因此$A$的不同特征值对应的特征向量构成正交向量空间，在每个r维特征向量空间中选取r个线性无关的模长为1的向量，按列构成矩阵$P$即可。</p>
<p>2）必要性：若$A&#x3D;P^{-1}\Lambda P$，$\Lambda$是对角矩阵，$P$是正交矩阵，则$P^{-1}&#x3D;P^T,A&#x3D;P^T\Lambda P,A^T&#x3D;P^T\Lambda^TP&#x3D;P^T\Lambda P&#x3D;A$, 则$A$为对称矩阵。下面证明$A$的特征值均为实数。</p>
<p>设$Ax&#x3D;\lambda x$, $\lambda&#x3D;a+bi$, $\bar{x}^TAx&#x3D;\lambda \bar{x}^Tx$, 直接计算可证$\bar{AB}&#x3D;\bar{A}\bar{B}$, 因此$\bar{x}^TAx&#x3D;\bar{x}^T\bar{A}^Tx&#x3D;(\bar{Ax})^Tx&#x3D;\lambda \bar{x}^Tx$, 于是$\bar{Ax}&#x3D;\lambda \bar{x}$, 则$Ax&#x3D;\bar{\lambda} x$, 则$b&#x3D;0$. 因此$A$所有特征值为实数。</p>
<h2 id="正定矩阵"><a href="#正定矩阵" class="headerlink" title="正定矩阵"></a>正定矩阵</h2><p>若实对称矩阵所有特征值均&gt;0,则为正定，若均$\ge0$, 则为半正定，均&lt;0为负定矩阵。</p>
<p>从几何意义考虑，一个正定矩阵$A_{n\times n}$满足$\forall x\in R^{n\times1}, x^TAx&gt;0$，这表明向量$x$与$Ax$的内积为正，如对于二，三维向量就是夹角小于$\frac{\pi}{2}$. </p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Math/" class="category-chain-item">Math</a>
  
  
    <span>></span>
    
  <a href="/categories/Math/Matrix/" class="category-chain-item">Matrix</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/matrix/">#matrix</a>
      
        <a href="/tags/math/">#math</a>
      
    </div>
  
</div>


              

              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/07/08/%E6%B7%B7%E4%B9%B1%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" title="混乱的机器学习">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">混乱的机器学习</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/04/06/Euler-s-Formula/" title="Euler&#39;s Formula">
                        <span class="hidden-mobile">Euler&#39;s Formula</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"AgcF1mQgwJbHELHjHp7JRJcB-gzGzoHsz","appKey":"0fENfeqhc4WLt6AcCXBkTH91","path":"window.location.pathname","placeholder":"Submit a comment here","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"https://agcf1mqg.lc-cn-n1-shared.com","emojiCDN":null,"emojiMaps":null,"enableQQ":true},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        Views: 
        <span id="leancloud-site-pv"></span>
        
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        Visitors: 
        <span id="leancloud-site-uv"></span>
        
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
