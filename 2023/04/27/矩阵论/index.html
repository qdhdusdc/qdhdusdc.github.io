

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/cube32.png">
  <link rel="icon" href="/img/cube32.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="M.Q.">
  <meta name="keywords" content="">
  
    <meta name="description" content="矩阵导数 ​ 矩阵导数即对矩阵的多个元素求导，作为一种简便的计算方式。在下面的计算过程中，标量，向量均作为特殊的矩阵，如无指出是标量或向量，矩阵的含义均包括了是标量，向量，矩阵的情况。在矩阵导数中，因变量矩阵的每个元素与自变量矩阵中的每个元素均存在映射。 ​ 在可能混淆的地方，小写字母为标量，加粗或希腊字母为向量，大写字母为矩阵。在函数中用\(y,Y\)做自变量，\(f\)表示函数关系">
<meta property="og:type" content="article">
<meta property="og:title" content="矩阵论">
<meta property="og:url" content="https://qdhdusdc.github.io/2023/04/27/%E7%9F%A9%E9%98%B5%E8%AE%BA/index.html">
<meta property="og:site_name" content="UTOPIA">
<meta property="og:description" content="矩阵导数 ​ 矩阵导数即对矩阵的多个元素求导，作为一种简便的计算方式。在下面的计算过程中，标量，向量均作为特殊的矩阵，如无指出是标量或向量，矩阵的含义均包括了是标量，向量，矩阵的情况。在矩阵导数中，因变量矩阵的每个元素与自变量矩阵中的每个元素均存在映射。 ​ 在可能混淆的地方，小写字母为标量，加粗或希腊字母为向量，大写字母为矩阵。在函数中用\(y,Y\)做自变量，\(f\)表示函数关系">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-04-27T07:30:10.000Z">
<meta property="article:modified_time" content="2023-09-10T13:04:58.765Z">
<meta property="article:author" content="M.Q.">
<meta property="article:tag" content="matrix">
<meta property="article:tag" content="math">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>矩阵论 - UTOPIA</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"qdhdusdc.github.io","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/0.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"AgcF1mQgwJbHELHjHp7JRJcB-gzGzoHsz","app_key":"0fENfeqhc4WLt6AcCXBkTH91","server_url":"https://agcf1mqg.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":true}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Home</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="矩阵论"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-04-27 15:30" pubdate>
          April 27, 2023 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          19k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          156 mins
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          Read <span id="leancloud-page-views"></span> times
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">矩阵论</h1>
            
            
              <div class="markdown-body">
                
                <span id="more"></span>
<h1 id="矩阵导数">矩阵导数</h1>
<p>​
矩阵导数即对矩阵的多个元素求导，作为一种简便的计算方式。在下面的计算过程中，标量，向量均作为特殊的矩阵，如无指出是标量或向量，矩阵的含义均包括了是标量，向量，矩阵的情况。在矩阵导数中，因变量矩阵的每个元素与自变量矩阵中的每个元素均存在映射。</p>
<p>​
在可能混淆的地方，小写字母为标量，加粗或希腊字母为向量，大写字母为矩阵。在函数中用<span class="math inline">\(y,Y\)</span>做自变量，<span class="math inline">\(f\)</span>表示函数关系，<span class="math inline">\(dy,dY\)</span>表示全微分，<span class="math inline">\(\frac{\partial f}{\partial x_i}\)</span>表示<span class="math inline">\(y\)</span>对<span class="math inline">\(x_i\)</span>的偏导数。将矩阵每个元素视为自变量，要求因变量（标量或矩阵）对该矩阵每个元素的导数构成的矩阵。在不引起混淆的情况下，使用<span class="math inline">\(f&#39;(X)\)</span>作为<span class="math inline">\(\frac{\partial f(X)}{\partial
X}\)</span>的简写形式。</p>
<p>​ 在形如<span class="math inline">\(Y=f(X_{m\times
n})\)</span>的矩阵运算中，自变量矩阵<span class="math inline">\(X\)</span>每个元素与因变量矩阵<span class="math inline">\(Y\)</span>存在映射，因此<span class="math inline">\(Y\)</span>为1×1矩阵(或标量)时，<span class="math inline">\(\frac{\partial f}{\partial X}\)</span>应有<span class="math inline">\(m\times n\)</span>个元素，若<span class="math inline">\(Y\)</span>为<span class="math inline">\(p\times
q\)</span>矩阵时，<span class="math inline">\(\frac{\partial f}{\partial
X}\)</span>应有<span class="math inline">\(p\times q\times m\times
n\)</span>个元素。然而，这可能有多种表示方式。本文采用向量化方法，详见下。</p>
<h2 id="计算技巧">计算技巧</h2>
<h3 id="hadamard积">Hadamard积</h3>
<p>对同形矩阵<span class="math inline">\(A,B\in R^{m\times
n}\)</span>，表现为按元素乘: <span class="math display">\[
[A\odot B]_{ij}=a_{ij}b_{ij}
\]</span> 在numpy中可以直接将两个同形矩阵相乘：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>A = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br>B = np.array([[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]])<br>Hadamard_A_B = A*B<br></code></pre></td></tr></table></figure>
<h3 id="kronecker积">Kronecker积</h3>
<p>对于<span class="math inline">\(A\in R^{m\times n},B\in R^{p\times
q}\)</span>，B对A中每个元素<span class="math inline">\(a_{ij}\)</span>进行数乘并将结果取代<span class="math inline">\(a_{ij}\)</span>位置: <span class="math display">\[
[A\otimes B]_{ij}=a_{ij}B
\]</span> numpy实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>A = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br>B = np.array([[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]])<br>Kronecker_A_B = np.kron(A, B)<br></code></pre></td></tr></table></figure>
<h3 id="向量化">向量化</h3>
<p><span class="math inline">\(vec(X_{m\times n})\)</span>表示将<span class="math inline">\(X\)</span>逐列转换为列向量： <span class="math display">\[
[vec(X)]_{i}=X_{[\frac{i}{m}],i-m[\frac{i}{m}]+1}
\]</span> 其中<span class="math inline">\([a]\)</span>表示对有理数<span class="math inline">\(a\)</span>向下取整。在numpy中使用reshape()即可。</p>
<h3 id="变量矩阵等价的唯一性">变量矩阵等价的唯一性</h3>
<p>​ 对于变量矩阵<span class="math inline">\(X_{mn},Y_{nq},Z_{nq}\)</span>，若存在<span class="math inline">\(X=X&#39;\)</span>使得<span class="math inline">\(X\)</span>列满秩，且有<span class="math inline">\(XY=XZ\)</span>，则<span class="math inline">\(Y=Z\)</span>。关于这个结论，暂未查找到资料。然而这是十分重要的。所以，在此给出一个证明。</p>
<p>​ For all column vectors <span class="math inline">\(X_{.1},X_{.2},...X_{.n}\)</span> in <span class="math inline">\(X\)</span>： <span class="math display">\[
k_1X_{.1}+k_2X_{.2}+...+k_nX_{.n}=0 \quad \Rightarrow
k_1,k_2,...k_n=0\tag{1}
\]</span> ​ Suppose <span class="math inline">\(Y\neq Z,XY=XZ\)</span>,
then <span class="math display">\[
X(Y-Z)=0
\]</span> ​ Let <span class="math inline">\(W=Y-Z\)</span>, <span class="math inline">\(W\neq 0\)</span>, for all column vectors <span class="math inline">\(W_{.1},W_{.2}...W_{.q}\)</span> in W: <span class="math display">\[
\exists {i} \quad s.t.W_{.i}\neq0
\]</span> Consider the multiplication of <span class="math inline">\(X\)</span> and <span class="math inline">\(W_{.i}\)</span>: <span class="math display">\[
XW_{.i}=(X_{.1},X_{.2},...X_{.n})W_{.i}=X_{.1}W_{1i}+X_{.2}W_{2i}+...X_{.n}W_{ni}=0
\]</span> ​
与(1)矛盾。对于可行满秩的变量矩阵，同理，改为左乘即可。在下面计算矩阵导数时将用到这个结论。</p>
<h2 id="多元函数导数">多元函数导数</h2>
<p>​ 对于多元函数<span class="math display">\[y=f(x_1,x_2,...,x_n)\]</span>，其微分<span class="math inline">\(dy\)</span>为： <span class="math display">\[
dy=\sum_{i=1}^{n}\frac{\partial f}{\partial x_i}dx_i
\]</span> 以向量形式表示，令 <span class="math display">\[
d\mathbf{x}=(dx_1,dx_2,...,dx_n)\\
\frac{\partial f}{\partial \mathbf{x}}=(\frac{\partial f}{\partial
x_1},\frac{\partial f}{\partial x_2},...,\frac{\partial f}{\partial
x_n})
\]</span> <span class="math inline">\(\frac{\partial f}{\partial
\mathbf{x} }\)</span>即在点<span class="math inline">\((x_1,x_2,...,x_n)\)</span>的梯度。则将<span class="math inline">\(dy\)</span>表示为 <span class="math display">\[
dy=\frac{\partial f}{\partial \mathbf{x}}^Td\mathbf{x}
\]</span></p>
<h2 id="标量对矩阵导数">标量对矩阵导数</h2>
<p>​ 对于<span class="math inline">\(y=f(X_{m\times n})\)</span>，<span class="math inline">\(f\)</span>将矩阵<span class="math inline">\(X\)</span>映射为标量，<span class="math inline">\(y\)</span>与<span class="math inline">\(X\)</span>的全体元素存在映射关系。仿照多元函数，微分<span class="math inline">\(dy\)</span>为： <span class="math display">\[
dy=\sum_{i=1}^{m}{\sum_{j=1}^{n}{\frac{\partial f}{\partial X_{ij} } }
}dX_{ij}
\]</span> 其中 <span class="math display">\[
dX=\left[
\begin{matrix}
dX_{11} &amp; \cdots &amp; dX_{1n}\\
\vdots &amp; \ddots &amp;  \\
dX_{m1} &amp;  &amp; dX_{mn}
\end{matrix}
\right]
\]</span> <span class="math inline">\(dX\)</span>是与<span class="math inline">\(X\)</span>同形的矩阵，<span class="math inline">\(dX_{ij}\)</span>表示<span class="math inline">\(X_{ij}\)</span>的微分。在<span class="math inline">\(X\)</span>中，每个元素都是一个自变量。令 <span class="math display">\[
\frac{\partial f}{\partial X}=\left[
\begin{matrix}
\frac{\partial{f} }{dX_{11} } &amp; \cdots &amp; \frac{\partial{f}
}{dX_{1n} }\\
\vdots &amp; \ddots &amp;  \\
\frac{\partial{f} }{dX_{m1} } &amp;  &amp; \frac{\partial{f} }{dX_{mn} }
\end{matrix}
\right]
\]</span> 于是 <span class="math display">\[
dy=tr(\frac{\partial f}{\partial X}^TdX)\tag{*}
\]</span> <span class="math inline">\(tr(X)\)</span> is the trace of
<span class="math inline">\(X\)</span>，which stands for the summary of
the diagonal elements of X. Notice that the <span class="math inline">\(X\)</span> here is a square.</p>
<h3 id="链式法则">链式法则</h3>
<ol type="1">
<li>若<span class="math inline">\(y=f(Y),Y=AXB+C\)</span>，则根据(*)式：</li>
</ol>
<p><span class="math display">\[
dy=tr(\frac{\partial f}{\partial Y}^TdY)=tr(\frac{\partial f}{\partial
X}^TdX)
\]</span></p>
<p>因为 <span class="math display">\[
dY=AdXB
\]</span> 则根据性质10： <span class="math display">\[
tr(\frac{\partial f}{\partial Y}^TdY)=tr(\frac{\partial f}{\partial
Y}^TAdXB)=tr(B\frac{\partial f}{\partial Y}^TAdX)=tr(\frac{\partial
f}{\partial X}^TdX)
\]</span> 于是 <span class="math display">\[
\frac{\partial f}{\partial X}^T=B\frac{\partial f}{\partial Y}^TA\\
\frac{\partial f}{\partial X}=A^T\frac{\partial f}{\partial Y}B^T
\]</span></p>
<ol start="2" type="1">
<li>若<span class="math inline">\(y=f(Y),Y=\sigma(X)\)</span>，其中<span class="math inline">\(\sigma(X)\)</span>为逐元素的函数，因为</li>
</ol>
<p><span class="math display">\[
dY=\frac{\partial \sigma}{\partial X_{} }\odot dX
\]</span></p>
<p>则根据性质11： <span class="math display">\[
tr(\frac{\partial f}{\partial Y}^TdY)=tr(\frac{\partial f}{\partial
Y}^T(\frac{\partial \sigma}{\partial X_{} }\odot dX))=tr((\frac{\partial
f}{\partial Y}^T\odot \frac{\partial \sigma}{\partial X
}^T)dX))=tr(\frac{\partial f}{\partial X}^TdX)
\]</span> 于是 <span class="math display">\[
\frac{\partial f}{\partial X}=\frac{\partial f}{\partial Y}\odot
\frac{\partial \sigma}{\partial X }
\]</span></p>
<h2 id="矩阵对矩阵导数未完">矩阵对矩阵导数（未完）</h2>
<p>​ 首先明确矩阵对矩阵导数的意义。对于<span class="math inline">\(Y_{p\times q}=f(X_{m\times n})\)</span>，函数<span class="math inline">\(f\)</span>将矩阵<span class="math inline">\(X\)</span>映射到另一个形状可能不同的矩阵<span class="math inline">\(Y\)</span>，对于<span class="math inline">\(Y\)</span>的任一元素，<span class="math inline">\(X\)</span>中的全体元素与之存在映射关系。因此，<span class="math inline">\(\frac{dY}{dX}\)</span>应当是一个具有<span class="math inline">\(p\times q \times m\times
n\)</span>个元素的矩阵。矩阵对矩阵求导时将因变量与自变量矩阵向量化，转化为向量对向量导数。向量对向量导数有分子布局与分母布局两种表示方式，这两种表示方式互为转置矩阵。</p>
<h3 id="分母布局">分母布局</h3>
<p>​ 本文向量对向量导数采用分母布局，对于<span class="math inline">\(\mathbf{y}=f(\mathbf{x})\)</span>，向量<span class="math inline">\(\mathbf{y}\)</span>对向量<span class="math inline">\(\mathbf{x}\)</span>的导数定义如下： <span class="math display">\[
\frac{\partial f}{\partial \mathbf{x} }=\left[
\begin{matrix}
\frac{\partial{y_1} }{dx_{1} } &amp;\frac{\partial{y_2} }{dx_{1} }&amp;
\cdots &amp; \frac{\partial{y_n} }{dx_{1} }\\
\frac{\partial{y_1} }{dx_{2} } &amp;\frac{\partial{y_2} }{dx_{2} }&amp;
\cdots &amp; \frac{\partial{y_n} }{dx_{2} }\\
\vdots &amp; &amp;\ddots &amp;  \\
\frac{\partial{y_1} }{dx_{m} } &amp;\frac{\partial{y_2} }{dx_{m} }&amp;
\cdots &amp; \frac{\partial{y_n} }{dx_{m} }\\
\end{matrix}
\right]
\]</span></p>
<h2 id="性质">性质</h2>
<ol type="1">
<li><span class="math inline">\(d(X \pm Y)=d(X)\pm d(Y)\)</span></li>
<li><span class="math inline">\(d(X_{m \times p}Y_{p\times
n})=(dX)Y+X(dY)\)</span></li>
</ol>
<p>​ 证明： <span class="math display">\[
d(XY)_{ij}=d(\sum_{i=1}^p{X_{ip}Y_{pj}
})=\sum_{i=1}^p(dX_{ip}Y_{pj})+\sum_{i=1}^p(X_{ip}dY_{pj})=(dX)_{i.}Y_{.j}+(dX)_{.i}Y_{j.}
\]</span></p>
<ol start="3" type="1">
<li><p><span class="math inline">\(d(X^T)=(dX)^T\)</span></p></li>
<li><p><span class="math inline">\((dX)^{-1}=-X^{-1}dXX^{-1}\)</span></p></li>
</ol>
<p>​ 证明： <span class="math display">\[
XX^{-1}=I\\
Xd(X^{-1})+(dX)X^{-1}=d(XX^{-1})=d(I)=0\\
(dX)^{-1}=-X^{-1}dXX^{-1}
\]</span></p>
<ol start="5" type="1">
<li><span class="math inline">\(d(\sigma(X))=\frac{\partial
\sigma}{\partial X}\odot dX\)</span></li>
</ol>
<p>证明： <span class="math display">\[
\begin{aligned}
d(\sigma(X))
=\left[
\begin{matrix}
\frac{\partial \sigma}{\partial X_{11}}dX_{11} &amp; \cdots &amp;
\frac{\partial \sigma}{\partial X_{1n}}dX_{1n}\\
\vdots &amp; \ddots &amp;  \\
\frac{\partial \sigma}{\partial X_{m1}}dX_{m1} &amp;  &amp;
\frac{\partial \sigma}{\partial X_{mn} }dX_{mn}
\end{matrix}
\right]= \left[
\begin{matrix}
\frac{\partial \sigma}{\partial X_{11} } &amp; \cdots &amp;
\frac{\partial \sigma}{\partial X_{1n} }\\
\vdots &amp; \ddots &amp;  \\
\frac{\partial \sigma}{\partial X_{m1} } &amp;  &amp; \frac{\partial
\sigma}{\partial X_{mn} }
\end{matrix}
\right]\odot\left[
\begin{matrix}
dX_{11} &amp; \cdots &amp;dX_{1n}\\
\vdots &amp; \ddots &amp;  \\
dX_{m1} &amp;  &amp; dX_{mn}
\end{matrix}
\right]
=\frac{\partial \sigma}{\partial X_{} }\odot dX
\end{aligned}
\]</span></p>
<ol start="6" type="1">
<li><p><span class="math inline">\(d(X\odot Y)=dX\odot Y+X\odot
dY\)</span></p></li>
<li><p><span class="math inline">\(x\)</span>为标量，或1×1矩阵，<span class="math inline">\(x=tr(x)\)</span></p></li>
<li><p><span class="math inline">\(tr(X^T)=tr(X)\)</span></p></li>
<li><p><span class="math inline">\(dtr(X)=tr(dX)\)</span></p></li>
<li><p><span class="math inline">\(tr(XY)=tr(YX)=tr(X^TY^T)\)</span></p></li>
<li><p><span class="math inline">\(tr(A^T(B\odot C))=tr((A\odot
B)^TC)=tr((A^T\odot B^T)C)\)</span> 可以看到，只有<span class="math inline">\(B\)</span>变成了<span class="math inline">\(B^T\)</span></p></li>
<li><p><span class="math inline">\(u,v\)</span>是维数相同的向量，<span class="math inline">\(\mathbf 1^T(u\odot v)=u^Tv\)</span></p></li>
<li><p><span class="math inline">\(u,v\)</span>是维数相同的向量，<span class="math inline">\(u^Tv=v^Tu\)</span></p></li>
<li><p><span class="math inline">\(vec(A+B)=vec(A)+vec(B)\)</span></p></li>
<li><p><span class="math inline">\(vec(AXB)=(B^T\otimes
A)vec(X)\)</span></p></li>
</ol>
<h2 id="示例">示例</h2>
<p>简便起见，将<span class="math inline">\(\frac{\partial f(a)}{\partial
a}\)</span>记为<span class="math inline">\(f&#39;(a)\)</span></p>
<h3 id="简单单层网络">简单单层网络</h3>
<p>已知 <span class="math display">\[
\hat{y}=XW,\quad X\in R^{1\times m},W\in R^{m\times n}
\]</span> 损失函数 <span class="math display">\[
L=\frac{1}{2}(\hat{y}-y)({\hat{y}-y)^T }
\]</span> <span class="math inline">\(W\)</span>更新公式 <span class="math display">\[
W-=lr*\frac{\partial L}{\partial W}
\]</span> 为了求<span class="math inline">\(\frac{\partial L}{\partial
W}\)</span>，根据(*)式先求<span class="math inline">\(dL\)</span> <span class="math display">\[
\begin{aligned}
dL&amp;=\frac{1}{2}d((\hat{y}-y))({\hat{y}-y)^T
}+\frac{1}{2}(\hat{y}-y)d(({\hat{y}-y)^T) }\\
&amp;=\frac{1}{2}(XdW)({\hat{y}-y)^T }+\frac{1}{2}(\hat{y}-y)(XdW)^T \\
\end{aligned}
\]</span> 注意到<span class="math inline">\((\hat{y}-y)^T\)</span>与<span class="math inline">\(dWX\)</span>均为向量，则根据性质13： <span class="math display">\[
dL=(XdW)(\hat{y}-y)^T\tag{1}
\]</span> 由于 <span class="math display">\[
dL=tr(\frac{\partial f}{\partial W}^TdW)
\]</span> 则要将(1)式化为<span class="math inline">\(tr(M^TdW)\)</span>形式，以得出<span class="math inline">\(\frac{\partial L}{\partial
W}\)</span>。注意到<span class="math inline">\(dL\)</span>为<span class="math inline">\(1\times1\)</span>方阵，于是 <span class="math display">\[
dL=tr(dL)=tr((XdW)(\hat{y}-y)^T)=tr((\hat{y}-y)^TXdW)=tr((X^T(\hat{y}-y))^TdW)
\]</span> 因此有 <span class="math display">\[
\frac{\partial L}{\partial W}=X^T(\hat{y}-y)
\]</span></p>
<h3 id="单隐藏层网络">单隐藏层网络</h3>
<p>已知 <span class="math display">\[
\hat{y}=g(hW_2+b_2),h=g(XW_1+b_1)\\
\quad X\in R^{1\times m},W_1\in R^{m\times n},b_1\in R^{1\times
n},W_2\in R^{n\times q},b_2\in R^{1\times q}
\]</span> 损失函数 <span class="math display">\[
L=\frac{1}{2}(\hat{y}-y)({\hat{y}-y)^T }
\]</span> 参数更新公式 <span class="math display">\[
W_1=lr*\frac{\partial L}{\partial W_1},W_2-=lr*\frac{\partial
L}{\partial W_2},b_1-=lr*\frac{\partial L}{\partial
b_1},b_2-=lr*\frac{\partial L}{\partial b_2}
\]</span> 在求某个变量的梯度时，暂且将其他变量视为常量。先求<span class="math inline">\(\frac{\partial L}{\partial b_2}\)</span>，由性质11
<span class="math display">\[
\begin{aligned}
dL&amp;=tr(dL)=tr((g&#39;(hW_2+b_2)\odot db_2)(\hat{y}-y)^T)\\
&amp;=tr((\hat{y}-y)(g&#39;(hW_2+b_2)^T\odot db_2^T))\\
&amp;=tr(((\hat{y}-y)\odot g&#39;(hW_2+b_2))db_2^T)\\
&amp;=tr(((\hat{y}-y)\odot g&#39;(hW_2+b_2))^Tdb_2)
\end{aligned}
\]</span> 于是 <span class="math display">\[
\frac{\partial L}{\partial b_2}=(\hat{y}-y)\odot g&#39;(hW_2+b_2)
\]</span> 对于<span class="math inline">\(\frac{\partial L}{\partial
W_2}\)</span>： <span class="math display">\[
\begin{aligned}
dL&amp;=tr(dL)=tr((g&#39;(hW_2+b_2)\odot (hdW_2))(\hat{y}-y)^T)\\
&amp;=tr((\hat{y}-y)(g&#39;(hW_2+b_2)^T\odot (hdW_2)^T))\\
&amp;=tr(((\hat{y}-y)\odot g&#39;(hW_2+b_2))(hdW_2)^T)\\
&amp;=tr(((\hat{y}-y)\odot g&#39;(hW_2+b_2))^ThdW_2)\\
&amp;=tr(((h^T((\hat{y}-y)\odot g&#39;(hW_2+b_2)))^TdW_2)
\end{aligned}
\]</span> 于是 <span class="math display">\[
\frac{\partial L}{\partial W_2}=h^T((\hat{y}-y)\odot g&#39;(hW_2+b_2))
\]</span> 为求<span class="math inline">\(\frac{\partial L}{\partial
b_1}\)</span>，<span class="math inline">\(\frac{\partial L}{\partial
W_1}\)</span>，根据链式法则，先求出<span class="math inline">\(\frac{\partial L}{\partial h}\)</span> <span class="math display">\[
\begin{aligned}
dL&amp;=tr(dL)=tr((g&#39;(hW_2+b_2)\odot (dhW_2))(\hat{y}-y)^T)\\
&amp;=tr((\hat{y}-y)(g&#39;(hW_2+b_2)^T\odot (dhW_2)^T))\\
&amp;=tr(((\hat{y}-y)\odot g&#39;(hW_2+b_2))(dhW_2)^T)\\
&amp;=tr(((\hat{y}-y)\odot g&#39;(hW_2+b_2))^TdhW_2)\\
&amp;=tr((((\hat{y}-y)\odot g&#39;(hW_2+b_2))W_2^T)^Tdh)
\end{aligned}
\]</span> 于是 <span class="math display">\[
\frac{\partial L}{\partial h}=((\hat{y}-y)\odot g&#39;(hW_2+b_2))W_2^T
\]</span> 设<span class="math inline">\(a=XW_1+b_1\)</span>，则<span class="math inline">\(h=g(a)\)</span>，根据链式法则 <span class="math display">\[
\frac{\partial L}{\partial a}=\frac{\partial L}{\partial h}\odot
\frac{\partial g}{\partial a}\\
\frac{\partial L}{\partial b_1}=\frac{\partial L}{\partial a}\\
\frac{\partial L}{\partial W_1}=X^T\frac{\partial L}{\partial a}
\]</span> 于是得到 <span class="math display">\[
\frac{\partial L}{\partial b_1}=(((\hat{y}-y)\odot
g&#39;(hW_2+b_2))W_2^T)\odot \frac{\partial g}{\partial a}\\
\frac{\partial L}{\partial W_1}=X^T((((\hat{y}-y)\odot
g&#39;(hW_2+b_2))W_2^T)\odot \frac{\partial g}{\partial a})
\]</span> ### 经典RNN（未完）</p>
<p>已知 <span class="math display">\[
y_t=f(h_tW_{hy}+b_y)\\
h_t=g(x_tW_{xh}+h_{t-1}W_{hh}+b_h)
\]</span> 其中<span class="math inline">\(x_t\in R^{1\times
m}\)</span>,<span class="math inline">\(W_{xh}\in R^{m\times
n}\)</span>,<span class="math inline">\(h_t\in R^{1\times
n}\)</span>,<span class="math inline">\(W_{hh}\in R^{n\times
n}\)</span>,<span class="math inline">\(W_{hy}\in R^{n\times
q}\)</span></p>
<p>注意，<span class="math inline">\(h_t\)</span>为常量。 <span class="math display">\[
\begin{aligned}
dL&amp;=tr(dL)=tr(dy_t(y-y_t)^T)\\
&amp;=tr(f&#39;(h_tW_{hy}+b_y))
\end{aligned}
\]</span></p>
<h1 id="特征值与特征向量">特征值与特征向量</h1>
<p>"There are some matrices for which there are not a full set of
eigenvectors. that's really the main sort of annoying point in the whole
subject of linear algebra is some matrices don't have enough
eigenvectors. "</p>
<p>对于任何一个n阶方阵<span class="math inline">\(A_{n\times
n}\)</span>，它的特征值一定有n个（算上虚特征值和重复特征值），换句话说这等价于实数特征值不超过n个，因为求特征值用<span class="math inline">\(|A-\lambda I|=0\)</span>,最终必定可以化为<span class="math inline">\(\lambda\)</span>的n阶方程，实数解的个数不超过n（代数基本定理）。</p>
<ul>
<li>定理1：不同特征值对应的特征向量线性无关。证明：</li>
</ul>
<p>设<span class="math inline">\(A\)</span>有两个特征值<span class="math inline">\(\lambda_{1},\lambda_{2}\)</span>
和两个对应的特征向量<span class="math inline">\(x_1,x_2\)</span>，且<span class="math inline">\(\lambda_{1}\neq\lambda_{2}\)</span>.假设<span class="math inline">\(x_1,x_2\)</span>线性相关，则<span class="math inline">\(x_1=kx_2\)</span>成立，则<span class="math inline">\(Ax_2=\lambda_2 x_2=A
kx_1=kAx_1=k\lambda_1x_1\)</span>,所以(1)当<span class="math inline">\(\lambda_1\ne0\)</span>，有<span class="math inline">\(x_2=\frac{\lambda_1}{\lambda_2}kx_1=kx_1\)</span>,<span class="math inline">\(\lambda_1=\lambda_2\)</span>，矛盾。(2)当<span class="math inline">\(\lambda_1=0\)</span>,因为特征向量非零，所以<span class="math inline">\(\lambda_2=0=\lambda_1\)</span>，矛盾。于是得证</p>
<h2 id="实数特征值与虚数特征值">实数特征值与虚数特征值</h2>
<p>从矩阵<span class="math inline">\(A\)</span>的特征值<span class="math inline">\(\lambda\)</span>与特征向量<span class="math inline">\(x\)</span>的计算来看，即<span class="math inline">\(Ax=\lambda x\)</span>，在<span class="math inline">\(x\)</span>方向上，矩阵<span class="math inline">\(A\)</span>只起到了拉伸的作用。若特征值为0，则<span class="math inline">\(A\)</span>将某些向量映射到0空间，这说明<span class="math inline">\(A\)</span>不满秩。若特征值为实数，说明对某些方向上的向量，<span class="math inline">\(A\)</span>仅起到了拉伸的效果，下面将这种矩阵称作拉伸矩阵。若特征值是虚数，说明<span class="math inline">\(A\)</span>对所有方向的向量都起到了旋转的效果，下面将这种矩阵称为旋转矩阵。</p>
<h3 id="拉伸矩阵">拉伸矩阵</h3>
<p>对于一个特征向量空间内的向量，拉伸矩阵让一个向量更靠近它最大的特征值对应的特征向量方向。也就是说，在拉伸矩阵的特征向量空间中的向量，一直让它左乘这个拉伸矩阵，那么它最终会逼近最大特征值对应的特征向量方向。为什么要强调是特征向量空间内？因为前面的定理1说明，r个不同特征值可以形成r维特征向量空间，而相同的k重特征值却不一定能形成k维特征向量空间（因为几何重数不大于代数重数，Jordan标准型给出证明）,因此，特征向量空间的维度不一定等于原矩阵的维度（即使原矩阵是满秩的）。因此要强调是特征向量空间内的向量，因为它可以由特征向量表出。</p>
<p>证明：设矩阵<span class="math inline">\(A_{n\times n}\)</span>有<span class="math inline">\(r\)</span>个不同的特征向量<span class="math inline">\(\lambda_1,...\lambda_r\)</span>，有一个特征向量空间内的向量<span class="math inline">\(x\)</span>,特征向量的基底是<span class="math inline">\(e_1,e_2,...e_r\)</span>,于是 <span class="math display">\[
\exists k_1,k_2,...k_n,s.t.\quad x=\sum_{i=1}^rk_ie_i
\]</span> 则 <span class="math display">\[
\lim_{y\to
\infty}A^yx=A^y\sum_{i=1}^rk_ie_i=\sum_{i=1}^rk_iA^ye_i=\sum_{i=1}^rk_i\lambda_i^ye_i
\]</span> 容易得知最大的特征值的特征向量方向对结果方向影响最大</p>
<h3 id="旋转矩阵的分解">旋转矩阵的分解</h3>
<p>旋转矩阵对所有实数向量都起到了旋转的效果，但也可能起到拉伸效果。事实上，旋转矩阵可以分解成一个纯旋转矩阵和一个拉伸矩阵的矩阵乘积。（<strong>该结论暂未证明</strong>）纯旋转矩阵指的是，左乘纯旋转矩阵只改变向量的方向，而不改变向量的模长。纯旋转矩阵一定是正交矩阵。满足下列关系的矩阵为纯旋转矩阵：
<span class="math display">\[
\forall \textbf x,k\in[-1,1],cos&lt;\textbf x,A\textbf
x&gt;=\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}x_ix_j}{\sum_{i=1}^{n}a_i^2}\equiv
k
\]</span>
即旋转矩阵对任意向量的旋转角均相等。这个结论直接计算化简即得。</p>
<h2 id="相似对角化">相似对角化</h2>
<p>矩阵<span class="math inline">\(A\)</span>可以相似于对角矩阵<span class="math inline">\(\Lambda\)</span>，当且仅当矩阵<span class="math inline">\(A_{n\times
n}\)</span>有n个线性无关的特征向量。即： <span class="math display">\[
\begin{aligned}
\Lambda&amp;=P^{-1}AP\\
P&amp;=(p_1,p_2,..p_n)\\
\Lambda&amp;=\left[
\begin{matrix}
\lambda_1 &amp;  &amp; \\
&amp; \ddots &amp;  \\
&amp;  &amp; \lambda_{n}
\end{matrix}
\right]
\end{aligned}
\]</span> 其中，<span class="math inline">\(\lambda_i\)</span>对应的特征向量为<span class="math inline">\(p_i\)</span>. 直接计算<span class="math inline">\(AP=P\Lambda\)</span>即可证明。若<span class="math inline">\(A\)</span>相似于对角矩阵，则很容易算<span class="math inline">\(A\)</span>的次方： <span class="math display">\[
A^k=P\Lambda^kP^{-1}
\]</span></p>
<h1 id="jordan标准型">Jordan标准型</h1>
<p>任何n阶方阵可以相似于具有以下形式的矩阵： <span class="math display">\[
J_{n\times n}=\left[\begin{matrix}
J_1 &amp;  &amp; &amp;\\
&amp; J_2&amp; \\
  &amp;  &amp; \ddots&amp; \\
&amp;  &amp;&amp; J_{k}
\end{matrix}
\right],J_i=\left[\begin{matrix}
\lambda_i &amp;1 &amp; &amp; \\
&amp; \lambda_i &amp;1 &amp;  \\
  &amp;  &amp;\ddots &amp; \ddots \\
&amp; &amp; &amp; \lambda_{i}
\end{matrix}
\right]
\]</span> 其中<span class="math inline">\(\sum_{i=1}^nR(J_i)=n\)</span>.
在Gilbert Strang 的 <em>Linear Algebra and Its Applications</em> 中
Appendix B 一章通过构造<span class="math inline">\(P\)</span>证明了<span class="math inline">\(\forall A\in R^{n\times n},\exists P,
s.t.A=P^{-1}JP\)</span>.</p>
<p>下面分析Jordan型的意义。</p>
<p>注意小块最后一行，这一行只有一列，因此，<span class="math inline">\(|J_i-\lambda_i I|=0\)</span>，<span class="math inline">\(\lambda_i\)</span>是<span class="math inline">\(J\)</span>的特征值。</p>
<p>接下来考虑<span class="math inline">\(J\)</span>的特征向量。对于小块<span class="math inline">\(J_i\)</span>，<span class="math inline">\((J_i-\lambda_iI)x=0\)</span>的解空间维数为1，因此小块<span class="math inline">\(J_i\)</span>对应一个1维特征向量空间。于是容易理解，代数重数大于等于几何重数的原因。每个小块至少是1维，当小块为1维时，这个小块的特征值对应的特征向量空间是1维，此时代数重数等于几何重数。如果小块大于1维，代数重数大于几何重数，这个小块的代数重数等于小块的维数，特征向量空间是1维。相同的特征值可能对应不同的特征向量，此时不同小块中存在相同的特征值，而这些不同的小块对应不同的特征向量。</p>
<h1 id="变换">变换</h1>
<p>矩阵乘向量在几何意义上就是向量的线性变换，满秩矩阵把这个向量映射到同维空间的另一个向量，不满秩矩阵把它映射到低维空间的另一个向量。矩阵乘矩阵在几何意义上是基底变换，一个矩阵的每一列视为一个基底向量，基底变换就是把一组基底映射到另一组基底，满秩矩阵把基底映射到同维空间的另一组基底，不满秩矩阵把基底映射到低维空间的另一组基底。</p>
<h2 id="基底变换">基底变换</h2>
<p>一个可逆矩阵可以将一组基底变换到在同维空间中的任意一组基底。这其实就是矩阵乘的意义。设有一组基底<span class="math inline">\((e_1,e_2,...e_n)\)</span>，他们构成矩阵<span class="math inline">\(M\)</span>，其中<span class="math inline">\(e_i\)</span>是<span class="math inline">\(n\times
1\)</span>列向量，对于n维空间中任意一组基底构成的矩阵<span class="math inline">\(N\)</span>，设经过左乘矩阵<span class="math inline">\(P\)</span>从<span class="math inline">\(M\)</span>变换到<span class="math inline">\(N\)</span>，则 <span class="math display">\[
\begin{aligned}
N&amp;=MP\\
P&amp;=M^{-1}N
\end{aligned}
\]</span> 于是可知存在唯一的矩阵<span class="math inline">\(P\)</span>使得基底<span class="math inline">\(M\)</span>变换到基底<span class="math inline">\(N\)</span></p>
<p>基底变换定理：基底变换不改变向量本身，通过基底变换变的是向量在这组基底下的各个分量的取值。也就是说，在标准正交基下（或者任何一组作为参考的基底）的向量是不变的。基底<span class="math inline">\(M\)</span>经过基底变换得到基底<span class="math inline">\(N\)</span>，即有<span class="math inline">\(N=MP\)</span>，对同一个标准正交基下的向量<span class="math inline">\(\textbf x\)</span>，其在<span class="math inline">\(M,N\)</span>下的分量为<span class="math inline">\(\alpha,\alpha&#39;\)</span>，即有<span class="math inline">\(M\alpha=N\alpha&#39;=\textbf
x,\alpha&#39;=P^{-1}\alpha\)</span></p>
<p>从这个结论来看，向量的线性变换的意义可以归结为以下两个等价的观点：</p>
<p>1）在基底不变的情况下，<span class="math inline">\(\textbf
x\)</span>变换到同基底下的<span class="math inline">\(P\textbf
x\)</span></p>
<p>2）在向量不变的情况下，基底<span class="math inline">\(M\)</span>变换到<span class="math inline">\(PM\)</span>，<span class="math inline">\(\textbf
x\)</span>在新基底<span class="math inline">\(PM\)</span>下的分量为<span class="math inline">\(P^{-1}\textbf x\)</span></p>
<p>这两种观点是等价的，区别只在于选择向量还是基底作为参考，而用两种观点来描述的他们之间的相对关系是一样的。基底不变，向量变等价于向量不变，基底变。</p>
<h2 id="相似变换">相似变换</h2>
<p>基底<span class="math inline">\(M\)</span>经过基底变换得到基底<span class="math inline">\(N\)</span>，即有<span class="math inline">\(N=MP\)</span>，对同一个标准正交基下的向量<span class="math inline">\(\textbf x\)</span>，其在<span class="math inline">\(M,N\)</span>下为<span class="math inline">\(\alpha,\alpha&#39;\)</span>，即有<span class="math inline">\(M\alpha=N\alpha&#39;=\textbf
x,\alpha&#39;=P^{-1}\alpha\)</span>，<span class="math inline">\(\alpha\)</span>经过线性变换得到向量<span class="math inline">\(\beta\)</span>，即<span class="math inline">\(\beta=A\alpha\)</span>，设<span class="math inline">\(\beta\)</span>在<span class="math inline">\(N\)</span>下为<span class="math inline">\(\beta&#39;\)</span>，于是有<span class="math inline">\(\beta&#39;=P^{-1}\beta\)</span>，设矩阵<span class="math inline">\(A&#39;\)</span>使得<span class="math inline">\(\beta&#39;=A&#39;\alpha&#39;\)</span>，于是 <span class="math display">\[
\beta&#39;=A&#39;\alpha&#39;=P^{-1}\beta=P^{-1}A\alpha=P^{-1}AP\alpha&#39;
\]</span> 则 <span class="math display">\[
A&#39;=P^{-1}AP\tag{2}
\]</span>
上面的结论说明，一个向量被映射到另一个向量，在两个不同的基底下的两个变换关系之间具有(2)式描述的关系。因此，相似变换是同一个线性变换在不同基底下的不同表现形式。</p>
<h3 id="相似对角化-1">相似对角化</h3>
<p>若方阵经相似变换能化为对角矩阵，则这个方阵可相似对角化。</p>
<p><span class="math inline">\(A_{n\times n}\)</span>可相似对角化<span class="math inline">\(\iff\)</span><span class="math inline">\(A_{n\times n}\)</span>有n个线性无关的特征向量</p>
<p>证明：充分性：若<span class="math inline">\(A_{n\times
n}\)</span>可相似对角化，设其相似于对角矩阵<span class="math inline">\(B=diag(b_1,b_2,...b_n)\)</span>，且<span class="math inline">\(B=P^{-1}AP\)</span>，则<span class="math inline">\(PB=AP\)</span>. 将<span class="math inline">\(P\)</span>以列向量形式表示，<span class="math inline">\(P={p_{.1},p_{.2},...p_{.n})\)</span> . 因为 <span class="math display">\[
PB=(p_{.1},p_{.2},...p_{.n})
\]</span> 从几何角度考虑，相似对角化实质上是把原矩阵<span class="math inline">\(A\)</span>对应的基底变换到特征向量方向上，这时新的基底两两正交。</p>
<h2 id="合同变换">合同变换</h2>
<p>任意一个二次型 <span class="math display">\[
f(\textbf x)=\sum_{i=1}^n\sum_{j=1}^{n} k_{ij}x_ix_j,\textbf
x=(x_1,x_2,...x_n)^T
\]</span> 均可以化为矩阵乘形式<span class="math inline">\(\textbf
x^TA\textbf x\)</span>，可以证明这样的<span class="math inline">\(A\)</span>有无数个。当<span class="math inline">\(A\)</span>为对称矩阵，则<span class="math inline">\(A\)</span>是唯一的，此时有 <span class="math display">\[
a_{ij}=\left\{
\begin{array}{l}
\frac{k_{ij}+k_{ji} }{2},\quad i\neq j \\
k_{ij},\quad i=j \\
\end{array}
\right.
\]</span> 当对基底<span class="math inline">\(M\)</span>下的向量<span class="math inline">\(\textbf x\)</span>进行线性变换<span class="math inline">\(C^{-1}\)</span>(为了让结论形式看起来简洁，写成C的逆矩阵。其实只要这里是个可逆矩阵就行)，得到另一个基底<span class="math inline">\(N\)</span>下的向量<span class="math inline">\(\textbf y\)</span>，即<span class="math inline">\(\textbf x=C\textbf y\)</span>，这时<span class="math inline">\(f(\textbf x)=g(\textbf y)=\textbf x^TA\textbf
x=\textbf y^TC^TAC\textbf y=\textbf y^TA&#39;\textbf y\)</span>，于是
<span class="math display">\[
A&#39;=C^TAC
\]</span>
上面的结论说明，同一个二次型在两个不同的基底下，两个变换关系之间具有(2)式描述的关系。</p>
<h3 id="惯性定理">惯性定理</h3>
<p>上面说到，合同变换可以视为同一个二次型在两个不同的基底下的不同的描述形式，因此，我们可以通过变换基底，让二次型只含有平方项，不含有<span class="math inline">\(x_1x_2\)</span>这种混合乘积，并且平方项系数只能为0，1，-1，也就是化成标准型。可以想象，一定存在某一个位置可以达到这种效果，例如一个椭圆，两个基底向量分别它的长轴和短轴重合时，就只含有平方项，将长轴一半和短轴一半长度作为两个基底向量的单位长度，就得到标准型，变成了一个单位圆。事实上，将任意二次型化成标准型存在且只存在一种合同变换。下面证明这个结论。</p>
<p>证明：设有二次型<span class="math inline">\(f(\textbf x)=g(\textbf
y)=\textbf x^TA\textbf x=\textbf y^T B\textbf y\)</span>，其中<span class="math inline">\(B\)</span>为对角矩阵。若证明了<span class="math inline">\(B\)</span>是对角矩阵，则可将<span class="math inline">\(B\)</span>化为只含0，1，-1的对角矩阵。于是原命题等价于证存在<span class="math inline">\(C\)</span>满足<span class="math inline">\(B=C^TAC\)</span>.</p>
<p>因为<span class="math inline">\(A\)</span>为对称矩阵，根据对称矩阵的结论，存在正交矩阵<span class="math inline">\(P\)</span>使得<span class="math inline">\(A=P^{-1}B P\)</span>，根据正交矩阵的结论，<span class="math inline">\(P^{-1}=P^T\)</span>，于是<span class="math inline">\(B=PAP^T\)</span>，即证</p>
<p>标准型中正、负号的个数称为正、负惯性系数。因为合同变换是同一个二次型在不同基底下的形式，而将任意二次型化成标准型存在且只存在一种合同变换，因此易知合同变换不改变正负惯性系数。</p>
<h2 id="正交化">正交化</h2>
<h3 id="向量投影">向量投影</h3>
<p>向量<span class="math inline">\(\alpha\)</span>在一组标准正交基<span class="math inline">\(V=(\beta_1,\beta_2,...\beta_m)\)</span>下的投影为
<span class="math display">\[
proj_V\alpha=\sum_{i=1}^m(\alpha\cdot\beta_i)\beta_i
\]</span> 其中，<span class="math inline">\(\alpha,\beta_i\)</span>均为n维向量（<span class="math inline">\(n\ge m\)</span>），<span class="math inline">\(\alpha\cdot\beta_i\)</span>是<span class="math inline">\(\alpha\)</span>在<span class="math inline">\(\beta_i\)</span>方向上投影的向量长度。当<span class="math inline">\(n=m\)</span>时，<span class="math inline">\(\alpha\)</span>可由<span class="math inline">\(V\)</span>线性表出，此时<span class="math inline">\(\alpha=proj_V\alpha\)</span>，当<span class="math inline">\(n&gt; m\)</span>，<span class="math inline">\(\alpha_p=\alpha-proj_V\alpha\)</span>与空间<span class="math inline">\(V\)</span>垂直。这里与空间垂直的意义是，<span class="math inline">\(\alpha_p\)</span>与每个基向量垂直。向量与线和平面垂直的情况容易想象。从想象中可以得到以下定理（<strong>该结论暂未证明</strong>）：所有<span class="math inline">\(\alpha\)</span>对应的<span class="math inline">\(\alpha_p\)</span>形成的向量空间的维度=m-n.</p>
<h3 id="正交化-1">正交化</h3>
<p>任意线性无关的一组向量均可以化为该向量空间下的一组标准正交基。</p>
<p>根据投影定义可以知道，任何一个与向量空间<span class="math inline">\(V\)</span>中所有基向量线性无关的向量<span class="math inline">\(\alpha\)</span>均能表示成这个空间内的投影<span class="math inline">\(proj_V\alpha\)</span>+与这个空间垂直的向量<span class="math inline">\(\alpha_p\)</span>.因此，可以采用如下方法得到一组基<span class="math inline">\((\beta_1,\beta_2,...\beta_m)\)</span>的标准正交基：</p>
<p>1）取第一个向量<span class="math inline">\(\beta_1\)</span>，将其归一化，加入到子空间<span class="math inline">\(W\)</span>中</p>
<p>2）取第k(k&gt;1)个向量<span class="math inline">\(\beta_k\)</span>，计算其与子空间<span class="math inline">\(W\)</span>垂直的向量<span class="math inline">\(\beta_{kp}=\beta-proj_V\beta_{k}\)</span>，并将<span class="math inline">\(\beta_{kp}\)</span>归一化，加入到<span class="math inline">\(W\)</span>中</p>
<p>3）重复2），直到k=n</p>
<h3 id="正交向量空间">正交向量空间</h3>
<p>若k维空间<span class="math inline">\(V\)</span>中的一组基<span class="math inline">\((\alpha_1,...\alpha_k)\)</span>与另一r维空间<span class="math inline">\(W\)</span>中的一组基<span class="math inline">\((\beta_1,...\beta_r)\)</span>正交(<span class="math inline">\(\alpha_i,\beta_j\)</span>为同维向量)，则空间<span class="math inline">\(V\)</span>与空间<span class="math inline">\(W\)</span>中所有向量均正交。</p>
<p>证明：设<span class="math inline">\(V\)</span>中一向量<span class="math inline">\(\alpha=\sum_{i=1}^{k}m_i\alpha_i\)</span>,<span class="math inline">\(W\)</span>中一向量<span class="math inline">\(\beta=\sum_{i=1}^{r}n_i\beta_i\)</span>, 则 <span class="math display">\[
\alpha \cdot
\beta=(\sum_{i=1}^{k}m_i\alpha_i)(\sum_{i=1}^{r}n_i\beta_i)=\sum_{i=1}^{k}\sum_{j=1}^r(m_in_j\alpha_i\cdot\beta_j)=0
\]</span> 于是两空间中任意向量均正交。</p>
<p>#特殊矩阵</p>
<h2 id="正交矩阵">正交矩阵</h2>
<ul>
<li>正交矩阵满足<span class="math inline">\(A^{-1}=A^T\)</span></li>
</ul>
<p>将矩阵写成列向量形式<span class="math inline">\(A=(a_1,a_2,...a_n)\)</span>，若 <span class="math display">\[
a_i\cdot a_{j}=\left\{
\begin{array}{l}
0,\quad i\neq j \\
1,\quad i=j \\
\end{array}
\right.
\]</span> 则<span class="math inline">\(A\)</span>为正交矩阵。</p>
<p>若<span class="math inline">\(A\)</span>为正交矩阵，则<span class="math inline">\(A^TA=I\)</span>，因为 <span class="math display">\[
A^TA=\left(
\begin{matrix}
a_1^T \\
a_2^T    \\
...\\
a_n^T
\end{matrix}
\right)(a_1,a_2,...a_n)=\left(
\begin{matrix}
a_1^Ta_1 &amp;a_1^Ta_2 &amp;...  \\
a_2^Ta_1 &amp; \ddots &amp;  \\
...&amp;  &amp; a_n^Ta_n
\end{matrix}
\right)=I
\]</span> 于是可得<span class="math inline">\(A^{-1}=A^T\)</span></p>
<h2 id="对称矩阵">对称矩阵</h2>
<ul>
<li>任何一个矩阵 A可以表示为一个反对称矩阵M + 一个对称矩阵S</li>
</ul>
<p>证明：如果原命题成立，只要证明存在这样的<span class="math inline">\(M\)</span>和<span class="math inline">\(N\)</span>，因为 <span class="math display">\[
\begin{aligned}
A_{ij}&amp;=M_{ij}+S_{ij}\\
A_{ji}&amp;=M_{ji}+S_{ji}=-M_{ji}+S_{ij}
\end{aligned}
\]</span> 两个未知数<span class="math inline">\(A_{ij},A_{ji}\)</span>两个等式可以解出两个未知数<span class="math inline">\(M_{ij},S_{ij}\)</span>，即证</p>
<ul>
<li>矩阵<span class="math inline">\(A\)</span>是实对称矩阵<span class="math inline">\(\iff\)</span><span class="math inline">\(A=P^{-1}\Lambda P\)</span>，<span class="math inline">\(\Lambda\)</span>是对角矩阵，<span class="math inline">\(P\)</span>是正交矩阵</li>
</ul>
<p>证明：1）充分性：若<span class="math inline">\(A\)</span>为实对称矩阵，设<span class="math inline">\(J=P^{-1}AP\)</span>, <span class="math inline">\(J\)</span>为Jordan矩阵。假设<span class="math inline">\(A\)</span>不可对角化，则<span class="math inline">\(J\)</span>存在维数大于1的小块。不妨设<span class="math inline">\(J_1\)</span>维数大于1，即 <span class="math display">\[
J_1=\left[\begin{matrix}
\lambda_1 &amp;1 &amp; &amp; \\
&amp; \lambda_1 &amp;\ddots &amp;  \\
  &amp;  &amp;\ddots &amp;  \\
\end{matrix}
\right]
\]</span> 令<span class="math inline">\(B=P^TP\)</span>，则<span class="math inline">\(B\)</span>为对称矩阵，<span class="math inline">\(b_{12}=b_{21}\)</span>，且<span class="math inline">\(b_{11}=\sum_{i=1}^{n}p_{i1}^2\)</span>，因为<span class="math inline">\(P\)</span>可逆，所以<span class="math inline">\(p_{i1}\)</span>不全为0，<span class="math inline">\(b_{11}&gt;0\)</span>. 因为<span class="math inline">\(P^TAP=P^TPJ=BJ\)</span>, <span class="math inline">\(PAP^T\)</span>为对称矩阵，则<span class="math inline">\(BJ\)</span>为对称矩阵。因为<span class="math inline">\(BJ\)</span>第1行第2列元素为<span class="math inline">\(b_{11}+\lambda_1b_{12}\)</span>，第2行第1列元素为<span class="math inline">\(\lambda_1b_{21}\)</span>，则<span class="math inline">\(BJ\)</span>为非对称矩阵，矛盾。因此<span class="math inline">\(A\)</span>可对角化。</p>
<p>下面证明实对称矩阵<span class="math inline">\(A\)</span>不同特征值对应的特征向量之间相互正交。</p>
<p>设<span class="math inline">\(\alpha_1,\alpha_2\)</span>是<span class="math inline">\(A\)</span>的两个不同的特征向量，<span class="math inline">\(A\alpha_1=\lambda_1\alpha_1,A\alpha_2=\lambda_2\alpha_2\)</span>,
且<span class="math inline">\(\lambda_2\ne\lambda_1\)</span>,
分别左乘<span class="math inline">\(\alpha_2^T,\alpha_1^T\)</span>,
则<span class="math inline">\(\alpha_2^TA\alpha_1=\lambda_1\alpha_2^T\alpha_1,\alpha_1^TA\alpha_2=\lambda_2\alpha_1^T\alpha_2\)</span>,
因为向量数乘可交换，所以<span class="math inline">\(\alpha_2^TA\alpha_1=(A\alpha_2)^T\alpha_1=\alpha_1^T(A\alpha_2)=\alpha_1^TA\alpha_2\)</span>,
于是<span class="math inline">\(\lambda_2\alpha_1^T\alpha_2=\lambda_1\alpha_2^T\alpha_1\)</span>,
<span class="math inline">\((\lambda_2-\lambda_1)\alpha_1^T\alpha_2=0\)</span>,
则<span class="math inline">\(\alpha_1^T\alpha_2=0\)</span>.</p>
<p>因此<span class="math inline">\(A\)</span>的不同特征值对应的特征向量构成正交向量空间，在每个r维特征向量空间中选取r个线性无关的模长为1的向量，按列构成矩阵<span class="math inline">\(P\)</span>即可。</p>
<p>2）必要性：若<span class="math inline">\(A=P^{-1}\Lambda
P\)</span>，<span class="math inline">\(\Lambda\)</span>是对角矩阵，<span class="math inline">\(P\)</span>是正交矩阵，则<span class="math inline">\(P^{-1}=P^T,A=P^T\Lambda
P,A^T=P^T\Lambda^TP=P^T\Lambda P=A\)</span>, 则<span class="math inline">\(A\)</span>为对称矩阵。下面证明<span class="math inline">\(A\)</span>的特征值均为实数。</p>
<p>设<span class="math inline">\(Ax=\lambda x\)</span>, <span class="math inline">\(\lambda=a+bi\)</span>, <span class="math inline">\(\bar{x}^TAx=\lambda \bar{x}^Tx\)</span>,
直接计算可证<span class="math inline">\(\bar{AB}=\bar{A}\bar{B}\)</span>, 因此<span class="math inline">\(\bar{x}^TAx=\bar{x}^T\bar{A}^Tx=(\bar{Ax})^Tx=\lambda
\bar{x}^Tx\)</span>, 于是<span class="math inline">\(\bar{Ax}=\lambda
\bar{x}\)</span>, 则<span class="math inline">\(Ax=\bar{\lambda}
x\)</span>, 则<span class="math inline">\(b=0\)</span>. 因此<span class="math inline">\(A\)</span>所有特征值为实数。</p>
<h2 id="正定矩阵">正定矩阵</h2>
<p>若实对称矩阵所有特征值均&gt;0,则为正定，若均<span class="math inline">\(\ge0\)</span>,则为半正定，均&lt;0为负定矩阵。</p>
<p>从几何意义考虑，一个正定矩阵<span class="math inline">\(A_{n\times
n}\)</span>满足<span class="math inline">\(\forall x\in R^{n\times1},
x^TAx&gt;0\)</span>，这表明向量<span class="math inline">\(x\)</span>与<span class="math inline">\(Ax\)</span>的内积为正，对于二，三维向量就是夹角小于<span class="math inline">\(\frac{\pi}{2}\)</span>，高维向量也类比这个方式理解。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Math/" class="category-chain-item">Math</a>
  
  
    <span>></span>
    
  <a href="/categories/Math/Matrix/" class="category-chain-item">Matrix</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/matrix/">#matrix</a>
      
        <a href="/tags/math/">#math</a>
      
    </div>
  
</div>


              

              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/05/05/MIMIC-IV-DERIVED/" title="MIMIC-IV-DERIVED 模块详解">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">MIMIC-IV-DERIVED 模块详解</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/04/21/Low-Dimensional-Data-with-a-Large-Number-of-Samples/" title="Exploring the Performance of ML Models for Low Dimensional Dataset with a Large Number of Samples">
                        <span class="hidden-mobile">Exploring the Performance of ML Models for Low Dimensional Dataset with a Large Number of Samples</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"AgcF1mQgwJbHELHjHp7JRJcB-gzGzoHsz","appKey":"0fENfeqhc4WLt6AcCXBkTH91","path":"window.location.pathname","placeholder":"Submit a comment here","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"https://agcf1mqg.lc-cn-n1-shared.com","emojiCDN":null,"emojiMaps":null,"enableQQ":true},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        Views: 
        <span id="leancloud-site-pv"></span>
        
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        Visitors: 
        <span id="leancloud-site-uv"></span>
        
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
